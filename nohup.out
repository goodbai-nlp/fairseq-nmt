Start training...
2021-03-10 20:46:19 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 200, 'max_update': 200000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/amax/Data/flstm/iwslt/multifeatgen_baseline_new', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 50, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='multifeatgen', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='multifeatgen', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, conv_kernel_size=3, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14_deen_s8000t6000', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_glu=True, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_conv_dim=512, encoder_conv_type='lightweight', encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_glu=True, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, ignore_prefix_size=0, input_dropout=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=200, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=200000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reccurrent_enc=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=50, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/home/amax/Data/flstm/iwslt/multifeatgen_baseline_new', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, weight_dropout=0.1, weight_softmax=True, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14_deen_s8000t6000', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'simul_type': None}
2021-03-10 20:46:19 | INFO | fairseq.tasks.translation | [de] dictionary: 8856 types
2021-03-10 20:46:19 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2021-03-10 20:46:19 | INFO | fairseq.data.data_utils | loaded 7,282 examples from: data-bin/iwslt14_deen_s8000t6000/valid.de-en.de
2021-03-10 20:46:19 | INFO | fairseq.data.data_utils | loaded 7,282 examples from: data-bin/iwslt14_deen_s8000t6000/valid.de-en.en
2021-03-10 20:46:19 | INFO | fairseq.tasks.translation | data-bin/iwslt14_deen_s8000t6000 valid de-en 7282 examples
No module named 'lightconv_cuda'
2021-03-10 20:46:19 | INFO | fairseq_cli.train | FeatLstmModel(
  (encoder): FeatLstmEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8856, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layer): MultiFeatsGenEncoderLayer(
      (feat0): LightConvEncoderLayer(
        dropout=0.3, relu_dropout=0.0, input_dropout=0.1, normalize_before=False
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (act): GLU(dim=-1)
        (conv): LightweightConv1dTBC(
          512, kernel_size=3, padding_l=1, num_heads=8, weight_softmax=True, bias=False, weight_dropout=0.1
          (weight_dropout_module): FairseqDropout()
        )
        (linear2): Linear(in_features=512, out_features=512, bias=True)
        (dropout_module): FairseqDropout()
        (relu_dropout_module): FairseqDropout()
        (input_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (layer_norms): ModuleList(
          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (feat1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (fusion_net): LSTMCell(1024, 512)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6632, bias=False)
  )
)
2021-03-10 20:46:19 | INFO | fairseq_cli.train | task: TranslationTask
2021-03-10 20:46:19 | INFO | fairseq_cli.train | model: FeatLstmModel
2021-03-10 20:46:19 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2021-03-10 20:46:19 | INFO | fairseq_cli.train | num. model params: 33,949,208 (num. trained: 33,949,208)
2021-03-10 20:46:20 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2021-03-10 20:46:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-03-10 20:46:20 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2021-03-10 20:46:20 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-03-10 20:46:20 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-03-10 20:46:20 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2021-03-10 20:46:20 | INFO | fairseq.trainer | Preparing to load checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 20:46:20 | INFO | fairseq.trainer | No existing checkpoint found /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 20:46:20 | INFO | fairseq.trainer | loading train data for epoch 1
2021-03-10 20:46:20 | INFO | fairseq.data.data_utils | loaded 160,215 examples from: data-bin/iwslt14_deen_s8000t6000/train.de-en.de
2021-03-10 20:46:20 | INFO | fairseq.data.data_utils | loaded 160,215 examples from: data-bin/iwslt14_deen_s8000t6000/train.de-en.en
2021-03-10 20:46:20 | INFO | fairseq.tasks.translation | data-bin/iwslt14_deen_s8000t6000 train de-en 160215 examples
Start training...
2021-03-10 20:46:25 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_algorithm': 'LocalSGD', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'tpu': False, 'distributed_num_procs': 1}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0}, 'optimization': {'_name': None, 'max_epoch': 200, 'max_update': 200000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False}, 'checkpoint': {'_name': None, 'save_dir': '/home/amax/Data/flstm/iwslt/multifeatgen_baseline_new', 'restore_file': 'checkpoint_last.pt', 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': 50, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'model_parallel_size': 1, 'distributed_rank': 0}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='multifeatgen', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='multifeatgen', attention_dropout=0.1, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, conv_kernel_size=3, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/iwslt14_deen_s8000t6000', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_glu=True, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=8, encoder_conv_dim=512, encoder_conv_type='lightweight', encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_glu=True, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=False, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe=None, eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, gen_subset='test', heartbeat_timeout=-1, ignore_prefix_size=0, input_dropout=0.1, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=200, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=200000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reccurrent_enc=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=50, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, relu_dropout=0.0, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/home/amax/Data/flstm/iwslt/multifeatgen_baseline_new', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=True, simul_type=None, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang=None, stop_min_lr=-1.0, stop_time_hours=0, suppress_crashes=False, target_lang=None, task='translation', tensorboard_logdir=None, threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_freq=[1], upsample_primary=-1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project=None, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001, weight_dropout=0.1, weight_softmax=True, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/iwslt14_deen_s8000t6000', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0001, 'use_old_adam': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'simul_type': None}
2021-03-10 20:46:25 | INFO | fairseq.tasks.translation | [de] dictionary: 8856 types
2021-03-10 20:46:25 | INFO | fairseq.tasks.translation | [en] dictionary: 6632 types
2021-03-10 20:46:25 | INFO | fairseq.data.data_utils | loaded 7,282 examples from: data-bin/iwslt14_deen_s8000t6000/valid.de-en.de
2021-03-10 20:46:25 | INFO | fairseq.data.data_utils | loaded 7,282 examples from: data-bin/iwslt14_deen_s8000t6000/valid.de-en.en
2021-03-10 20:46:25 | INFO | fairseq.tasks.translation | data-bin/iwslt14_deen_s8000t6000 valid de-en 7282 examples
No module named 'lightconv_cuda'
2021-03-10 20:46:25 | INFO | fairseq_cli.train | FeatLstmModel(
  (encoder): FeatLstmEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(8856, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layer): MultiFeatsGenEncoderLayer(
      (feat0): LightConvEncoderLayer(
        dropout=0.3, relu_dropout=0.0, input_dropout=0.1, normalize_before=False
        (linear1): Linear(in_features=512, out_features=1024, bias=True)
        (act): GLU(dim=-1)
        (conv): LightweightConv1dTBC(
          512, kernel_size=3, padding_l=1, num_heads=8, weight_softmax=True, bias=False, weight_dropout=0.1
          (weight_dropout_module): FairseqDropout()
        )
        (linear2): Linear(in_features=512, out_features=512, bias=True)
        (dropout_module): FairseqDropout()
        (relu_dropout_module): FairseqDropout()
        (input_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (layer_norms): ModuleList(
          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
      (feat1): TransformerEncoderLayer(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (fusion_net): LSTMCell(1024, 512)
    )
  )
  (decoder): TransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(6632, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=6632, bias=False)
  )
)
2021-03-10 20:46:25 | INFO | fairseq_cli.train | task: TranslationTask
2021-03-10 20:46:25 | INFO | fairseq_cli.train | model: FeatLstmModel
2021-03-10 20:46:25 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion
2021-03-10 20:46:25 | INFO | fairseq_cli.train | num. model params: 33,949,208 (num. trained: 33,949,208)
2021-03-10 20:46:27 | INFO | fairseq.trainer | detected shared parameter: decoder.embed_tokens.weight <- decoder.output_projection.weight
2021-03-10 20:46:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-03-10 20:46:27 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 10.761 GB ; name = GeForce RTX 2080 Ti                     
2021-03-10 20:46:27 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************
2021-03-10 20:46:27 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)
2021-03-10 20:46:27 | INFO | fairseq_cli.train | max tokens per GPU = 4096 and batch size per GPU = None
2021-03-10 20:46:27 | INFO | fairseq.trainer | Preparing to load checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 20:46:27 | INFO | fairseq.trainer | No existing checkpoint found /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 20:46:27 | INFO | fairseq.trainer | loading train data for epoch 1
2021-03-10 20:46:27 | INFO | fairseq.data.data_utils | loaded 160,215 examples from: data-bin/iwslt14_deen_s8000t6000/train.de-en.de
2021-03-10 20:46:27 | INFO | fairseq.data.data_utils | loaded 160,215 examples from: data-bin/iwslt14_deen_s8000t6000/train.de-en.en
2021-03-10 20:46:27 | INFO | fairseq.tasks.translation | data-bin/iwslt14_deen_s8000t6000 train de-en 160215 examples
2021-03-10 20:46:27 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16
2021-03-10 20:46:27 | INFO | fairseq.trainer | begin training epoch 1
2021-03-10 20:46:27 | INFO | fairseq_cli.train | Start iterating over samples
/home/amax/Codes/flstm-nmt/fairseq/utils.py:344: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
2021-03-10 20:46:49 | INFO | train_inner | epoch 001:    100 / 1103 loss=12.27, nll_loss=12.133, ppl=4491.5, wps=16601.4, ups=4.71, wpb=3515.6, bsz=121.8, num_updates=100, lr=1.25e-05, gnorm=3.112, train_wall=21, gb_free=7, wall=22
2021-03-10 20:47:11 | INFO | train_inner | epoch 001:    200 / 1103 loss=10.66, nll_loss=10.334, ppl=1290.92, wps=15977.3, ups=4.5, wpb=3550.5, bsz=141.4, num_updates=200, lr=2.5e-05, gnorm=1.387, train_wall=22, gb_free=7, wall=44
2021-03-10 20:47:33 | INFO | train_inner | epoch 001:    300 / 1103 loss=10.025, nll_loss=9.607, ppl=779.84, wps=15801.4, ups=4.45, wpb=3551.7, bsz=133.6, num_updates=300, lr=3.75e-05, gnorm=1.408, train_wall=22, gb_free=6.8, wall=66
2021-03-10 20:47:56 | INFO | train_inner | epoch 001:    400 / 1103 loss=9.557, nll_loss=9.042, ppl=527.21, wps=16235, ups=4.45, wpb=3651.2, bsz=158.2, num_updates=400, lr=5e-05, gnorm=1.4, train_wall=22, gb_free=7, wall=89
2021-03-10 20:48:18 | INFO | train_inner | epoch 001:    500 / 1103 loss=9.442, nll_loss=8.896, ppl=476.24, wps=16039.7, ups=4.4, wpb=3646.7, bsz=152.6, num_updates=500, lr=6.25e-05, gnorm=1.421, train_wall=23, gb_free=7, wall=112
2021-03-10 20:48:41 | INFO | train_inner | epoch 001:    600 / 1103 loss=9.192, nll_loss=8.609, ppl=390.48, wps=15943.3, ups=4.44, wpb=3591, bsz=156, num_updates=600, lr=7.5e-05, gnorm=1.402, train_wall=22, gb_free=7.4, wall=134
2021-03-10 20:49:03 | INFO | train_inner | epoch 001:    700 / 1103 loss=8.953, nll_loss=8.333, ppl=322.49, wps=15919.9, ups=4.47, wpb=3562, bsz=163, num_updates=700, lr=8.75e-05, gnorm=1.529, train_wall=22, gb_free=6.9, wall=157
2021-03-10 20:49:26 | INFO | train_inner | epoch 001:    800 / 1103 loss=8.832, nll_loss=8.194, ppl=292.78, wps=15964.2, ups=4.42, wpb=3613.3, bsz=138.2, num_updates=800, lr=0.0001, gnorm=1.199, train_wall=23, gb_free=7.5, wall=179
2021-03-10 20:49:48 | INFO | train_inner | epoch 001:    900 / 1103 loss=8.665, nll_loss=7.998, ppl=255.66, wps=15902.5, ups=4.5, wpb=3532.1, bsz=139.5, num_updates=900, lr=0.0001125, gnorm=1.366, train_wall=22, gb_free=6.7, wall=201
2021-03-10 20:50:11 | INFO | train_inner | epoch 001:   1000 / 1103 loss=8.463, nll_loss=7.764, ppl=217.31, wps=15815.9, ups=4.41, wpb=3583.6, bsz=152.7, num_updates=1000, lr=0.000125, gnorm=1.364, train_wall=23, gb_free=7, wall=224
2021-03-10 20:50:33 | INFO | train_inner | epoch 001:   1100 / 1103 loss=8.409, nll_loss=7.697, ppl=207.46, wps=15858, ups=4.47, wpb=3549.9, bsz=140.7, num_updates=1100, lr=0.0001375, gnorm=1.453, train_wall=22, gb_free=6.9, wall=246
2021-03-10 20:50:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 20:50:38 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 8.122 | nll_loss 7.325 | ppl 160.31 | wps 48197.2 | wpb 2873.1 | bsz 115.6 | num_updates 1103
2021-03-10 20:50:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1103 updates
2021-03-10 20:50:38 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 20:50:38 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 20:50:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 1 @ 1103 updates, score 8.122) (writing took 0.5291342586278915 seconds)
2021-03-10 20:50:38 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2021-03-10 20:50:38 | INFO | train | epoch 001 | loss 9.489 | nll_loss 8.955 | ppl 496.13 | wps 15725.2 | ups 4.39 | wpb 3577.8 | bsz 145.3 | num_updates 1103 | lr 0.000137875 | gnorm 1.548 | train_wall 246 | gb_free 7 | wall 251
2021-03-10 20:50:38 | INFO | fairseq.trainer | begin training epoch 2
2021-03-10 20:50:38 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 20:51:00 | INFO | train_inner | epoch 002:     97 / 1103 loss=8.092, nll_loss=7.334, ppl=161.34, wps=13292.3, ups=3.67, wpb=3617.3, bsz=161.5, num_updates=1200, lr=0.00015, gnorm=1.281, train_wall=23, gb_free=6.9, wall=274
2021-03-10 20:51:23 | INFO | train_inner | epoch 002:    197 / 1103 loss=8.05, nll_loss=7.284, ppl=155.84, wps=15903.1, ups=4.43, wpb=3592.6, bsz=144.8, num_updates=1300, lr=0.0001625, gnorm=1.244, train_wall=22, gb_free=7, wall=296
2021-03-10 20:51:46 | INFO | train_inner | epoch 002:    297 / 1103 loss=7.884, nll_loss=7.091, ppl=136.38, wps=15686.1, ups=4.41, wpb=3555.7, bsz=157.4, num_updates=1400, lr=0.000175, gnorm=1.349, train_wall=23, gb_free=7.1, wall=319
2021-03-10 20:52:08 | INFO | train_inner | epoch 002:    397 / 1103 loss=7.882, nll_loss=7.087, ppl=135.92, wps=15836.8, ups=4.45, wpb=3560.8, bsz=132.4, num_updates=1500, lr=0.0001875, gnorm=1.228, train_wall=22, gb_free=7.1, wall=341
2021-03-10 20:52:31 | INFO | train_inner | epoch 002:    497 / 1103 loss=7.732, nll_loss=6.913, ppl=120.49, wps=16043.6, ups=4.45, wpb=3607.3, bsz=142.7, num_updates=1600, lr=0.0002, gnorm=1.284, train_wall=22, gb_free=7, wall=364
2021-03-10 20:52:53 | INFO | train_inner | epoch 002:    597 / 1103 loss=7.627, nll_loss=6.792, ppl=110.84, wps=15895.8, ups=4.43, wpb=3588.9, bsz=147, num_updates=1700, lr=0.0002125, gnorm=1.26, train_wall=22, gb_free=7, wall=386
2021-03-10 20:53:16 | INFO | train_inner | epoch 002:    697 / 1103 loss=7.438, nll_loss=6.576, ppl=95.41, wps=15800, ups=4.44, wpb=3555.9, bsz=151.4, num_updates=1800, lr=0.000225, gnorm=1.171, train_wall=22, gb_free=7, wall=409
2021-03-10 20:53:38 | INFO | train_inner | epoch 002:    797 / 1103 loss=7.378, nll_loss=6.505, ppl=90.8, wps=15906.6, ups=4.44, wpb=3585, bsz=146.2, num_updates=1900, lr=0.0002375, gnorm=1.233, train_wall=22, gb_free=6.8, wall=431
2021-03-10 20:54:01 | INFO | train_inner | epoch 002:    897 / 1103 loss=7.317, nll_loss=6.434, ppl=86.44, wps=16021.9, ups=4.46, wpb=3593.9, bsz=135.4, num_updates=2000, lr=0.00025, gnorm=1.16, train_wall=22, gb_free=7, wall=454
2021-03-10 20:54:23 | INFO | train_inner | epoch 002:    997 / 1103 loss=7.144, nll_loss=6.236, ppl=75.39, wps=15666.9, ups=4.47, wpb=3503.2, bsz=142.8, num_updates=2100, lr=0.0002625, gnorm=1.157, train_wall=22, gb_free=7.4, wall=476
2021-03-10 20:54:45 | INFO | train_inner | epoch 002:   1097 / 1103 loss=7.124, nll_loss=6.211, ppl=74.07, wps=16216.4, ups=4.5, wpb=3607.4, bsz=139.8, num_updates=2200, lr=0.000275, gnorm=1.221, train_wall=22, gb_free=6.8, wall=499
2021-03-10 20:54:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 20:54:51 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 6.698 | nll_loss 5.664 | ppl 50.72 | wps 48199.2 | wpb 2873.1 | bsz 115.6 | num_updates 2206 | best_loss 6.698
2021-03-10 20:54:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2206 updates
2021-03-10 20:54:51 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 20:54:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 20:54:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 2 @ 2206 updates, score 6.698) (writing took 4.509477291256189 seconds)
2021-03-10 20:54:55 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2021-03-10 20:54:55 | INFO | train | epoch 002 | loss 7.604 | nll_loss 6.767 | ppl 108.91 | wps 15373.5 | ups 4.3 | wpb 3577.8 | bsz 145.3 | num_updates 2206 | lr 0.00027575 | gnorm 1.236 | train_wall 247 | gb_free 6.6 | wall 508
2021-03-10 20:54:55 | INFO | fairseq.trainer | begin training epoch 3
2021-03-10 20:54:55 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 20:55:16 | INFO | train_inner | epoch 003:     94 / 1103 loss=6.908, nll_loss=5.963, ppl=62.39, wps=11620.6, ups=3.24, wpb=3584.9, bsz=140.2, num_updates=2300, lr=0.0002875, gnorm=1.121, train_wall=22, gb_free=7.3, wall=529
2021-03-10 20:55:39 | INFO | train_inner | epoch 003:    194 / 1103 loss=6.806, nll_loss=5.845, ppl=57.46, wps=15792, ups=4.45, wpb=3547.5, bsz=138.9, num_updates=2400, lr=0.0003, gnorm=1.193, train_wall=22, gb_free=7, wall=552
2021-03-10 20:56:01 | INFO | train_inner | epoch 003:    294 / 1103 loss=6.648, nll_loss=5.663, ppl=50.68, wps=15866.9, ups=4.45, wpb=3566, bsz=143.4, num_updates=2500, lr=0.0003125, gnorm=1.152, train_wall=22, gb_free=6.7, wall=574
2021-03-10 20:56:23 | INFO | train_inner | epoch 003:    394 / 1103 loss=6.524, nll_loss=5.517, ppl=45.79, wps=16144.7, ups=4.48, wpb=3605, bsz=143.4, num_updates=2600, lr=0.000325, gnorm=1.143, train_wall=22, gb_free=7.1, wall=597
2021-03-10 20:56:46 | INFO | train_inner | epoch 003:    494 / 1103 loss=6.478, nll_loss=5.462, ppl=44.09, wps=16096.7, ups=4.5, wpb=3577.6, bsz=135.7, num_updates=2700, lr=0.0003375, gnorm=1.143, train_wall=22, gb_free=7.2, wall=619
2021-03-10 20:57:08 | INFO | train_inner | epoch 003:    594 / 1103 loss=6.239, nll_loss=5.185, ppl=36.39, wps=15752.9, ups=4.43, wpb=3557.8, bsz=152.4, num_updates=2800, lr=0.00035, gnorm=1.128, train_wall=22, gb_free=7, wall=641
2021-03-10 20:57:31 | INFO | train_inner | epoch 003:    694 / 1103 loss=6.151, nll_loss=5.083, ppl=33.89, wps=15934.5, ups=4.4, wpb=3623.5, bsz=157.7, num_updates=2900, lr=0.0003625, gnorm=1.132, train_wall=23, gb_free=6.8, wall=664
2021-03-10 20:57:54 | INFO | train_inner | epoch 003:    794 / 1103 loss=6.041, nll_loss=4.953, ppl=30.98, wps=15922.3, ups=4.45, wpb=3581, bsz=153, num_updates=3000, lr=0.000375, gnorm=1.129, train_wall=22, gb_free=6.9, wall=687
2021-03-10 20:58:16 | INFO | train_inner | epoch 003:    894 / 1103 loss=6.092, nll_loss=5.009, ppl=32.19, wps=15974.9, ups=4.44, wpb=3595.6, bsz=122.1, num_updates=3100, lr=0.0003875, gnorm=1.11, train_wall=22, gb_free=7.2, wall=709
2021-03-10 20:58:38 | INFO | train_inner | epoch 003:    994 / 1103 loss=5.916, nll_loss=4.805, ppl=27.96, wps=15995.7, ups=4.5, wpb=3557.8, bsz=149.6, num_updates=3200, lr=0.0004, gnorm=1.104, train_wall=22, gb_free=6.9, wall=731
2021-03-10 20:59:01 | INFO | train_inner | epoch 003:   1094 / 1103 loss=5.783, nll_loss=4.65, ppl=25.1, wps=15889.6, ups=4.46, wpb=3560.4, bsz=158.1, num_updates=3300, lr=0.0004125, gnorm=1.13, train_wall=22, gb_free=7.1, wall=754
2021-03-10 20:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 20:59:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 5.548 | nll_loss 4.286 | ppl 19.5 | wps 48239.9 | wpb 2873.1 | bsz 115.6 | num_updates 3309 | best_loss 5.548
2021-03-10 20:59:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 3309 updates
2021-03-10 20:59:06 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 20:59:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 20:59:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 3 @ 3309 updates, score 5.548) (writing took 4.470445849001408 seconds)
2021-03-10 20:59:11 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2021-03-10 20:59:11 | INFO | train | epoch 003 | loss 6.316 | nll_loss 5.274 | ppl 38.68 | wps 15419.3 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 3309 | lr 0.000413625 | gnorm 1.134 | train_wall 246 | gb_free 6.9 | wall 764
2021-03-10 20:59:11 | INFO | fairseq.trainer | begin training epoch 4
2021-03-10 20:59:11 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 20:59:32 | INFO | train_inner | epoch 004:     91 / 1103 loss=5.647, nll_loss=4.493, ppl=22.51, wps=11558.1, ups=3.24, wpb=3565.6, bsz=151.6, num_updates=3400, lr=0.000425, gnorm=1.066, train_wall=22, gb_free=6.5, wall=785
2021-03-10 20:59:54 | INFO | train_inner | epoch 004:    191 / 1103 loss=5.659, nll_loss=4.502, ppl=22.66, wps=15923.8, ups=4.49, wpb=3543.4, bsz=136.6, num_updates=3500, lr=0.0004375, gnorm=1.084, train_wall=22, gb_free=6.6, wall=807
2021-03-10 21:00:16 | INFO | train_inner | epoch 004:    291 / 1103 loss=5.573, nll_loss=4.401, ppl=21.12, wps=16177.7, ups=4.47, wpb=3621.9, bsz=132.6, num_updates=3600, lr=0.00045, gnorm=1.052, train_wall=22, gb_free=7, wall=829
2021-03-10 21:00:38 | INFO | train_inner | epoch 004:    391 / 1103 loss=5.618, nll_loss=4.451, ppl=21.88, wps=15895.9, ups=4.52, wpb=3516.7, bsz=134.6, num_updates=3700, lr=0.0004625, gnorm=1.094, train_wall=22, gb_free=7, wall=851
2021-03-10 21:01:01 | INFO | train_inner | epoch 004:    491 / 1103 loss=5.451, nll_loss=4.258, ppl=19.13, wps=16016.3, ups=4.44, wpb=3611.1, bsz=140.3, num_updates=3800, lr=0.000475, gnorm=1.013, train_wall=22, gb_free=7, wall=874
2021-03-10 21:01:23 | INFO | train_inner | epoch 004:    591 / 1103 loss=5.447, nll_loss=4.252, ppl=19.06, wps=15986.5, ups=4.44, wpb=3596.9, bsz=139.8, num_updates=3900, lr=0.0004875, gnorm=1.054, train_wall=22, gb_free=7.3, wall=896
2021-03-10 21:01:46 | INFO | train_inner | epoch 004:    691 / 1103 loss=5.363, nll_loss=4.156, ppl=17.82, wps=15886.5, ups=4.41, wpb=3598.3, bsz=145.3, num_updates=4000, lr=0.0005, gnorm=1.038, train_wall=23, gb_free=6.8, wall=919
2021-03-10 21:02:08 | INFO | train_inner | epoch 004:    791 / 1103 loss=5.291, nll_loss=4.072, ppl=16.82, wps=15732.3, ups=4.47, wpb=3521.2, bsz=162.6, num_updates=4100, lr=0.000493865, gnorm=1.075, train_wall=22, gb_free=6.9, wall=942
2021-03-10 21:02:31 | INFO | train_inner | epoch 004:    891 / 1103 loss=5.277, nll_loss=4.055, ppl=16.62, wps=15917.2, ups=4.46, wpb=3571.6, bsz=151.5, num_updates=4200, lr=0.00048795, gnorm=1.026, train_wall=22, gb_free=7, wall=964
2021-03-10 21:02:53 | INFO | train_inner | epoch 004:    991 / 1103 loss=5.207, nll_loss=3.975, ppl=15.72, wps=16249.8, ups=4.46, wpb=3640, bsz=145, num_updates=4300, lr=0.000482243, gnorm=0.959, train_wall=22, gb_free=6.8, wall=986
2021-03-10 21:03:16 | INFO | train_inner | epoch 004:   1091 / 1103 loss=5.12, nll_loss=3.876, ppl=14.69, wps=15918.3, ups=4.46, wpb=3572.9, bsz=157.1, num_updates=4400, lr=0.000476731, gnorm=0.961, train_wall=22, gb_free=7.2, wall=1009
2021-03-10 21:03:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:03:22 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 4.961 | nll_loss 3.572 | ppl 11.89 | wps 48243.5 | wpb 2873.1 | bsz 115.6 | num_updates 4412 | best_loss 4.961
2021-03-10 21:03:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 4412 updates
2021-03-10 21:03:22 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:03:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:03:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 4 @ 4412 updates, score 4.961) (writing took 4.435573291033506 seconds)
2021-03-10 21:03:27 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2021-03-10 21:03:27 | INFO | train | epoch 004 | loss 5.417 | nll_loss 4.22 | ppl 18.63 | wps 15438 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 4412 | lr 0.000476083 | gnorm 1.037 | train_wall 246 | gb_free 7.2 | wall 1020
2021-03-10 21:03:27 | INFO | fairseq.trainer | begin training epoch 5
2021-03-10 21:03:27 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:03:46 | INFO | train_inner | epoch 005:     88 / 1103 loss=5.043, nll_loss=3.788, ppl=13.81, wps=11533.9, ups=3.25, wpb=3545.8, bsz=155.8, num_updates=4500, lr=0.000471405, gnorm=0.974, train_wall=22, gb_free=7, wall=1040
2021-03-10 21:04:09 | INFO | train_inner | epoch 005:    188 / 1103 loss=4.97, nll_loss=3.702, ppl=13.02, wps=16087.5, ups=4.44, wpb=3626.9, bsz=152.2, num_updates=4600, lr=0.000466252, gnorm=0.927, train_wall=22, gb_free=7, wall=1062
2021-03-10 21:04:31 | INFO | train_inner | epoch 005:    288 / 1103 loss=5.032, nll_loss=3.773, ppl=13.67, wps=15907.8, ups=4.47, wpb=3561.3, bsz=141.7, num_updates=4700, lr=0.000461266, gnorm=0.959, train_wall=22, gb_free=7.6, wall=1084
2021-03-10 21:04:54 | INFO | train_inner | epoch 005:    388 / 1103 loss=4.958, nll_loss=3.689, ppl=12.9, wps=16025.1, ups=4.47, wpb=3583.8, bsz=140.7, num_updates=4800, lr=0.000456435, gnorm=0.909, train_wall=22, gb_free=7, wall=1107
2021-03-10 21:05:16 | INFO | train_inner | epoch 005:    488 / 1103 loss=4.962, nll_loss=3.692, ppl=12.93, wps=15629.1, ups=4.43, wpb=3527.9, bsz=129, num_updates=4900, lr=0.000451754, gnorm=0.934, train_wall=22, gb_free=6.8, wall=1129
2021-03-10 21:05:39 | INFO | train_inner | epoch 005:    588 / 1103 loss=4.908, nll_loss=3.63, ppl=12.38, wps=15812.6, ups=4.45, wpb=3554.2, bsz=137.5, num_updates=5000, lr=0.000447214, gnorm=0.916, train_wall=22, gb_free=6.9, wall=1152
2021-03-10 21:06:01 | INFO | train_inner | epoch 005:    688 / 1103 loss=4.756, nll_loss=3.457, ppl=10.99, wps=16126.9, ups=4.42, wpb=3646, bsz=162.2, num_updates=5100, lr=0.000442807, gnorm=0.877, train_wall=22, gb_free=7.2, wall=1175
2021-03-10 21:06:24 | INFO | train_inner | epoch 005:    788 / 1103 loss=4.874, nll_loss=3.594, ppl=12.07, wps=15798.2, ups=4.47, wpb=3532.7, bsz=136.1, num_updates=5200, lr=0.000438529, gnorm=0.919, train_wall=22, gb_free=6.9, wall=1197
2021-03-10 21:06:46 | INFO | train_inner | epoch 005:    888 / 1103 loss=4.766, nll_loss=3.47, ppl=11.08, wps=15912.9, ups=4.45, wpb=3572.4, bsz=158.9, num_updates=5300, lr=0.000434372, gnorm=0.941, train_wall=22, gb_free=7, wall=1219
2021-03-10 21:07:09 | INFO | train_inner | epoch 005:    988 / 1103 loss=4.787, nll_loss=3.494, ppl=11.27, wps=15886.7, ups=4.46, wpb=3562.5, bsz=139, num_updates=5400, lr=0.000430331, gnorm=0.892, train_wall=22, gb_free=6.9, wall=1242
2021-03-10 21:07:31 | INFO | train_inner | epoch 005:   1088 / 1103 loss=4.765, nll_loss=3.47, ppl=11.08, wps=15994.4, ups=4.41, wpb=3629.4, bsz=144.1, num_updates=5500, lr=0.000426401, gnorm=0.899, train_wall=23, gb_free=6.6, wall=1264
2021-03-10 21:07:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:07:38 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 4.599 | nll_loss 3.157 | ppl 8.92 | wps 48285.5 | wpb 2873.1 | bsz 115.6 | num_updates 5515 | best_loss 4.599
2021-03-10 21:07:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 5515 updates
2021-03-10 21:07:38 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:07:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:07:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 5 @ 5515 updates, score 4.599) (writing took 4.789154198020697 seconds)
2021-03-10 21:07:43 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2021-03-10 21:07:43 | INFO | train | epoch 005 | loss 4.886 | nll_loss 3.607 | ppl 12.19 | wps 15375.2 | ups 4.3 | wpb 3577.8 | bsz 145.3 | num_updates 5515 | lr 0.000425821 | gnorm 0.922 | train_wall 247 | gb_free 7.2 | wall 1276
2021-03-10 21:07:43 | INFO | fairseq.trainer | begin training epoch 6
2021-03-10 21:07:43 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:08:02 | INFO | train_inner | epoch 006:     85 / 1103 loss=4.68, nll_loss=3.372, ppl=10.36, wps=11542.7, ups=3.26, wpb=3544.4, bsz=144.4, num_updates=5600, lr=0.000422577, gnorm=0.923, train_wall=22, gb_free=7.2, wall=1295
2021-03-10 21:08:24 | INFO | train_inner | epoch 006:    185 / 1103 loss=4.618, nll_loss=3.3, ppl=9.85, wps=15905.8, ups=4.49, wpb=3545.7, bsz=155.5, num_updates=5700, lr=0.000418854, gnorm=0.908, train_wall=22, gb_free=7.1, wall=1317
2021-03-10 21:08:47 | INFO | train_inner | epoch 006:    285 / 1103 loss=4.612, nll_loss=3.292, ppl=9.8, wps=15576.4, ups=4.45, wpb=3498.8, bsz=137.2, num_updates=5800, lr=0.000415227, gnorm=0.892, train_wall=22, gb_free=7.3, wall=1340
2021-03-10 21:09:10 | INFO | train_inner | epoch 006:    385 / 1103 loss=4.557, nll_loss=3.231, ppl=9.39, wps=15898.1, ups=4.38, wpb=3627.9, bsz=146.6, num_updates=5900, lr=0.000411693, gnorm=0.847, train_wall=23, gb_free=6.8, wall=1363
2021-03-10 21:09:32 | INFO | train_inner | epoch 006:    485 / 1103 loss=4.602, nll_loss=3.281, ppl=9.72, wps=16081.5, ups=4.43, wpb=3629.9, bsz=143.4, num_updates=6000, lr=0.000408248, gnorm=0.876, train_wall=22, gb_free=7.1, wall=1385
2021-03-10 21:09:54 | INFO | train_inner | epoch 006:    585 / 1103 loss=4.637, nll_loss=3.323, ppl=10.01, wps=15949.2, ups=4.49, wpb=3552.5, bsz=138.1, num_updates=6100, lr=0.000404888, gnorm=0.899, train_wall=22, gb_free=7.4, wall=1408
2021-03-10 21:10:17 | INFO | train_inner | epoch 006:    685 / 1103 loss=4.534, nll_loss=3.206, ppl=9.23, wps=15826.3, ups=4.48, wpb=3535.5, bsz=157.8, num_updates=6200, lr=0.00040161, gnorm=0.889, train_wall=22, gb_free=7.2, wall=1430
2021-03-10 21:10:40 | INFO | train_inner | epoch 006:    785 / 1103 loss=4.489, nll_loss=3.155, ppl=8.91, wps=16158.7, ups=4.4, wpb=3675.8, bsz=159.6, num_updates=6300, lr=0.00039841, gnorm=0.842, train_wall=23, gb_free=7.3, wall=1453
2021-03-10 21:11:02 | INFO | train_inner | epoch 006:    885 / 1103 loss=4.617, nll_loss=3.302, ppl=9.86, wps=15961.4, ups=4.48, wpb=3563.8, bsz=137.9, num_updates=6400, lr=0.000395285, gnorm=0.9, train_wall=22, gb_free=7, wall=1475
2021-03-10 21:11:24 | INFO | train_inner | epoch 006:    985 / 1103 loss=4.608, nll_loss=3.292, ppl=9.79, wps=15847, ups=4.44, wpb=3566.2, bsz=130.9, num_updates=6500, lr=0.000392232, gnorm=0.87, train_wall=22, gb_free=7, wall=1497
2021-03-10 21:11:47 | INFO | train_inner | epoch 006:   1085 / 1103 loss=4.499, nll_loss=3.167, ppl=8.98, wps=16194.5, ups=4.49, wpb=3610.4, bsz=150.6, num_updates=6600, lr=0.000389249, gnorm=0.864, train_wall=22, gb_free=7.3, wall=1520
2021-03-10 21:11:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:11:55 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 4.471 | nll_loss 3.006 | ppl 8.03 | wps 48329.7 | wpb 2873.1 | bsz 115.6 | num_updates 6618 | best_loss 4.471
2021-03-10 21:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 6618 updates
2021-03-10 21:11:55 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:11:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:11:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 6 @ 6618 updates, score 4.471) (writing took 4.44807855039835 seconds)
2021-03-10 21:11:59 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2021-03-10 21:11:59 | INFO | train | epoch 006 | loss 4.584 | nll_loss 3.263 | ppl 9.6 | wps 15432.5 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 6618 | lr 0.00038872 | gnorm 0.883 | train_wall 246 | gb_free 7 | wall 1532
2021-03-10 21:11:59 | INFO | fairseq.trainer | begin training epoch 7
2021-03-10 21:11:59 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:12:18 | INFO | train_inner | epoch 007:     82 / 1103 loss=4.389, nll_loss=3.04, ppl=8.23, wps=11596, ups=3.23, wpb=3592.4, bsz=146.5, num_updates=6700, lr=0.000386334, gnorm=0.84, train_wall=22, gb_free=7.4, wall=1551
2021-03-10 21:12:40 | INFO | train_inner | epoch 007:    182 / 1103 loss=4.317, nll_loss=2.958, ppl=7.77, wps=15948.6, ups=4.41, wpb=3618.3, bsz=151, num_updates=6800, lr=0.000383482, gnorm=0.841, train_wall=23, gb_free=7.1, wall=1573
2021-03-10 21:13:03 | INFO | train_inner | epoch 007:    282 / 1103 loss=4.432, nll_loss=3.09, ppl=8.51, wps=16207.3, ups=4.5, wpb=3602.7, bsz=151.6, num_updates=6900, lr=0.000380693, gnorm=0.856, train_wall=22, gb_free=7.6, wall=1596
2021-03-10 21:13:25 | INFO | train_inner | epoch 007:    382 / 1103 loss=4.398, nll_loss=3.051, ppl=8.29, wps=16214.3, ups=4.45, wpb=3644.2, bsz=145.9, num_updates=7000, lr=0.000377964, gnorm=0.83, train_wall=22, gb_free=6.8, wall=1618
2021-03-10 21:13:47 | INFO | train_inner | epoch 007:    482 / 1103 loss=4.459, nll_loss=3.121, ppl=8.7, wps=15631.9, ups=4.45, wpb=3512.6, bsz=133.4, num_updates=7100, lr=0.000375293, gnorm=0.884, train_wall=22, gb_free=6.8, wall=1641
2021-03-10 21:14:10 | INFO | train_inner | epoch 007:    582 / 1103 loss=4.418, nll_loss=3.074, ppl=8.42, wps=16107.1, ups=4.45, wpb=3616.3, bsz=144.3, num_updates=7200, lr=0.000372678, gnorm=0.859, train_wall=22, gb_free=6.9, wall=1663
2021-03-10 21:14:32 | INFO | train_inner | epoch 007:    682 / 1103 loss=4.44, nll_loss=3.099, ppl=8.57, wps=15750.1, ups=4.51, wpb=3495.7, bsz=139.5, num_updates=7300, lr=0.000370117, gnorm=0.913, train_wall=22, gb_free=7, wall=1685
2021-03-10 21:14:55 | INFO | train_inner | epoch 007:    782 / 1103 loss=4.335, nll_loss=2.979, ppl=7.89, wps=16064.6, ups=4.43, wpb=3623.4, bsz=148.9, num_updates=7400, lr=0.000367607, gnorm=0.828, train_wall=22, gb_free=7.1, wall=1708
2021-03-10 21:15:17 | INFO | train_inner | epoch 007:    882 / 1103 loss=4.375, nll_loss=3.027, ppl=8.15, wps=15735.5, ups=4.47, wpb=3524, bsz=152.2, num_updates=7500, lr=0.000365148, gnorm=0.85, train_wall=22, gb_free=6.6, wall=1730
2021-03-10 21:15:39 | INFO | train_inner | epoch 007:    982 / 1103 loss=4.38, nll_loss=3.032, ppl=8.18, wps=15968.5, ups=4.47, wpb=3571.5, bsz=143.2, num_updates=7600, lr=0.000362738, gnorm=0.87, train_wall=22, gb_free=7.2, wall=1753
2021-03-10 21:16:02 | INFO | train_inner | epoch 007:   1082 / 1103 loss=4.381, nll_loss=3.034, ppl=8.19, wps=15998.9, ups=4.49, wpb=3562.3, bsz=136.4, num_updates=7700, lr=0.000360375, gnorm=0.838, train_wall=22, gb_free=7.3, wall=1775
2021-03-10 21:16:06 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:16:10 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 4.338 | nll_loss 2.855 | ppl 7.24 | wps 48320.7 | wpb 2873.1 | bsz 115.6 | num_updates 7721 | best_loss 4.338
2021-03-10 21:16:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 7721 updates
2021-03-10 21:16:10 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:16:12 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:16:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 7 @ 7721 updates, score 4.338) (writing took 4.402955695986748 seconds)
2021-03-10 21:16:15 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2021-03-10 21:16:15 | INFO | train | epoch 007 | loss 4.389 | nll_loss 3.041 | ppl 8.23 | wps 15437 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 7721 | lr 0.000359885 | gnorm 0.854 | train_wall 246 | gb_free 7 | wall 1788
2021-03-10 21:16:15 | INFO | fairseq.trainer | begin training epoch 8
2021-03-10 21:16:15 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:16:33 | INFO | train_inner | epoch 008:     79 / 1103 loss=4.283, nll_loss=2.921, ppl=7.57, wps=11640.5, ups=3.25, wpb=3586.8, bsz=136.5, num_updates=7800, lr=0.000358057, gnorm=0.839, train_wall=22, gb_free=7.1, wall=1806
2021-03-10 21:16:55 | INFO | train_inner | epoch 008:    179 / 1103 loss=4.173, nll_loss=2.796, ppl=6.95, wps=15906.5, ups=4.42, wpb=3597.8, bsz=166.2, num_updates=7900, lr=0.000355784, gnorm=0.82, train_wall=23, gb_free=6.9, wall=1828
2021-03-10 21:17:18 | INFO | train_inner | epoch 008:    279 / 1103 loss=4.195, nll_loss=2.819, ppl=7.05, wps=15713.1, ups=4.44, wpb=3538.8, bsz=145.7, num_updates=8000, lr=0.000353553, gnorm=0.839, train_wall=22, gb_free=7, wall=1851
2021-03-10 21:17:39 | INFO | train_inner | epoch 008:    379 / 1103 loss=4.287, nll_loss=2.926, ppl=7.6, wps=16215.7, ups=4.58, wpb=3538.3, bsz=143.4, num_updates=8100, lr=0.000351364, gnorm=0.853, train_wall=22, gb_free=7.1, wall=1873
2021-03-10 21:18:00 | INFO | train_inner | epoch 008:    479 / 1103 loss=4.263, nll_loss=2.899, ppl=7.46, wps=17668.3, ups=4.9, wpb=3604.5, bsz=152, num_updates=8200, lr=0.000349215, gnorm=0.871, train_wall=20, gb_free=6.9, wall=1893
2021-03-10 21:18:20 | INFO | train_inner | epoch 008:    579 / 1103 loss=4.254, nll_loss=2.887, ppl=7.4, wps=17583.1, ups=4.91, wpb=3582.9, bsz=143.2, num_updates=8300, lr=0.000347105, gnorm=0.827, train_wall=20, gb_free=7, wall=1913
2021-03-10 21:18:41 | INFO | train_inner | epoch 008:    679 / 1103 loss=4.238, nll_loss=2.871, ppl=7.31, wps=17421.8, ups=4.87, wpb=3579.5, bsz=151.8, num_updates=8400, lr=0.000345033, gnorm=0.815, train_wall=20, gb_free=6.9, wall=1934
2021-03-10 21:19:01 | INFO | train_inner | epoch 008:    779 / 1103 loss=4.284, nll_loss=2.922, ppl=7.58, wps=17361.2, ups=4.92, wpb=3526.1, bsz=136.4, num_updates=8500, lr=0.000342997, gnorm=0.854, train_wall=20, gb_free=6.8, wall=1954
2021-03-10 21:19:22 | INFO | train_inner | epoch 008:    879 / 1103 loss=4.289, nll_loss=2.928, ppl=7.61, wps=17324.2, ups=4.86, wpb=3561.1, bsz=135, num_updates=8600, lr=0.000340997, gnorm=0.849, train_wall=20, gb_free=6.9, wall=1975
2021-03-10 21:19:42 | INFO | train_inner | epoch 008:    979 / 1103 loss=4.24, nll_loss=2.873, ppl=7.33, wps=17679.2, ups=4.86, wpb=3639.5, bsz=148.7, num_updates=8700, lr=0.000339032, gnorm=0.818, train_wall=20, gb_free=7.1, wall=1995
2021-03-10 21:20:04 | INFO | train_inner | epoch 008:   1079 / 1103 loss=4.252, nll_loss=2.888, ppl=7.4, wps=16567.1, ups=4.61, wpb=3595.6, bsz=143.3, num_updates=8800, lr=0.0003371, gnorm=0.828, train_wall=22, gb_free=7, wall=2017
2021-03-10 21:20:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:20:13 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 4.26 | nll_loss 2.76 | ppl 6.77 | wps 48311.7 | wpb 2873.1 | bsz 115.6 | num_updates 8824 | best_loss 4.26
2021-03-10 21:20:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 8824 updates
2021-03-10 21:20:13 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:20:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:20:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 8 @ 8824 updates, score 4.26) (writing took 4.499916851520538 seconds)
2021-03-10 21:20:18 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2021-03-10 21:20:18 | INFO | train | epoch 008 | loss 4.249 | nll_loss 2.883 | ppl 7.38 | wps 16236.1 | ups 4.54 | wpb 3577.8 | bsz 145.3 | num_updates 8824 | lr 0.000336641 | gnorm 0.838 | train_wall 233 | gb_free 7.2 | wall 2031
2021-03-10 21:20:18 | INFO | fairseq.trainer | begin training epoch 9
2021-03-10 21:20:18 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:20:35 | INFO | train_inner | epoch 009:     76 / 1103 loss=4.077, nll_loss=2.686, ppl=6.44, wps=11826.6, ups=3.21, wpb=3678.9, bsz=160.2, num_updates=8900, lr=0.000335201, gnorm=0.79, train_wall=23, gb_free=7.4, wall=2048
2021-03-10 21:20:58 | INFO | train_inner | epoch 009:    176 / 1103 loss=4.166, nll_loss=2.786, ppl=6.9, wps=15973.6, ups=4.43, wpb=3603.1, bsz=129.5, num_updates=9000, lr=0.000333333, gnorm=0.822, train_wall=22, gb_free=7, wall=2071
2021-03-10 21:21:20 | INFO | train_inner | epoch 009:    276 / 1103 loss=4.156, nll_loss=2.776, ppl=6.85, wps=15859.8, ups=4.46, wpb=3555.6, bsz=138.2, num_updates=9100, lr=0.000331497, gnorm=0.856, train_wall=22, gb_free=6.6, wall=2093
2021-03-10 21:21:43 | INFO | train_inner | epoch 009:    376 / 1103 loss=4.154, nll_loss=2.774, ppl=6.84, wps=15824.8, ups=4.44, wpb=3566.8, bsz=139.9, num_updates=9200, lr=0.00032969, gnorm=0.855, train_wall=22, gb_free=6.8, wall=2116
2021-03-10 21:22:05 | INFO | train_inner | epoch 009:    476 / 1103 loss=4.141, nll_loss=2.758, ppl=6.77, wps=16001.5, ups=4.45, wpb=3596.1, bsz=141.5, num_updates=9300, lr=0.000327913, gnorm=0.809, train_wall=22, gb_free=7.2, wall=2138
2021-03-10 21:22:27 | INFO | train_inner | epoch 009:    576 / 1103 loss=4.11, nll_loss=2.725, ppl=6.61, wps=15970.1, ups=4.5, wpb=3545.7, bsz=157.8, num_updates=9400, lr=0.000326164, gnorm=0.849, train_wall=22, gb_free=7.5, wall=2160
2021-03-10 21:22:50 | INFO | train_inner | epoch 009:    676 / 1103 loss=4.194, nll_loss=2.821, ppl=7.07, wps=15877.3, ups=4.49, wpb=3539.8, bsz=134.3, num_updates=9500, lr=0.000324443, gnorm=0.849, train_wall=22, gb_free=7, wall=2183
2021-03-10 21:23:12 | INFO | train_inner | epoch 009:    776 / 1103 loss=4.164, nll_loss=2.787, ppl=6.9, wps=15962.9, ups=4.51, wpb=3541.4, bsz=145.1, num_updates=9600, lr=0.000322749, gnorm=0.835, train_wall=22, gb_free=7, wall=2205
2021-03-10 21:23:34 | INFO | train_inner | epoch 009:    876 / 1103 loss=4.129, nll_loss=2.747, ppl=6.71, wps=16011, ups=4.48, wpb=3574.1, bsz=149.2, num_updates=9700, lr=0.000321081, gnorm=0.83, train_wall=22, gb_free=7.3, wall=2227
2021-03-10 21:23:57 | INFO | train_inner | epoch 009:    976 / 1103 loss=4.14, nll_loss=2.76, ppl=6.78, wps=15985.1, ups=4.42, wpb=3612.7, bsz=151.2, num_updates=9800, lr=0.000319438, gnorm=0.822, train_wall=22, gb_free=6.9, wall=2250
2021-03-10 21:24:19 | INFO | train_inner | epoch 009:   1076 / 1103 loss=4.144, nll_loss=2.766, ppl=6.8, wps=15870.8, ups=4.45, wpb=3566.9, bsz=145.8, num_updates=9900, lr=0.000317821, gnorm=0.828, train_wall=22, gb_free=7.2, wall=2272
2021-03-10 21:24:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:24:29 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 4.178 | nll_loss 2.664 | ppl 6.34 | wps 48233.9 | wpb 2873.1 | bsz 115.6 | num_updates 9927 | best_loss 4.178
2021-03-10 21:24:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 9927 updates
2021-03-10 21:24:29 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:24:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:24:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 9 @ 9927 updates, score 4.178) (writing took 4.822915319353342 seconds)
2021-03-10 21:24:34 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2021-03-10 21:24:34 | INFO | train | epoch 009 | loss 4.141 | nll_loss 2.761 | ppl 6.78 | wps 15414.9 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 9927 | lr 0.000317388 | gnorm 0.831 | train_wall 246 | gb_free 7 | wall 2287
2021-03-10 21:24:34 | INFO | fairseq.trainer | begin training epoch 10
2021-03-10 21:24:34 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:24:50 | INFO | train_inner | epoch 010:     73 / 1103 loss=4.06, nll_loss=2.669, ppl=6.36, wps=11366.7, ups=3.23, wpb=3517.5, bsz=146.3, num_updates=10000, lr=0.000316228, gnorm=0.822, train_wall=22, gb_free=7, wall=2303
2021-03-10 21:25:12 | INFO | train_inner | epoch 010:    173 / 1103 loss=4.066, nll_loss=2.674, ppl=6.38, wps=16083.9, ups=4.5, wpb=3576.1, bsz=139, num_updates=10100, lr=0.000314658, gnorm=0.813, train_wall=22, gb_free=7, wall=2325
2021-03-10 21:25:35 | INFO | train_inner | epoch 010:    273 / 1103 loss=4.001, nll_loss=2.599, ppl=6.06, wps=16025.7, ups=4.45, wpb=3603.1, bsz=150.5, num_updates=10200, lr=0.000313112, gnorm=0.802, train_wall=22, gb_free=7, wall=2348
2021-03-10 21:25:57 | INFO | train_inner | epoch 010:    373 / 1103 loss=4.047, nll_loss=2.653, ppl=6.29, wps=16021.8, ups=4.46, wpb=3591, bsz=146.4, num_updates=10300, lr=0.000311588, gnorm=0.822, train_wall=22, gb_free=7, wall=2370
2021-03-10 21:26:20 | INFO | train_inner | epoch 010:    473 / 1103 loss=4.085, nll_loss=2.695, ppl=6.48, wps=15856.5, ups=4.48, wpb=3539.6, bsz=134.9, num_updates=10400, lr=0.000310087, gnorm=0.864, train_wall=22, gb_free=6.9, wall=2393
2021-03-10 21:26:42 | INFO | train_inner | epoch 010:    573 / 1103 loss=4.029, nll_loss=2.633, ppl=6.2, wps=16187.4, ups=4.45, wpb=3634.8, bsz=150.2, num_updates=10500, lr=0.000308607, gnorm=0.799, train_wall=22, gb_free=7.4, wall=2415
2021-03-10 21:27:04 | INFO | train_inner | epoch 010:    673 / 1103 loss=4.063, nll_loss=2.672, ppl=6.37, wps=15881.6, ups=4.49, wpb=3539.7, bsz=146.3, num_updates=10600, lr=0.000307148, gnorm=0.842, train_wall=22, gb_free=7.1, wall=2437
2021-03-10 21:27:26 | INFO | train_inner | epoch 010:    773 / 1103 loss=4.09, nll_loss=2.702, ppl=6.51, wps=16366, ups=4.58, wpb=3569.5, bsz=141, num_updates=10700, lr=0.000305709, gnorm=0.809, train_wall=22, gb_free=7, wall=2459
2021-03-10 21:27:48 | INFO | train_inner | epoch 010:    873 / 1103 loss=4.088, nll_loss=2.7, ppl=6.5, wps=16008.1, ups=4.48, wpb=3572.1, bsz=133.9, num_updates=10800, lr=0.00030429, gnorm=0.832, train_wall=22, gb_free=6.8, wall=2482
2021-03-10 21:28:11 | INFO | train_inner | epoch 010:    973 / 1103 loss=4.073, nll_loss=2.684, ppl=6.43, wps=16171.6, ups=4.52, wpb=3580, bsz=153.5, num_updates=10900, lr=0.000302891, gnorm=0.861, train_wall=22, gb_free=7, wall=2504
2021-03-10 21:28:33 | INFO | train_inner | epoch 010:   1073 / 1103 loss=4.029, nll_loss=2.634, ppl=6.21, wps=16162.4, ups=4.43, wpb=3647.9, bsz=161.3, num_updates=11000, lr=0.000301511, gnorm=0.832, train_wall=22, gb_free=6.9, wall=2526
2021-03-10 21:28:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:28:44 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 4.157 | nll_loss 2.638 | ppl 6.22 | wps 48315.1 | wpb 2873.1 | bsz 115.6 | num_updates 11030 | best_loss 4.157
2021-03-10 21:28:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 11030 updates
2021-03-10 21:28:44 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:28:46 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:28:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 10 @ 11030 updates, score 4.157) (writing took 4.487572491168976 seconds)
2021-03-10 21:28:48 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2021-03-10 21:28:48 | INFO | train | epoch 010 | loss 4.056 | nll_loss 2.664 | ppl 6.34 | wps 15513.7 | ups 4.34 | wpb 3577.8 | bsz 145.3 | num_updates 11030 | lr 0.000301101 | gnorm 0.829 | train_wall 245 | gb_free 7.3 | wall 2541
2021-03-10 21:28:48 | INFO | fairseq.trainer | begin training epoch 11
2021-03-10 21:28:48 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:29:04 | INFO | train_inner | epoch 011:     70 / 1103 loss=4.001, nll_loss=2.602, ppl=6.07, wps=11600.8, ups=3.26, wpb=3557.4, bsz=143.8, num_updates=11100, lr=0.00030015, gnorm=0.837, train_wall=22, gb_free=6.7, wall=2557
2021-03-10 21:29:26 | INFO | train_inner | epoch 011:    170 / 1103 loss=3.98, nll_loss=2.575, ppl=5.96, wps=15760.9, ups=4.47, wpb=3526.9, bsz=135, num_updates=11200, lr=0.000298807, gnorm=0.817, train_wall=22, gb_free=7, wall=2579
2021-03-10 21:29:49 | INFO | train_inner | epoch 011:    270 / 1103 loss=3.956, nll_loss=2.548, ppl=5.85, wps=16153.8, ups=4.4, wpb=3672.5, bsz=139, num_updates=11300, lr=0.000297482, gnorm=0.783, train_wall=23, gb_free=6.6, wall=2602
2021-03-10 21:30:11 | INFO | train_inner | epoch 011:    370 / 1103 loss=3.935, nll_loss=2.526, ppl=5.76, wps=15993.1, ups=4.47, wpb=3579.1, bsz=161.4, num_updates=11400, lr=0.000296174, gnorm=0.809, train_wall=22, gb_free=7, wall=2624
2021-03-10 21:30:34 | INFO | train_inner | epoch 011:    470 / 1103 loss=3.934, nll_loss=2.525, ppl=5.76, wps=15759.7, ups=4.42, wpb=3568, bsz=160.2, num_updates=11500, lr=0.000294884, gnorm=0.832, train_wall=23, gb_free=6.8, wall=2647
2021-03-10 21:30:56 | INFO | train_inner | epoch 011:    570 / 1103 loss=3.946, nll_loss=2.54, ppl=5.81, wps=16367.1, ups=4.44, wpb=3685.7, bsz=161, num_updates=11600, lr=0.00029361, gnorm=0.776, train_wall=22, gb_free=7, wall=2670
2021-03-10 21:31:19 | INFO | train_inner | epoch 011:    670 / 1103 loss=4, nll_loss=2.6, ppl=6.06, wps=15712.2, ups=4.51, wpb=3482.1, bsz=144.8, num_updates=11700, lr=0.000292353, gnorm=0.843, train_wall=22, gb_free=7.2, wall=2692
2021-03-10 21:31:41 | INFO | train_inner | epoch 011:    770 / 1103 loss=4.009, nll_loss=2.611, ppl=6.11, wps=16015.1, ups=4.49, wpb=3566.6, bsz=143.9, num_updates=11800, lr=0.000291111, gnorm=0.823, train_wall=22, gb_free=7.4, wall=2714
2021-03-10 21:32:03 | INFO | train_inner | epoch 011:    870 / 1103 loss=4.001, nll_loss=2.602, ppl=6.07, wps=16045.6, ups=4.43, wpb=3619, bsz=144.1, num_updates=11900, lr=0.000289886, gnorm=0.802, train_wall=22, gb_free=7.1, wall=2737
2021-03-10 21:32:26 | INFO | train_inner | epoch 011:    970 / 1103 loss=4.043, nll_loss=2.649, ppl=6.27, wps=15811, ups=4.46, wpb=3545.8, bsz=124.2, num_updates=12000, lr=0.000288675, gnorm=0.834, train_wall=22, gb_free=7, wall=2759
2021-03-10 21:32:48 | INFO | train_inner | epoch 011:   1070 / 1103 loss=4.026, nll_loss=2.631, ppl=6.2, wps=15686.3, ups=4.47, wpb=3511.1, bsz=138.9, num_updates=12100, lr=0.00028748, gnorm=0.871, train_wall=22, gb_free=6.9, wall=2781
2021-03-10 21:32:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:32:59 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 4.094 | nll_loss 2.576 | ppl 5.96 | wps 48256.6 | wpb 2873.1 | bsz 115.6 | num_updates 12133 | best_loss 4.094
2021-03-10 21:32:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 12133 updates
2021-03-10 21:32:59 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:33:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:33:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 11 @ 12133 updates, score 4.094) (writing took 4.508799973875284 seconds)
2021-03-10 21:33:04 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2021-03-10 21:33:04 | INFO | train | epoch 011 | loss 3.981 | nll_loss 2.579 | ppl 5.97 | wps 15427.2 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 12133 | lr 0.000287089 | gnorm 0.818 | train_wall 246 | gb_free 7.5 | wall 2797
2021-03-10 21:33:04 | INFO | fairseq.trainer | begin training epoch 12
2021-03-10 21:33:04 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:33:19 | INFO | train_inner | epoch 012:     67 / 1103 loss=3.947, nll_loss=2.539, ppl=5.81, wps=11557.8, ups=3.26, wpb=3543.3, bsz=131, num_updates=12200, lr=0.000286299, gnorm=0.823, train_wall=22, gb_free=6.8, wall=2812
2021-03-10 21:33:42 | INFO | train_inner | epoch 012:    167 / 1103 loss=3.884, nll_loss=2.467, ppl=5.53, wps=15996.3, ups=4.42, wpb=3617.3, bsz=153, num_updates=12300, lr=0.000285133, gnorm=0.843, train_wall=22, gb_free=7, wall=2835
2021-03-10 21:34:04 | INFO | train_inner | epoch 012:    267 / 1103 loss=3.85, nll_loss=2.43, ppl=5.39, wps=15595.8, ups=4.44, wpb=3515.2, bsz=161.3, num_updates=12400, lr=0.000283981, gnorm=0.805, train_wall=22, gb_free=7.1, wall=2857
2021-03-10 21:34:27 | INFO | train_inner | epoch 012:    367 / 1103 loss=3.922, nll_loss=2.51, ppl=5.7, wps=16134.4, ups=4.44, wpb=3630.9, bsz=145.8, num_updates=12500, lr=0.000282843, gnorm=0.81, train_wall=22, gb_free=6.8, wall=2880
2021-03-10 21:34:49 | INFO | train_inner | epoch 012:    467 / 1103 loss=3.945, nll_loss=2.536, ppl=5.8, wps=15966.4, ups=4.45, wpb=3587, bsz=134.7, num_updates=12600, lr=0.000281718, gnorm=0.803, train_wall=22, gb_free=7.1, wall=2902
2021-03-10 21:35:11 | INFO | train_inner | epoch 012:    567 / 1103 loss=3.932, nll_loss=2.522, ppl=5.74, wps=15906.7, ups=4.45, wpb=3573.5, bsz=142.3, num_updates=12700, lr=0.000280607, gnorm=0.829, train_wall=22, gb_free=6.7, wall=2925
2021-03-10 21:35:34 | INFO | train_inner | epoch 012:    667 / 1103 loss=3.97, nll_loss=2.566, ppl=5.92, wps=15948, ups=4.54, wpb=3515, bsz=142.4, num_updates=12800, lr=0.000279508, gnorm=0.829, train_wall=22, gb_free=7.6, wall=2947
2021-03-10 21:35:56 | INFO | train_inner | epoch 012:    767 / 1103 loss=3.966, nll_loss=2.561, ppl=5.9, wps=15966.6, ups=4.48, wpb=3566, bsz=135.1, num_updates=12900, lr=0.000278423, gnorm=0.828, train_wall=22, gb_free=7, wall=2969
2021-03-10 21:36:19 | INFO | train_inner | epoch 012:    867 / 1103 loss=3.878, nll_loss=2.462, ppl=5.51, wps=16174.5, ups=4.4, wpb=3674.8, bsz=160.6, num_updates=13000, lr=0.00027735, gnorm=0.791, train_wall=23, gb_free=7.3, wall=2992
2021-03-10 21:36:41 | INFO | train_inner | epoch 012:    967 / 1103 loss=3.891, nll_loss=2.478, ppl=5.57, wps=16085, ups=4.41, wpb=3644.8, bsz=154.8, num_updates=13100, lr=0.000276289, gnorm=0.781, train_wall=23, gb_free=6.8, wall=3014
2021-03-10 21:37:03 | INFO | train_inner | epoch 012:   1067 / 1103 loss=3.976, nll_loss=2.573, ppl=5.95, wps=15712.6, ups=4.51, wpb=3487.3, bsz=130.6, num_updates=13200, lr=0.000275241, gnorm=0.852, train_wall=22, gb_free=7.2, wall=3037
2021-03-10 21:37:11 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:37:15 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 4.057 | nll_loss 2.536 | ppl 5.8 | wps 48357.8 | wpb 2873.1 | bsz 115.6 | num_updates 13236 | best_loss 4.057
2021-03-10 21:37:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 13236 updates
2021-03-10 21:37:15 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:37:18 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:37:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 12 @ 13236 updates, score 4.057) (writing took 4.941672507673502 seconds)
2021-03-10 21:37:20 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2021-03-10 21:37:20 | INFO | train | epoch 012 | loss 3.92 | nll_loss 2.509 | ppl 5.69 | wps 15395.3 | ups 4.3 | wpb 3577.8 | bsz 145.3 | num_updates 13236 | lr 0.000274866 | gnorm 0.818 | train_wall 246 | gb_free 7.1 | wall 3053
2021-03-10 21:37:20 | INFO | fairseq.trainer | begin training epoch 13
2021-03-10 21:37:20 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:37:35 | INFO | train_inner | epoch 013:     64 / 1103 loss=3.839, nll_loss=2.417, ppl=5.34, wps=11470.2, ups=3.19, wpb=3597.8, bsz=151.1, num_updates=13300, lr=0.000274204, gnorm=0.796, train_wall=22, gb_free=7, wall=3068
2021-03-10 21:37:57 | INFO | train_inner | epoch 013:    164 / 1103 loss=3.817, nll_loss=2.392, ppl=5.25, wps=16160.9, ups=4.47, wpb=3612, bsz=156.2, num_updates=13400, lr=0.000273179, gnorm=0.789, train_wall=22, gb_free=6.9, wall=3090
2021-03-10 21:38:19 | INFO | train_inner | epoch 013:    264 / 1103 loss=3.879, nll_loss=2.461, ppl=5.51, wps=15756.4, ups=4.49, wpb=3511.8, bsz=131, num_updates=13500, lr=0.000272166, gnorm=0.827, train_wall=22, gb_free=7.2, wall=3113
2021-03-10 21:38:42 | INFO | train_inner | epoch 013:    364 / 1103 loss=3.872, nll_loss=2.453, ppl=5.47, wps=15647.2, ups=4.47, wpb=3499.7, bsz=139.8, num_updates=13600, lr=0.000271163, gnorm=0.855, train_wall=22, gb_free=7.1, wall=3135
2021-03-10 21:39:04 | INFO | train_inner | epoch 013:    464 / 1103 loss=3.819, nll_loss=2.394, ppl=5.25, wps=16054.6, ups=4.41, wpb=3642.4, bsz=153.4, num_updates=13700, lr=0.000270172, gnorm=0.796, train_wall=23, gb_free=7.2, wall=3158
2021-03-10 21:39:27 | INFO | train_inner | epoch 013:    564 / 1103 loss=3.872, nll_loss=2.454, ppl=5.48, wps=15881.7, ups=4.43, wpb=3583.6, bsz=145, num_updates=13800, lr=0.000269191, gnorm=0.814, train_wall=22, gb_free=7.2, wall=3180
2021-03-10 21:39:49 | INFO | train_inner | epoch 013:    664 / 1103 loss=3.89, nll_loss=2.475, ppl=5.56, wps=16202.3, ups=4.47, wpb=3625.7, bsz=151.7, num_updates=13900, lr=0.000268221, gnorm=0.826, train_wall=22, gb_free=6.9, wall=3203
2021-03-10 21:40:12 | INFO | train_inner | epoch 013:    764 / 1103 loss=3.876, nll_loss=2.459, ppl=5.5, wps=15894.5, ups=4.45, wpb=3571.6, bsz=140.2, num_updates=14000, lr=0.000267261, gnorm=0.834, train_wall=22, gb_free=7.1, wall=3225
2021-03-10 21:40:34 | INFO | train_inner | epoch 013:    864 / 1103 loss=3.885, nll_loss=2.47, ppl=5.54, wps=15782, ups=4.48, wpb=3526.7, bsz=140.4, num_updates=14100, lr=0.000266312, gnorm=0.803, train_wall=22, gb_free=6.9, wall=3247
2021-03-10 21:40:57 | INFO | train_inner | epoch 013:    964 / 1103 loss=3.915, nll_loss=2.503, ppl=5.67, wps=15901.4, ups=4.46, wpb=3566.4, bsz=140.3, num_updates=14200, lr=0.000265372, gnorm=0.852, train_wall=22, gb_free=7, wall=3270
2021-03-10 21:41:19 | INFO | train_inner | epoch 013:   1064 / 1103 loss=3.884, nll_loss=2.47, ppl=5.54, wps=16179.3, ups=4.49, wpb=3601.1, bsz=148.5, num_updates=14300, lr=0.000264443, gnorm=0.804, train_wall=22, gb_free=6.9, wall=3292
2021-03-10 21:41:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:41:31 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 4.051 | nll_loss 2.522 | ppl 5.74 | wps 48265.9 | wpb 2873.1 | bsz 115.6 | num_updates 14339 | best_loss 4.051
2021-03-10 21:41:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 14339 updates
2021-03-10 21:41:31 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:41:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:41:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 13 @ 14339 updates, score 4.051) (writing took 4.76478186249733 seconds)
2021-03-10 21:41:36 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2021-03-10 21:41:36 | INFO | train | epoch 013 | loss 3.865 | nll_loss 2.446 | ppl 5.45 | wps 15414 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 14339 | lr 0.000264083 | gnorm 0.817 | train_wall 246 | gb_free 6.8 | wall 3309
2021-03-10 21:41:36 | INFO | fairseq.trainer | begin training epoch 14
2021-03-10 21:41:36 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:41:50 | INFO | train_inner | epoch 014:     61 / 1103 loss=3.808, nll_loss=2.382, ppl=5.21, wps=11466, ups=3.23, wpb=3555, bsz=147.1, num_updates=14400, lr=0.000263523, gnorm=0.825, train_wall=22, gb_free=7.2, wall=3323
2021-03-10 21:42:12 | INFO | train_inner | epoch 014:    161 / 1103 loss=3.807, nll_loss=2.379, ppl=5.2, wps=15799.8, ups=4.47, wpb=3535.4, bsz=135.8, num_updates=14500, lr=0.000262613, gnorm=0.807, train_wall=22, gb_free=6.8, wall=3345
2021-03-10 21:42:35 | INFO | train_inner | epoch 014:    261 / 1103 loss=3.762, nll_loss=2.329, ppl=5.02, wps=15737.2, ups=4.45, wpb=3533.9, bsz=158.9, num_updates=14600, lr=0.000261712, gnorm=0.825, train_wall=22, gb_free=7.2, wall=3368
2021-03-10 21:42:55 | INFO | train_inner | epoch 014:    361 / 1103 loss=3.836, nll_loss=2.411, ppl=5.32, wps=17253.9, ups=4.84, wpb=3566.4, bsz=132.4, num_updates=14700, lr=0.00026082, gnorm=0.842, train_wall=21, gb_free=6.7, wall=3389
2021-03-10 21:43:16 | INFO | train_inner | epoch 014:    461 / 1103 loss=3.817, nll_loss=2.391, ppl=5.25, wps=17265.3, ups=4.84, wpb=3567.7, bsz=144.4, num_updates=14800, lr=0.000259938, gnorm=0.83, train_wall=21, gb_free=7.5, wall=3409
2021-03-10 21:43:37 | INFO | train_inner | epoch 014:    561 / 1103 loss=3.838, nll_loss=2.414, ppl=5.33, wps=17273.7, ups=4.82, wpb=3584.4, bsz=137, num_updates=14900, lr=0.000259064, gnorm=0.83, train_wall=21, gb_free=6.6, wall=3430
2021-03-10 21:43:59 | INFO | train_inner | epoch 014:    661 / 1103 loss=3.789, nll_loss=2.361, ppl=5.14, wps=16338.1, ups=4.49, wpb=3638.5, bsz=158.9, num_updates=15000, lr=0.000258199, gnorm=0.785, train_wall=22, gb_free=6.9, wall=3452
2021-03-10 21:44:21 | INFO | train_inner | epoch 014:    761 / 1103 loss=3.837, nll_loss=2.415, ppl=5.33, wps=16391.9, ups=4.48, wpb=3656.4, bsz=145, num_updates=15100, lr=0.000257343, gnorm=0.789, train_wall=22, gb_free=7, wall=3475
2021-03-10 21:44:44 | INFO | train_inner | epoch 014:    861 / 1103 loss=3.798, nll_loss=2.371, ppl=5.17, wps=16026, ups=4.44, wpb=3611.2, bsz=156.5, num_updates=15200, lr=0.000256495, gnorm=0.799, train_wall=22, gb_free=6.8, wall=3497
2021-03-10 21:45:06 | INFO | train_inner | epoch 014:    961 / 1103 loss=3.872, nll_loss=2.455, ppl=5.48, wps=16088.9, ups=4.49, wpb=3579.7, bsz=133.7, num_updates=15300, lr=0.000255655, gnorm=0.843, train_wall=22, gb_free=6.9, wall=3519
2021-03-10 21:45:29 | INFO | train_inner | epoch 014:   1061 / 1103 loss=3.86, nll_loss=2.442, ppl=5.44, wps=16000.8, ups=4.48, wpb=3570.4, bsz=144.6, num_updates=15400, lr=0.000254824, gnorm=0.817, train_wall=22, gb_free=7.1, wall=3542
2021-03-10 21:45:38 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:45:42 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 4.016 | nll_loss 2.481 | ppl 5.58 | wps 48271.2 | wpb 2873.1 | bsz 115.6 | num_updates 15442 | best_loss 4.016
2021-03-10 21:45:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 15442 updates
2021-03-10 21:45:42 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:45:45 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:45:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 14 @ 15442 updates, score 4.016) (writing took 4.650046944618225 seconds)
2021-03-10 21:45:46 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2021-03-10 21:45:46 | INFO | train | epoch 014 | loss 3.818 | nll_loss 2.393 | ppl 5.25 | wps 15778.6 | ups 4.41 | wpb 3577.8 | bsz 145.3 | num_updates 15442 | lr 0.000254477 | gnorm 0.818 | train_wall 240 | gb_free 7.2 | wall 3559
2021-03-10 21:45:46 | INFO | fairseq.trainer | begin training epoch 15
2021-03-10 21:45:46 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:45:59 | INFO | train_inner | epoch 015:     58 / 1103 loss=3.761, nll_loss=2.328, ppl=5.02, wps=11372.2, ups=3.25, wpb=3504.3, bsz=145.4, num_updates=15500, lr=0.000254, gnorm=0.804, train_wall=22, gb_free=7.4, wall=3572
2021-03-10 21:46:22 | INFO | train_inner | epoch 015:    158 / 1103 loss=3.768, nll_loss=2.334, ppl=5.04, wps=15868.2, ups=4.49, wpb=3537.3, bsz=139.4, num_updates=15600, lr=0.000253185, gnorm=0.808, train_wall=22, gb_free=6.9, wall=3595
2021-03-10 21:46:44 | INFO | train_inner | epoch 015:    258 / 1103 loss=3.753, nll_loss=2.318, ppl=4.99, wps=15906.5, ups=4.42, wpb=3601.2, bsz=142, num_updates=15700, lr=0.000252377, gnorm=0.794, train_wall=23, gb_free=6.8, wall=3617
2021-03-10 21:47:07 | INFO | train_inner | epoch 015:    358 / 1103 loss=3.762, nll_loss=2.328, ppl=5.02, wps=16113.3, ups=4.45, wpb=3624.2, bsz=145.8, num_updates=15800, lr=0.000251577, gnorm=0.805, train_wall=22, gb_free=7, wall=3640
2021-03-10 21:47:29 | INFO | train_inner | epoch 015:    458 / 1103 loss=3.751, nll_loss=2.317, ppl=4.98, wps=15896.3, ups=4.49, wpb=3541, bsz=161.6, num_updates=15900, lr=0.000250785, gnorm=0.832, train_wall=22, gb_free=6.6, wall=3662
2021-03-10 21:47:51 | INFO | train_inner | epoch 015:    558 / 1103 loss=3.803, nll_loss=2.374, ppl=5.18, wps=16097.1, ups=4.49, wpb=3583.3, bsz=137, num_updates=16000, lr=0.00025, gnorm=0.813, train_wall=22, gb_free=7.3, wall=3684
2021-03-10 21:48:14 | INFO | train_inner | epoch 015:    658 / 1103 loss=3.76, nll_loss=2.327, ppl=5.02, wps=16011.6, ups=4.48, wpb=3576.7, bsz=150.7, num_updates=16100, lr=0.000249222, gnorm=0.808, train_wall=22, gb_free=7.2, wall=3707
2021-03-10 21:48:36 | INFO | train_inner | epoch 015:    758 / 1103 loss=3.805, nll_loss=2.377, ppl=5.19, wps=16284.5, ups=4.45, wpb=3660.3, bsz=137.4, num_updates=16200, lr=0.000248452, gnorm=0.798, train_wall=22, gb_free=7.1, wall=3729
2021-03-10 21:48:59 | INFO | train_inner | epoch 015:    858 / 1103 loss=3.775, nll_loss=2.344, ppl=5.08, wps=15686.1, ups=4.44, wpb=3529.2, bsz=140.9, num_updates=16300, lr=0.000247689, gnorm=0.828, train_wall=22, gb_free=6.8, wall=3752
2021-03-10 21:49:21 | INFO | train_inner | epoch 015:    958 / 1103 loss=3.771, nll_loss=2.34, ppl=5.06, wps=15772.3, ups=4.46, wpb=3538.2, bsz=159.3, num_updates=16400, lr=0.000246932, gnorm=0.831, train_wall=22, gb_free=6.8, wall=3774
2021-03-10 21:49:44 | INFO | train_inner | epoch 015:   1058 / 1103 loss=3.777, nll_loss=2.346, ppl=5.08, wps=16119.8, ups=4.42, wpb=3645.8, bsz=146.9, num_updates=16500, lr=0.000246183, gnorm=0.81, train_wall=23, gb_free=7, wall=3797
2021-03-10 21:49:54 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:49:57 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 3.988 | nll_loss 2.453 | ppl 5.48 | wps 48241.8 | wpb 2873.1 | bsz 115.6 | num_updates 16545 | best_loss 3.988
2021-03-10 21:49:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 16545 updates
2021-03-10 21:49:57 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:49:59 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:50:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 15 @ 16545 updates, score 3.988) (writing took 4.396377939730883 seconds)
2021-03-10 21:50:02 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2021-03-10 21:50:02 | INFO | train | epoch 015 | loss 3.773 | nll_loss 2.341 | ppl 5.07 | wps 15448.2 | ups 4.32 | wpb 3577.8 | bsz 145.3 | num_updates 16545 | lr 0.000245848 | gnorm 0.813 | train_wall 246 | gb_free 6.9 | wall 3815
2021-03-10 21:50:02 | INFO | fairseq.trainer | begin training epoch 16
2021-03-10 21:50:02 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:50:14 | INFO | train_inner | epoch 016:     55 / 1103 loss=3.744, nll_loss=2.309, ppl=4.96, wps=11673.1, ups=3.28, wpb=3562.8, bsz=139.8, num_updates=16600, lr=0.00024544, gnorm=0.822, train_wall=22, gb_free=7, wall=3827
2021-03-10 21:50:36 | INFO | train_inner | epoch 016:    155 / 1103 loss=3.732, nll_loss=2.294, ppl=4.9, wps=16085.7, ups=4.51, wpb=3567.5, bsz=140.3, num_updates=16700, lr=0.000244704, gnorm=0.825, train_wall=22, gb_free=7, wall=3850
2021-03-10 21:50:59 | INFO | train_inner | epoch 016:    255 / 1103 loss=3.714, nll_loss=2.274, ppl=4.84, wps=15927.5, ups=4.44, wpb=3589.9, bsz=145, num_updates=16800, lr=0.000243975, gnorm=0.828, train_wall=22, gb_free=6.6, wall=3872
2021-03-10 21:51:21 | INFO | train_inner | epoch 016:    355 / 1103 loss=3.732, nll_loss=2.293, ppl=4.9, wps=15777.9, ups=4.5, wpb=3505.2, bsz=137, num_updates=16900, lr=0.000243252, gnorm=0.847, train_wall=22, gb_free=7.1, wall=3894
2021-03-10 21:51:44 | INFO | train_inner | epoch 016:    455 / 1103 loss=3.746, nll_loss=2.308, ppl=4.95, wps=15906.4, ups=4.46, wpb=3564, bsz=133.8, num_updates=17000, lr=0.000242536, gnorm=0.821, train_wall=22, gb_free=7.1, wall=3917
2021-03-10 21:52:06 | INFO | train_inner | epoch 016:    555 / 1103 loss=3.685, nll_loss=2.242, ppl=4.73, wps=16184.9, ups=4.42, wpb=3662.9, bsz=160.4, num_updates=17100, lr=0.000241825, gnorm=0.789, train_wall=23, gb_free=7, wall=3939
2021-03-10 21:52:29 | INFO | train_inner | epoch 016:    655 / 1103 loss=3.744, nll_loss=2.308, ppl=4.95, wps=15944.4, ups=4.46, wpb=3574.4, bsz=150, num_updates=17200, lr=0.000241121, gnorm=0.839, train_wall=22, gb_free=7.2, wall=3962
2021-03-10 21:52:51 | INFO | train_inner | epoch 016:    755 / 1103 loss=3.745, nll_loss=2.309, ppl=4.96, wps=16022.5, ups=4.44, wpb=3607.9, bsz=145.8, num_updates=17300, lr=0.000240424, gnorm=0.809, train_wall=22, gb_free=7.3, wall=3984
2021-03-10 21:53:14 | INFO | train_inner | epoch 016:    855 / 1103 loss=3.814, nll_loss=2.388, ppl=5.23, wps=15806, ups=4.46, wpb=3546.3, bsz=132.1, num_updates=17400, lr=0.000239732, gnorm=0.859, train_wall=22, gb_free=7.4, wall=4007
2021-03-10 21:53:36 | INFO | train_inner | epoch 016:    955 / 1103 loss=3.716, nll_loss=2.277, ppl=4.85, wps=15924.6, ups=4.43, wpb=3591, bsz=163.3, num_updates=17500, lr=0.000239046, gnorm=0.82, train_wall=22, gb_free=6.9, wall=4029
2021-03-10 21:53:59 | INFO | train_inner | epoch 016:   1055 / 1103 loss=3.724, nll_loss=2.287, ppl=4.88, wps=16106.8, ups=4.44, wpb=3628.8, bsz=159, num_updates=17600, lr=0.000238366, gnorm=0.788, train_wall=22, gb_free=7, wall=4052
2021-03-10 21:54:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:54:13 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 3.986 | nll_loss 2.446 | ppl 5.45 | wps 48258.4 | wpb 2873.1 | bsz 115.6 | num_updates 17648 | best_loss 3.986
2021-03-10 21:54:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 17648 updates
2021-03-10 21:54:13 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:54:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:54:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 16 @ 17648 updates, score 3.986) (writing took 4.75147071108222 seconds)
2021-03-10 21:54:18 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2021-03-10 21:54:18 | INFO | train | epoch 016 | loss 3.736 | nll_loss 2.299 | ppl 4.92 | wps 15404.6 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 17648 | lr 0.000238041 | gnorm 0.823 | train_wall 246 | gb_free 7.1 | wall 4071
2021-03-10 21:54:18 | INFO | fairseq.trainer | begin training epoch 17
2021-03-10 21:54:18 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:54:30 | INFO | train_inner | epoch 017:     52 / 1103 loss=3.718, nll_loss=2.276, ppl=4.84, wps=11268, ups=3.21, wpb=3513.5, bsz=131.4, num_updates=17700, lr=0.000237691, gnorm=0.824, train_wall=22, gb_free=7.1, wall=4083
2021-03-10 21:54:52 | INFO | train_inner | epoch 017:    152 / 1103 loss=3.696, nll_loss=2.252, ppl=4.76, wps=16025, ups=4.46, wpb=3595.6, bsz=140.9, num_updates=17800, lr=0.000237023, gnorm=0.804, train_wall=22, gb_free=7, wall=4105
2021-03-10 21:55:15 | INFO | train_inner | epoch 017:    252 / 1103 loss=3.646, nll_loss=2.195, ppl=4.58, wps=15689.5, ups=4.46, wpb=3518.5, bsz=150.3, num_updates=17900, lr=0.00023636, gnorm=0.803, train_wall=22, gb_free=7.1, wall=4128
2021-03-10 21:55:37 | INFO | train_inner | epoch 017:    352 / 1103 loss=3.656, nll_loss=2.207, ppl=4.62, wps=15986.2, ups=4.43, wpb=3606.9, bsz=156.2, num_updates=18000, lr=0.000235702, gnorm=0.808, train_wall=22, gb_free=6.7, wall=4150
2021-03-10 21:56:00 | INFO | train_inner | epoch 017:    452 / 1103 loss=3.664, nll_loss=2.217, ppl=4.65, wps=16055.9, ups=4.43, wpb=3622.6, bsz=150.4, num_updates=18100, lr=0.00023505, gnorm=0.797, train_wall=22, gb_free=6.9, wall=4173
2021-03-10 21:56:22 | INFO | train_inner | epoch 017:    552 / 1103 loss=3.689, nll_loss=2.244, ppl=4.74, wps=16073.7, ups=4.45, wpb=3613.3, bsz=142.2, num_updates=18200, lr=0.000234404, gnorm=0.803, train_wall=22, gb_free=7.4, wall=4195
2021-03-10 21:56:45 | INFO | train_inner | epoch 017:    652 / 1103 loss=3.692, nll_loss=2.249, ppl=4.75, wps=15972.5, ups=4.41, wpb=3621.3, bsz=152.6, num_updates=18300, lr=0.000233762, gnorm=0.813, train_wall=23, gb_free=6.8, wall=4218
2021-03-10 21:57:07 | INFO | train_inner | epoch 017:    752 / 1103 loss=3.709, nll_loss=2.269, ppl=4.82, wps=15790.1, ups=4.44, wpb=3553.9, bsz=147.8, num_updates=18400, lr=0.000233126, gnorm=0.826, train_wall=22, gb_free=7.2, wall=4241
2021-03-10 21:57:30 | INFO | train_inner | epoch 017:    852 / 1103 loss=3.715, nll_loss=2.277, ppl=4.85, wps=15957.8, ups=4.48, wpb=3563.8, bsz=155.4, num_updates=18500, lr=0.000232495, gnorm=0.827, train_wall=22, gb_free=7.3, wall=4263
2021-03-10 21:57:52 | INFO | train_inner | epoch 017:    952 / 1103 loss=3.775, nll_loss=2.342, ppl=5.07, wps=15938.1, ups=4.47, wpb=3567, bsz=125.4, num_updates=18600, lr=0.000231869, gnorm=0.833, train_wall=22, gb_free=7.1, wall=4285
2021-03-10 21:58:14 | INFO | train_inner | epoch 017:   1052 / 1103 loss=3.715, nll_loss=2.276, ppl=4.84, wps=16193.1, ups=4.51, wpb=3592.5, bsz=153.6, num_updates=18700, lr=0.000231249, gnorm=0.828, train_wall=22, gb_free=7.1, wall=4307
2021-03-10 21:58:25 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 21:58:29 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 3.984 | nll_loss 2.446 | ppl 5.45 | wps 48233.4 | wpb 2873.1 | bsz 115.6 | num_updates 18751 | best_loss 3.984
2021-03-10 21:58:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 18751 updates
2021-03-10 21:58:29 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:58:31 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 21:58:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 17 @ 18751 updates, score 3.984) (writing took 4.591665800660849 seconds)
2021-03-10 21:58:34 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2021-03-10 21:58:34 | INFO | train | epoch 017 | loss 3.699 | nll_loss 2.257 | ppl 4.78 | wps 15416.4 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 18751 | lr 0.000230934 | gnorm 0.818 | train_wall 246 | gb_free 6.9 | wall 4327
2021-03-10 21:58:34 | INFO | fairseq.trainer | begin training epoch 18
2021-03-10 21:58:34 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 21:58:45 | INFO | train_inner | epoch 018:     49 / 1103 loss=3.737, nll_loss=2.298, ppl=4.92, wps=11404.5, ups=3.26, wpb=3498.3, bsz=117.5, num_updates=18800, lr=0.000230633, gnorm=0.859, train_wall=22, gb_free=6.9, wall=4338
2021-03-10 21:59:07 | INFO | train_inner | epoch 018:    149 / 1103 loss=3.685, nll_loss=2.238, ppl=4.72, wps=16078.5, ups=4.5, wpb=3575.4, bsz=127.5, num_updates=18900, lr=0.000230022, gnorm=0.831, train_wall=22, gb_free=7, wall=4360
2021-03-10 21:59:30 | INFO | train_inner | epoch 018:    249 / 1103 loss=3.661, nll_loss=2.211, ppl=4.63, wps=15776.3, ups=4.44, wpb=3553.3, bsz=134.2, num_updates=19000, lr=0.000229416, gnorm=0.816, train_wall=22, gb_free=7, wall=4383
2021-03-10 21:59:52 | INFO | train_inner | epoch 018:    349 / 1103 loss=3.634, nll_loss=2.181, ppl=4.54, wps=16092.3, ups=4.45, wpb=3614.2, bsz=153.4, num_updates=19100, lr=0.000228814, gnorm=0.816, train_wall=22, gb_free=7.1, wall=4405
2021-03-10 22:00:15 | INFO | train_inner | epoch 018:    449 / 1103 loss=3.681, nll_loss=2.235, ppl=4.71, wps=15722.9, ups=4.48, wpb=3513.4, bsz=138.2, num_updates=19200, lr=0.000228218, gnorm=0.843, train_wall=22, gb_free=7.1, wall=4428
2021-03-10 22:00:37 | INFO | train_inner | epoch 018:    549 / 1103 loss=3.621, nll_loss=2.167, ppl=4.49, wps=15771.5, ups=4.42, wpb=3568.1, bsz=150.7, num_updates=19300, lr=0.000227626, gnorm=0.801, train_wall=23, gb_free=7, wall=4450
2021-03-10 22:01:00 | INFO | train_inner | epoch 018:    649 / 1103 loss=3.655, nll_loss=2.207, ppl=4.62, wps=16002.4, ups=4.46, wpb=3589, bsz=155.4, num_updates=19400, lr=0.000227038, gnorm=0.807, train_wall=22, gb_free=6.8, wall=4473
2021-03-10 22:01:22 | INFO | train_inner | epoch 018:    749 / 1103 loss=3.615, nll_loss=2.163, ppl=4.48, wps=16086.9, ups=4.45, wpb=3617.4, bsz=170.9, num_updates=19500, lr=0.000226455, gnorm=0.797, train_wall=22, gb_free=6.9, wall=4495
2021-03-10 22:01:45 | INFO | train_inner | epoch 018:    849 / 1103 loss=3.682, nll_loss=2.238, ppl=4.72, wps=16081.2, ups=4.46, wpb=3605.5, bsz=147.5, num_updates=19600, lr=0.000225877, gnorm=0.815, train_wall=22, gb_free=7.2, wall=4518
2021-03-10 22:02:07 | INFO | train_inner | epoch 018:    949 / 1103 loss=3.702, nll_loss=2.261, ppl=4.79, wps=15832.5, ups=4.43, wpb=3575.8, bsz=144.7, num_updates=19700, lr=0.000225303, gnorm=0.86, train_wall=22, gb_free=6.9, wall=4540
2021-03-10 22:02:29 | INFO | train_inner | epoch 018:   1049 / 1103 loss=3.709, nll_loss=2.27, ppl=4.82, wps=16092.5, ups=4.48, wpb=3591.6, bsz=147.4, num_updates=19800, lr=0.000224733, gnorm=0.845, train_wall=22, gb_free=6.9, wall=4563
2021-03-10 22:02:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:02:45 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 3.972 | nll_loss 2.431 | ppl 5.39 | wps 48218.3 | wpb 2873.1 | bsz 115.6 | num_updates 19854 | best_loss 3.972
2021-03-10 22:02:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 19854 updates
2021-03-10 22:02:45 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:02:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:02:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 18 @ 19854 updates, score 3.972) (writing took 4.400252193212509 seconds)
2021-03-10 22:02:50 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2021-03-10 22:02:50 | INFO | train | epoch 018 | loss 3.667 | nll_loss 2.22 | ppl 4.66 | wps 15427.2 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 19854 | lr 0.000224427 | gnorm 0.823 | train_wall 246 | gb_free 7.2 | wall 4583
2021-03-10 22:02:50 | INFO | fairseq.trainer | begin training epoch 19
2021-03-10 22:02:50 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:03:00 | INFO | train_inner | epoch 019:     46 / 1103 loss=3.661, nll_loss=2.213, ppl=4.64, wps=11570.9, ups=3.26, wpb=3553.7, bsz=135.5, num_updates=19900, lr=0.000224168, gnorm=0.816, train_wall=22, gb_free=6.9, wall=4593
2021-03-10 22:03:23 | INFO | train_inner | epoch 019:    146 / 1103 loss=3.587, nll_loss=2.128, ppl=4.37, wps=15995.8, ups=4.44, wpb=3602.7, bsz=147.4, num_updates=20000, lr=0.000223607, gnorm=0.799, train_wall=22, gb_free=7.2, wall=4616
2021-03-10 22:03:45 | INFO | train_inner | epoch 019:    246 / 1103 loss=3.63, nll_loss=2.175, ppl=4.52, wps=15889.4, ups=4.42, wpb=3595.2, bsz=131.7, num_updates=20100, lr=0.00022305, gnorm=0.82, train_wall=23, gb_free=7, wall=4638
2021-03-10 22:04:08 | INFO | train_inner | epoch 019:    346 / 1103 loss=3.614, nll_loss=2.158, ppl=4.46, wps=15812.6, ups=4.49, wpb=3525.2, bsz=147.4, num_updates=20200, lr=0.000222497, gnorm=0.828, train_wall=22, gb_free=7.5, wall=4661
2021-03-10 22:04:30 | INFO | train_inner | epoch 019:    446 / 1103 loss=3.63, nll_loss=2.177, ppl=4.52, wps=15853.8, ups=4.49, wpb=3531.7, bsz=145.1, num_updates=20300, lr=0.000221948, gnorm=0.829, train_wall=22, gb_free=6.9, wall=4683
2021-03-10 22:04:52 | INFO | train_inner | epoch 019:    546 / 1103 loss=3.668, nll_loss=2.221, ppl=4.66, wps=16001, ups=4.57, wpb=3503, bsz=138.2, num_updates=20400, lr=0.000221404, gnorm=0.83, train_wall=22, gb_free=7, wall=4705
2021-03-10 22:05:14 | INFO | train_inner | epoch 019:    646 / 1103 loss=3.641, nll_loss=2.191, ppl=4.57, wps=16291.6, ups=4.42, wpb=3681.9, bsz=155.8, num_updates=20500, lr=0.000220863, gnorm=0.81, train_wall=22, gb_free=6.9, wall=4728
2021-03-10 22:05:37 | INFO | train_inner | epoch 019:    746 / 1103 loss=3.679, nll_loss=2.232, ppl=4.7, wps=16040.3, ups=4.51, wpb=3558.2, bsz=135.8, num_updates=20600, lr=0.000220326, gnorm=0.85, train_wall=22, gb_free=6.9, wall=4750
2021-03-10 22:05:59 | INFO | train_inner | epoch 019:    846 / 1103 loss=3.637, nll_loss=2.186, ppl=4.55, wps=15884.3, ups=4.42, wpb=3595.4, bsz=140.2, num_updates=20700, lr=0.000219793, gnorm=0.83, train_wall=23, gb_free=6.9, wall=4772
2021-03-10 22:06:22 | INFO | train_inner | epoch 019:    946 / 1103 loss=3.65, nll_loss=2.203, ppl=4.6, wps=16130.3, ups=4.44, wpb=3632.5, bsz=158.2, num_updates=20800, lr=0.000219265, gnorm=0.806, train_wall=22, gb_free=7.4, wall=4795
2021-03-10 22:06:44 | INFO | train_inner | epoch 019:   1046 / 1103 loss=3.616, nll_loss=2.163, ppl=4.48, wps=15768.8, ups=4.44, wpb=3554.5, bsz=156.7, num_updates=20900, lr=0.000218739, gnorm=0.815, train_wall=22, gb_free=7.1, wall=4817
2021-03-10 22:06:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:07:01 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 3.964 | nll_loss 2.422 | ppl 5.36 | wps 48250.1 | wpb 2873.1 | bsz 115.6 | num_updates 20957 | best_loss 3.964
2021-03-10 22:07:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 20957 updates
2021-03-10 22:07:01 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:07:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:07:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 19 @ 20957 updates, score 3.964) (writing took 4.639871787279844 seconds)
2021-03-10 22:07:06 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2021-03-10 22:07:06 | INFO | train | epoch 019 | loss 3.635 | nll_loss 2.183 | ppl 4.54 | wps 15424.4 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 20957 | lr 0.000218442 | gnorm 0.823 | train_wall 246 | gb_free 7.1 | wall 4839
2021-03-10 22:07:06 | INFO | fairseq.trainer | begin training epoch 20
2021-03-10 22:07:06 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:07:15 | INFO | train_inner | epoch 020:     43 / 1103 loss=3.67, nll_loss=2.222, ppl=4.67, wps=11617.5, ups=3.22, wpb=3608.8, bsz=130.6, num_updates=21000, lr=0.000218218, gnorm=0.854, train_wall=22, gb_free=7, wall=4848
2021-03-10 22:07:38 | INFO | train_inner | epoch 020:    143 / 1103 loss=3.562, nll_loss=2.097, ppl=4.28, wps=15708.2, ups=4.44, wpb=3536.6, bsz=138.6, num_updates=21100, lr=0.0002177, gnorm=0.821, train_wall=22, gb_free=7, wall=4871
2021-03-10 22:08:00 | INFO | train_inner | epoch 020:    243 / 1103 loss=3.583, nll_loss=2.122, ppl=4.35, wps=16007.8, ups=4.42, wpb=3622.5, bsz=139.2, num_updates=21200, lr=0.000217186, gnorm=0.808, train_wall=23, gb_free=6.9, wall=4894
2021-03-10 22:08:23 | INFO | train_inner | epoch 020:    343 / 1103 loss=3.574, nll_loss=2.114, ppl=4.33, wps=16179.1, ups=4.42, wpb=3664.5, bsz=156.2, num_updates=21300, lr=0.000216676, gnorm=0.788, train_wall=23, gb_free=6.7, wall=4916
2021-03-10 22:08:46 | INFO | train_inner | epoch 020:    443 / 1103 loss=3.603, nll_loss=2.146, ppl=4.42, wps=16078, ups=4.4, wpb=3653.4, bsz=145.6, num_updates=21400, lr=0.000216169, gnorm=0.821, train_wall=23, gb_free=7, wall=4939
2021-03-10 22:09:08 | INFO | train_inner | epoch 020:    543 / 1103 loss=3.571, nll_loss=2.111, ppl=4.32, wps=15897.9, ups=4.45, wpb=3571.7, bsz=165, num_updates=21500, lr=0.000215666, gnorm=0.822, train_wall=22, gb_free=7.6, wall=4961
2021-03-10 22:09:31 | INFO | train_inner | epoch 020:    643 / 1103 loss=3.616, nll_loss=2.161, ppl=4.47, wps=15764.2, ups=4.48, wpb=3522.5, bsz=143.3, num_updates=21600, lr=0.000215166, gnorm=0.844, train_wall=22, gb_free=7.1, wall=4984
2021-03-10 22:09:53 | INFO | train_inner | epoch 020:    743 / 1103 loss=3.637, nll_loss=2.184, ppl=4.55, wps=15661.3, ups=4.49, wpb=3488.8, bsz=140.4, num_updates=21700, lr=0.000214669, gnorm=0.858, train_wall=22, gb_free=7.2, wall=5006
2021-03-10 22:10:15 | INFO | train_inner | epoch 020:    843 / 1103 loss=3.656, nll_loss=2.206, ppl=4.61, wps=15889.2, ups=4.48, wpb=3547.4, bsz=131.8, num_updates=21800, lr=0.000214176, gnorm=0.861, train_wall=22, gb_free=7.6, wall=5028
2021-03-10 22:10:38 | INFO | train_inner | epoch 020:    943 / 1103 loss=3.608, nll_loss=2.152, ppl=4.44, wps=15772.6, ups=4.41, wpb=3575.4, bsz=147, num_updates=21900, lr=0.000213687, gnorm=0.816, train_wall=23, gb_free=7.7, wall=5051
2021-03-10 22:11:00 | INFO | train_inner | epoch 020:   1043 / 1103 loss=3.623, nll_loss=2.172, ppl=4.51, wps=15998.4, ups=4.5, wpb=3555.5, bsz=163.6, num_updates=22000, lr=0.000213201, gnorm=0.83, train_wall=22, gb_free=7.3, wall=5073
2021-03-10 22:11:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:11:17 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 3.947 | nll_loss 2.405 | ppl 5.3 | wps 48183.1 | wpb 2873.1 | bsz 115.6 | num_updates 22060 | best_loss 3.947
2021-03-10 22:11:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 22060 updates
2021-03-10 22:11:17 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:11:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 20 @ 22060 updates, score 3.947) (writing took 4.949876539409161 seconds)
2021-03-10 22:11:22 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2021-03-10 22:11:22 | INFO | train | epoch 020 | loss 3.609 | nll_loss 2.153 | ppl 4.45 | wps 15369.5 | ups 4.3 | wpb 3577.8 | bsz 145.3 | num_updates 22060 | lr 0.000212911 | gnorm 0.828 | train_wall 247 | gb_free 7 | wall 5095
2021-03-10 22:11:22 | INFO | fairseq.trainer | begin training epoch 21
2021-03-10 22:11:22 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:11:32 | INFO | train_inner | epoch 021:     40 / 1103 loss=3.585, nll_loss=2.126, ppl=4.36, wps=11529.6, ups=3.19, wpb=3616.1, bsz=146.4, num_updates=22100, lr=0.000212718, gnorm=0.811, train_wall=22, gb_free=7, wall=5105
2021-03-10 22:11:54 | INFO | train_inner | epoch 021:    140 / 1103 loss=3.564, nll_loss=2.102, ppl=4.29, wps=15990.3, ups=4.46, wpb=3588.6, bsz=143.4, num_updates=22200, lr=0.000212238, gnorm=0.813, train_wall=22, gb_free=6.9, wall=5127
2021-03-10 22:12:16 | INFO | train_inner | epoch 021:    240 / 1103 loss=3.56, nll_loss=2.097, ppl=4.28, wps=15818.2, ups=4.5, wpb=3513.7, bsz=151.8, num_updates=22300, lr=0.000211762, gnorm=0.818, train_wall=22, gb_free=7, wall=5149
2021-03-10 22:12:39 | INFO | train_inner | epoch 021:    340 / 1103 loss=3.57, nll_loss=2.107, ppl=4.31, wps=15950.7, ups=4.46, wpb=3579.7, bsz=136.1, num_updates=22400, lr=0.000211289, gnorm=0.844, train_wall=22, gb_free=6.9, wall=5172
2021-03-10 22:13:01 | INFO | train_inner | epoch 021:    440 / 1103 loss=3.549, nll_loss=2.084, ppl=4.24, wps=15864.8, ups=4.43, wpb=3577.2, bsz=146.7, num_updates=22500, lr=0.000210819, gnorm=0.804, train_wall=22, gb_free=7.2, wall=5194
2021-03-10 22:13:24 | INFO | train_inner | epoch 021:    540 / 1103 loss=3.571, nll_loss=2.109, ppl=4.31, wps=15960.5, ups=4.41, wpb=3619.8, bsz=148.1, num_updates=22600, lr=0.000210352, gnorm=0.849, train_wall=23, gb_free=7.3, wall=5217
2021-03-10 22:13:46 | INFO | train_inner | epoch 021:    640 / 1103 loss=3.603, nll_loss=2.146, ppl=4.42, wps=15804.6, ups=4.47, wpb=3534.4, bsz=141.2, num_updates=22700, lr=0.000209888, gnorm=0.832, train_wall=22, gb_free=7, wall=5239
2021-03-10 22:14:09 | INFO | train_inner | epoch 021:    740 / 1103 loss=3.588, nll_loss=2.129, ppl=4.37, wps=15792.1, ups=4.45, wpb=3551.2, bsz=144.4, num_updates=22800, lr=0.000209427, gnorm=0.832, train_wall=22, gb_free=7.5, wall=5262
2021-03-10 22:14:31 | INFO | train_inner | epoch 021:    840 / 1103 loss=3.572, nll_loss=2.112, ppl=4.32, wps=16097, ups=4.44, wpb=3628.2, bsz=155.4, num_updates=22900, lr=0.000208969, gnorm=0.808, train_wall=22, gb_free=7.2, wall=5284
2021-03-10 22:14:54 | INFO | train_inner | epoch 021:    940 / 1103 loss=3.614, nll_loss=2.158, ppl=4.46, wps=15870, ups=4.44, wpb=3575.8, bsz=141.3, num_updates=23000, lr=0.000208514, gnorm=0.828, train_wall=22, gb_free=7.4, wall=5307
2021-03-10 22:15:16 | INFO | train_inner | epoch 021:   1040 / 1103 loss=3.577, nll_loss=2.119, ppl=4.35, wps=16038.2, ups=4.46, wpb=3596.7, bsz=162.2, num_updates=23100, lr=0.000208063, gnorm=0.825, train_wall=22, gb_free=7.4, wall=5329
2021-03-10 22:15:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:15:34 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 3.933 | nll_loss 2.389 | ppl 5.24 | wps 48279.7 | wpb 2873.1 | bsz 115.6 | num_updates 23163 | best_loss 3.933
2021-03-10 22:15:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 23163 updates
2021-03-10 22:15:34 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:15:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:15:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 21 @ 23163 updates, score 3.933) (writing took 4.594738505780697 seconds)
2021-03-10 22:15:39 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2021-03-10 22:15:39 | INFO | train | epoch 021 | loss 3.581 | nll_loss 2.121 | ppl 4.35 | wps 15399.5 | ups 4.3 | wpb 3577.8 | bsz 145.3 | num_updates 23163 | lr 0.000207779 | gnorm 0.826 | train_wall 246 | gb_free 6.7 | wall 5352
2021-03-10 22:15:39 | INFO | fairseq.trainer | begin training epoch 22
2021-03-10 22:15:39 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:15:47 | INFO | train_inner | epoch 022:     37 / 1103 loss=3.64, nll_loss=2.187, ppl=4.55, wps=11613.8, ups=3.23, wpb=3591.9, bsz=121.4, num_updates=23200, lr=0.000207614, gnorm=0.839, train_wall=22, gb_free=6.9, wall=5360
2021-03-10 22:16:10 | INFO | train_inner | epoch 022:    137 / 1103 loss=3.51, nll_loss=2.038, ppl=4.11, wps=16057.8, ups=4.44, wpb=3614.5, bsz=144.6, num_updates=23300, lr=0.000207168, gnorm=0.835, train_wall=22, gb_free=7.1, wall=5383
2021-03-10 22:16:32 | INFO | train_inner | epoch 022:    237 / 1103 loss=3.556, nll_loss=2.091, ppl=4.26, wps=16174.3, ups=4.46, wpb=3624.1, bsz=140.4, num_updates=23400, lr=0.000206725, gnorm=0.812, train_wall=22, gb_free=7, wall=5405
2021-03-10 22:16:54 | INFO | train_inner | epoch 022:    337 / 1103 loss=3.5, nll_loss=2.029, ppl=4.08, wps=15619.9, ups=4.48, wpb=3484.8, bsz=161.4, num_updates=23500, lr=0.000206284, gnorm=0.826, train_wall=22, gb_free=7, wall=5427
2021-03-10 22:17:17 | INFO | train_inner | epoch 022:    437 / 1103 loss=3.585, nll_loss=2.125, ppl=4.36, wps=16162.4, ups=4.5, wpb=3590.1, bsz=147.4, num_updates=23600, lr=0.000205847, gnorm=0.827, train_wall=22, gb_free=7, wall=5450
2021-03-10 22:17:39 | INFO | train_inner | epoch 022:    537 / 1103 loss=3.569, nll_loss=2.106, ppl=4.3, wps=15866.6, ups=4.48, wpb=3542.7, bsz=138.7, num_updates=23700, lr=0.000205412, gnorm=0.838, train_wall=22, gb_free=7, wall=5472
2021-03-10 22:18:01 | INFO | train_inner | epoch 022:    637 / 1103 loss=3.594, nll_loss=2.135, ppl=4.39, wps=15884.4, ups=4.49, wpb=3536.4, bsz=133.5, num_updates=23800, lr=0.00020498, gnorm=0.845, train_wall=22, gb_free=7.2, wall=5494
2021-03-10 22:18:24 | INFO | train_inner | epoch 022:    737 / 1103 loss=3.512, nll_loss=2.043, ppl=4.12, wps=15974.3, ups=4.4, wpb=3633.9, bsz=158.4, num_updates=23900, lr=0.000204551, gnorm=0.794, train_wall=23, gb_free=6.9, wall=5517
2021-03-10 22:18:46 | INFO | train_inner | epoch 022:    837 / 1103 loss=3.597, nll_loss=2.139, ppl=4.41, wps=15975.1, ups=4.47, wpb=3575.3, bsz=142.6, num_updates=24000, lr=0.000204124, gnorm=0.864, train_wall=22, gb_free=7.5, wall=5539
2021-03-10 22:19:09 | INFO | train_inner | epoch 022:    937 / 1103 loss=3.612, nll_loss=2.154, ppl=4.45, wps=15732.5, ups=4.44, wpb=3544.2, bsz=125.1, num_updates=24100, lr=0.0002037, gnorm=0.853, train_wall=22, gb_free=7.2, wall=5562
2021-03-10 22:19:31 | INFO | train_inner | epoch 022:   1037 / 1103 loss=3.561, nll_loss=2.098, ppl=4.28, wps=15859.3, ups=4.41, wpb=3595.1, bsz=149.9, num_updates=24200, lr=0.000203279, gnorm=0.85, train_wall=23, gb_free=7.2, wall=5585
2021-03-10 22:19:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:19:50 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 3.951 | nll_loss 2.401 | ppl 5.28 | wps 48156.4 | wpb 2873.1 | bsz 115.6 | num_updates 24266 | best_loss 3.933
2021-03-10 22:19:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 24266 updates
2021-03-10 22:19:50 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:19:52 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:19:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 22 @ 24266 updates, score 3.951) (writing took 2.12720188125968 seconds)
2021-03-10 22:19:52 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2021-03-10 22:19:52 | INFO | train | epoch 022 | loss 3.557 | nll_loss 2.093 | ppl 4.27 | wps 15549.9 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 24266 | lr 0.000203002 | gnorm 0.834 | train_wall 246 | gb_free 6.8 | wall 5606
2021-03-10 22:19:52 | INFO | fairseq.trainer | begin training epoch 23
2021-03-10 22:19:52 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:20:00 | INFO | train_inner | epoch 023:     34 / 1103 loss=3.522, nll_loss=2.055, ppl=4.15, wps=12553.6, ups=3.48, wpb=3612.2, bsz=158.6, num_updates=24300, lr=0.00020286, gnorm=0.827, train_wall=23, gb_free=7.2, wall=5613
2021-03-10 22:20:23 | INFO | train_inner | epoch 023:    134 / 1103 loss=3.515, nll_loss=2.043, ppl=4.12, wps=15938.6, ups=4.47, wpb=3562.5, bsz=139.1, num_updates=24400, lr=0.000202444, gnorm=0.825, train_wall=22, gb_free=7, wall=5636
2021-03-10 22:20:45 | INFO | train_inner | epoch 023:    234 / 1103 loss=3.484, nll_loss=2.008, ppl=4.02, wps=15897.3, ups=4.42, wpb=3597.2, bsz=146.2, num_updates=24500, lr=0.000202031, gnorm=0.814, train_wall=23, gb_free=7, wall=5658
2021-03-10 22:21:07 | INFO | train_inner | epoch 023:    334 / 1103 loss=3.531, nll_loss=2.063, ppl=4.18, wps=16477.4, ups=4.54, wpb=3630.6, bsz=142.7, num_updates=24600, lr=0.000201619, gnorm=0.834, train_wall=22, gb_free=7.1, wall=5680
2021-03-10 22:21:28 | INFO | train_inner | epoch 023:    434 / 1103 loss=3.504, nll_loss=2.034, ppl=4.1, wps=17694.7, ups=4.86, wpb=3642.9, bsz=161, num_updates=24700, lr=0.000201211, gnorm=0.81, train_wall=20, gb_free=6.9, wall=5701
2021-03-10 22:21:48 | INFO | train_inner | epoch 023:    534 / 1103 loss=3.566, nll_loss=2.101, ppl=4.29, wps=17204.3, ups=4.89, wpb=3517.7, bsz=131.9, num_updates=24800, lr=0.000200805, gnorm=0.868, train_wall=20, gb_free=6.8, wall=5721
2021-03-10 22:22:09 | INFO | train_inner | epoch 023:    634 / 1103 loss=3.57, nll_loss=2.106, ppl=4.3, wps=16929.4, ups=4.86, wpb=3484.3, bsz=124.5, num_updates=24900, lr=0.000200401, gnorm=0.869, train_wall=20, gb_free=7, wall=5742
2021-03-10 22:22:29 | INFO | train_inner | epoch 023:    734 / 1103 loss=3.556, nll_loss=2.091, ppl=4.26, wps=17534.3, ups=4.87, wpb=3600.1, bsz=150.1, num_updates=25000, lr=0.0002, gnorm=0.827, train_wall=20, gb_free=7, wall=5763
2021-03-10 22:22:50 | INFO | train_inner | epoch 023:    834 / 1103 loss=3.581, nll_loss=2.12, ppl=4.35, wps=17199.2, ups=4.82, wpb=3567.8, bsz=137.8, num_updates=25100, lr=0.000199601, gnorm=0.85, train_wall=21, gb_free=7.2, wall=5783
2021-03-10 22:23:11 | INFO | train_inner | epoch 023:    934 / 1103 loss=3.534, nll_loss=2.068, ppl=4.19, wps=17420.6, ups=4.89, wpb=3560.3, bsz=153.9, num_updates=25200, lr=0.000199205, gnorm=0.839, train_wall=20, gb_free=6.8, wall=5804
2021-03-10 22:23:31 | INFO | train_inner | epoch 023:   1034 / 1103 loss=3.53, nll_loss=2.064, ppl=4.18, wps=17421.4, ups=4.83, wpb=3603.6, bsz=158.6, num_updates=25300, lr=0.000198811, gnorm=0.837, train_wall=21, gb_free=7.2, wall=5824
2021-03-10 22:23:46 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:23:49 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 3.933 | nll_loss 2.379 | ppl 5.2 | wps 48138 | wpb 2873.1 | bsz 115.6 | num_updates 25369 | best_loss 3.933
2021-03-10 22:23:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 25369 updates
2021-03-10 22:23:49 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:23:51 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:23:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 23 @ 25369 updates, score 3.933) (writing took 4.501278501003981 seconds)
2021-03-10 22:23:54 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2021-03-10 22:23:54 | INFO | train | epoch 023 | loss 3.534 | nll_loss 2.067 | ppl 4.19 | wps 16338.3 | ups 4.57 | wpb 3577.8 | bsz 145.3 | num_updates 25369 | lr 0.00019854 | gnorm 0.835 | train_wall 232 | gb_free 7 | wall 5847
2021-03-10 22:23:54 | INFO | fairseq.trainer | begin training epoch 24
2021-03-10 22:23:54 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:24:01 | INFO | train_inner | epoch 024:     31 / 1103 loss=3.514, nll_loss=2.045, ppl=4.13, wps=11997, ups=3.36, wpb=3568.2, bsz=150, num_updates=25400, lr=0.000198419, gnorm=0.833, train_wall=21, gb_free=6.9, wall=5854
2021-03-10 22:24:24 | INFO | train_inner | epoch 024:    131 / 1103 loss=3.464, nll_loss=1.986, ppl=3.96, wps=15846.1, ups=4.43, wpb=3573.6, bsz=147.8, num_updates=25500, lr=0.00019803, gnorm=0.832, train_wall=22, gb_free=7, wall=5877
2021-03-10 22:24:46 | INFO | train_inner | epoch 024:    231 / 1103 loss=3.492, nll_loss=2.016, ppl=4.04, wps=15855.8, ups=4.42, wpb=3585.5, bsz=137.4, num_updates=25600, lr=0.000197642, gnorm=0.847, train_wall=22, gb_free=6.8, wall=5899
2021-03-10 22:25:09 | INFO | train_inner | epoch 024:    331 / 1103 loss=3.516, nll_loss=2.044, ppl=4.13, wps=16016, ups=4.46, wpb=3592.3, bsz=142.4, num_updates=25700, lr=0.000197257, gnorm=0.839, train_wall=22, gb_free=7.1, wall=5922
2021-03-10 22:25:31 | INFO | train_inner | epoch 024:    431 / 1103 loss=3.537, nll_loss=2.069, ppl=4.19, wps=15928.7, ups=4.45, wpb=3581.7, bsz=136.8, num_updates=25800, lr=0.000196875, gnorm=0.856, train_wall=22, gb_free=6.9, wall=5944
2021-03-10 22:25:54 | INFO | train_inner | epoch 024:    531 / 1103 loss=3.477, nll_loss=2.002, ppl=4, wps=15654.7, ups=4.46, wpb=3513, bsz=156.4, num_updates=25900, lr=0.000196494, gnorm=0.838, train_wall=22, gb_free=7, wall=5967
2021-03-10 22:26:16 | INFO | train_inner | epoch 024:    631 / 1103 loss=3.542, nll_loss=2.074, ppl=4.21, wps=16009.7, ups=4.42, wpb=3620.9, bsz=132.3, num_updates=26000, lr=0.000196116, gnorm=0.817, train_wall=23, gb_free=6.9, wall=5989
2021-03-10 22:26:39 | INFO | train_inner | epoch 024:    731 / 1103 loss=3.505, nll_loss=2.034, ppl=4.09, wps=15919.7, ups=4.47, wpb=3558.8, bsz=153.4, num_updates=26100, lr=0.00019574, gnorm=0.84, train_wall=22, gb_free=7, wall=6012
2021-03-10 22:27:01 | INFO | train_inner | epoch 024:    831 / 1103 loss=3.529, nll_loss=2.062, ppl=4.17, wps=15764.3, ups=4.45, wpb=3545.5, bsz=147.4, num_updates=26200, lr=0.000195366, gnorm=0.858, train_wall=22, gb_free=7, wall=6034
2021-03-10 22:27:23 | INFO | train_inner | epoch 024:    931 / 1103 loss=3.529, nll_loss=2.06, ppl=4.17, wps=15965.3, ups=4.45, wpb=3585.4, bsz=144.1, num_updates=26300, lr=0.000194994, gnorm=0.848, train_wall=22, gb_free=6.9, wall=6057
2021-03-10 22:27:46 | INFO | train_inner | epoch 024:   1031 / 1103 loss=3.506, nll_loss=2.036, ppl=4.1, wps=15926, ups=4.46, wpb=3569.3, bsz=158.1, num_updates=26400, lr=0.000194625, gnorm=0.824, train_wall=22, gb_free=7, wall=6079
2021-03-10 22:28:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:28:06 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 3.926 | nll_loss 2.379 | ppl 5.2 | wps 48235.1 | wpb 2873.1 | bsz 115.6 | num_updates 26472 | best_loss 3.926
2021-03-10 22:28:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 26472 updates
2021-03-10 22:28:06 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:28:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:28:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 24 @ 26472 updates, score 3.926) (writing took 5.120054736733437 seconds)
2021-03-10 22:28:11 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2021-03-10 22:28:11 | INFO | train | epoch 024 | loss 3.513 | nll_loss 2.042 | ppl 4.12 | wps 15362.6 | ups 4.29 | wpb 3577.8 | bsz 145.3 | num_updates 26472 | lr 0.00019436 | gnorm 0.839 | train_wall 247 | gb_free 7 | wall 6104
2021-03-10 22:28:11 | INFO | fairseq.trainer | begin training epoch 25
2021-03-10 22:28:11 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:28:17 | INFO | train_inner | epoch 025:     28 / 1103 loss=3.525, nll_loss=2.056, ppl=4.16, wps=11506, ups=3.18, wpb=3613.2, bsz=139.6, num_updates=26500, lr=0.000194257, gnorm=0.835, train_wall=22, gb_free=6.9, wall=6110
2021-03-10 22:28:40 | INFO | train_inner | epoch 025:    128 / 1103 loss=3.452, nll_loss=1.97, ppl=3.92, wps=16001.1, ups=4.46, wpb=3586.5, bsz=139.1, num_updates=26600, lr=0.000193892, gnorm=0.828, train_wall=22, gb_free=7.3, wall=6133
2021-03-10 22:29:02 | INFO | train_inner | epoch 025:    228 / 1103 loss=3.474, nll_loss=1.997, ppl=3.99, wps=16104.9, ups=4.45, wpb=3616, bsz=144.2, num_updates=26700, lr=0.000193528, gnorm=0.869, train_wall=22, gb_free=6.8, wall=6155
2021-03-10 22:29:25 | INFO | train_inner | epoch 025:    328 / 1103 loss=3.451, nll_loss=1.973, ppl=3.93, wps=15987.9, ups=4.44, wpb=3597.8, bsz=157.3, num_updates=26800, lr=0.000193167, gnorm=0.851, train_wall=22, gb_free=7.1, wall=6178
2021-03-10 22:29:47 | INFO | train_inner | epoch 025:    428 / 1103 loss=3.498, nll_loss=2.024, ppl=4.07, wps=15823.3, ups=4.44, wpb=3565.7, bsz=136.1, num_updates=26900, lr=0.000192807, gnorm=0.833, train_wall=22, gb_free=7.3, wall=6200
2021-03-10 22:30:10 | INFO | train_inner | epoch 025:    528 / 1103 loss=3.525, nll_loss=2.055, ppl=4.15, wps=16066.8, ups=4.45, wpb=3610.1, bsz=137.4, num_updates=27000, lr=0.00019245, gnorm=0.847, train_wall=22, gb_free=7.3, wall=6223
2021-03-10 22:30:32 | INFO | train_inner | epoch 025:    628 / 1103 loss=3.497, nll_loss=2.023, ppl=4.06, wps=15771.5, ups=4.47, wpb=3526.9, bsz=142.6, num_updates=27100, lr=0.000192095, gnorm=0.869, train_wall=22, gb_free=7.2, wall=6245
2021-03-10 22:30:54 | INFO | train_inner | epoch 025:    728 / 1103 loss=3.556, nll_loss=2.089, ppl=4.26, wps=16191.7, ups=4.48, wpb=3617.2, bsz=130.1, num_updates=27200, lr=0.000191741, gnorm=0.848, train_wall=22, gb_free=7.3, wall=6267
2021-03-10 22:31:17 | INFO | train_inner | epoch 025:    828 / 1103 loss=3.517, nll_loss=2.046, ppl=4.13, wps=15681.4, ups=4.47, wpb=3511.8, bsz=136.2, num_updates=27300, lr=0.00019139, gnorm=0.86, train_wall=22, gb_free=7.4, wall=6290
2021-03-10 22:31:39 | INFO | train_inner | epoch 025:    928 / 1103 loss=3.482, nll_loss=2.008, ppl=4.02, wps=16265.5, ups=4.42, wpb=3681.4, bsz=163.3, num_updates=27400, lr=0.00019104, gnorm=0.806, train_wall=23, gb_free=6.9, wall=6313
2021-03-10 22:32:02 | INFO | train_inner | epoch 025:   1028 / 1103 loss=3.479, nll_loss=2.007, ppl=4.02, wps=15719.2, ups=4.46, wpb=3526, bsz=170.4, num_updates=27500, lr=0.000190693, gnorm=0.845, train_wall=22, gb_free=7.2, wall=6335
2021-03-10 22:32:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:32:22 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 3.921 | nll_loss 2.373 | ppl 5.18 | wps 48288.7 | wpb 2873.1 | bsz 115.6 | num_updates 27575 | best_loss 3.921
2021-03-10 22:32:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 27575 updates
2021-03-10 22:32:22 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:32:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:32:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 25 @ 27575 updates, score 3.921) (writing took 4.592916831374168 seconds)
2021-03-10 22:32:27 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2021-03-10 22:32:27 | INFO | train | epoch 025 | loss 3.492 | nll_loss 2.018 | ppl 4.05 | wps 15416.3 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 27575 | lr 0.000190433 | gnorm 0.846 | train_wall 246 | gb_free 6.8 | wall 6360
2021-03-10 22:32:27 | INFO | fairseq.trainer | begin training epoch 26
2021-03-10 22:32:27 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:32:33 | INFO | train_inner | epoch 026:     25 / 1103 loss=3.479, nll_loss=2.005, ppl=4.01, wps=11597.5, ups=3.25, wpb=3570.8, bsz=153.5, num_updates=27600, lr=0.000190347, gnorm=0.838, train_wall=22, gb_free=6.8, wall=6366
2021-03-10 22:32:55 | INFO | train_inner | epoch 026:    125 / 1103 loss=3.424, nll_loss=1.94, ppl=3.84, wps=16250.5, ups=4.47, wpb=3638.7, bsz=151.9, num_updates=27700, lr=0.000190003, gnorm=0.807, train_wall=22, gb_free=6.9, wall=6388
2021-03-10 22:33:17 | INFO | train_inner | epoch 026:    225 / 1103 loss=3.435, nll_loss=1.952, ppl=3.87, wps=15655.5, ups=4.46, wpb=3513.4, bsz=147.1, num_updates=27800, lr=0.000189661, gnorm=0.885, train_wall=22, gb_free=6.9, wall=6411
2021-03-10 22:33:40 | INFO | train_inner | epoch 026:    325 / 1103 loss=3.472, nll_loss=1.994, ppl=3.98, wps=16108.9, ups=4.45, wpb=3622.7, bsz=135.6, num_updates=27900, lr=0.000189321, gnorm=0.836, train_wall=22, gb_free=6.9, wall=6433
2021-03-10 22:34:03 | INFO | train_inner | epoch 026:    425 / 1103 loss=3.472, nll_loss=1.994, ppl=3.98, wps=15966.1, ups=4.42, wpb=3612.3, bsz=140.9, num_updates=28000, lr=0.000188982, gnorm=0.836, train_wall=23, gb_free=7, wall=6456
2021-03-10 22:34:25 | INFO | train_inner | epoch 026:    525 / 1103 loss=3.499, nll_loss=2.025, ppl=4.07, wps=15955.5, ups=4.47, wpb=3570.6, bsz=132.6, num_updates=28100, lr=0.000188646, gnorm=0.879, train_wall=22, gb_free=7, wall=6478
2021-03-10 22:34:47 | INFO | train_inner | epoch 026:    625 / 1103 loss=3.492, nll_loss=2.017, ppl=4.05, wps=15849.3, ups=4.46, wpb=3551.4, bsz=145.5, num_updates=28200, lr=0.000188311, gnorm=0.85, train_wall=22, gb_free=6.4, wall=6500
2021-03-10 22:35:10 | INFO | train_inner | epoch 026:    725 / 1103 loss=3.491, nll_loss=2.016, ppl=4.04, wps=15795.1, ups=4.43, wpb=3562.6, bsz=133.1, num_updates=28300, lr=0.000187978, gnorm=0.844, train_wall=22, gb_free=6.8, wall=6523
2021-03-10 22:35:32 | INFO | train_inner | epoch 026:    825 / 1103 loss=3.478, nll_loss=2.001, ppl=4, wps=15971.2, ups=4.45, wpb=3585.2, bsz=146.8, num_updates=28400, lr=0.000187647, gnorm=0.84, train_wall=22, gb_free=6.9, wall=6545
2021-03-10 22:35:55 | INFO | train_inner | epoch 026:    925 / 1103 loss=3.453, nll_loss=1.976, ppl=3.94, wps=15770.4, ups=4.49, wpb=3515.6, bsz=169.8, num_updates=28500, lr=0.000187317, gnorm=0.845, train_wall=22, gb_free=7.1, wall=6568
2021-03-10 22:36:17 | INFO | train_inner | epoch 026:   1025 / 1103 loss=3.484, nll_loss=2.01, ppl=4.03, wps=16062.9, ups=4.43, wpb=3625.3, bsz=153.4, num_updates=28600, lr=0.000186989, gnorm=0.822, train_wall=22, gb_free=6.6, wall=6590
2021-03-10 22:36:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:36:38 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 3.921 | nll_loss 2.369 | ppl 5.17 | wps 48294.6 | wpb 2873.1 | bsz 115.6 | num_updates 28678 | best_loss 3.921
2021-03-10 22:36:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 28678 updates
2021-03-10 22:36:38 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:36:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:36:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 26 @ 28678 updates, score 3.921) (writing took 4.500081427395344 seconds)
2021-03-10 22:36:43 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2021-03-10 22:36:43 | INFO | train | epoch 026 | loss 3.473 | nll_loss 1.996 | ppl 3.99 | wps 15416.3 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 28678 | lr 0.000186735 | gnorm 0.846 | train_wall 246 | gb_free 6.4 | wall 6616
2021-03-10 22:36:43 | INFO | fairseq.trainer | begin training epoch 27
2021-03-10 22:36:43 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:36:48 | INFO | train_inner | epoch 027:     22 / 1103 loss=3.518, nll_loss=2.046, ppl=4.13, wps=11534.2, ups=3.26, wpb=3537.9, bsz=127, num_updates=28700, lr=0.000186663, gnorm=0.87, train_wall=22, gb_free=6.9, wall=6621
2021-03-10 22:37:10 | INFO | train_inner | epoch 027:    122 / 1103 loss=3.418, nll_loss=1.932, ppl=3.82, wps=15975.5, ups=4.46, wpb=3585, bsz=146.1, num_updates=28800, lr=0.000186339, gnorm=0.834, train_wall=22, gb_free=7, wall=6643
2021-03-10 22:37:33 | INFO | train_inner | epoch 027:    222 / 1103 loss=3.441, nll_loss=1.96, ppl=3.89, wps=15940.8, ups=4.48, wpb=3558.9, bsz=149.4, num_updates=28900, lr=0.000186016, gnorm=0.833, train_wall=22, gb_free=7.3, wall=6666
2021-03-10 22:37:55 | INFO | train_inner | epoch 027:    322 / 1103 loss=3.428, nll_loss=1.943, ppl=3.85, wps=15855.5, ups=4.47, wpb=3549.8, bsz=146.6, num_updates=29000, lr=0.000185695, gnorm=0.855, train_wall=22, gb_free=6.7, wall=6688
2021-03-10 22:38:18 | INFO | train_inner | epoch 027:    422 / 1103 loss=3.436, nll_loss=1.954, ppl=3.88, wps=16013.6, ups=4.43, wpb=3610.9, bsz=146.1, num_updates=29100, lr=0.000185376, gnorm=0.839, train_wall=22, gb_free=7, wall=6711
2021-03-10 22:38:40 | INFO | train_inner | epoch 027:    522 / 1103 loss=3.419, nll_loss=1.936, ppl=3.83, wps=16087.2, ups=4.46, wpb=3605.1, bsz=164.5, num_updates=29200, lr=0.000185058, gnorm=0.843, train_wall=22, gb_free=6.8, wall=6733
2021-03-10 22:39:02 | INFO | train_inner | epoch 027:    622 / 1103 loss=3.463, nll_loss=1.984, ppl=3.95, wps=15924.1, ups=4.46, wpb=3572.8, bsz=139.3, num_updates=29300, lr=0.000184742, gnorm=0.877, train_wall=22, gb_free=7.1, wall=6756
2021-03-10 22:39:25 | INFO | train_inner | epoch 027:    722 / 1103 loss=3.482, nll_loss=2.006, ppl=4.02, wps=15949.7, ups=4.44, wpb=3593.2, bsz=137.5, num_updates=29400, lr=0.000184428, gnorm=0.842, train_wall=22, gb_free=7.1, wall=6778
2021-03-10 22:39:48 | INFO | train_inner | epoch 027:    822 / 1103 loss=3.453, nll_loss=1.973, ppl=3.93, wps=15837.5, ups=4.43, wpb=3576.8, bsz=144.4, num_updates=29500, lr=0.000184115, gnorm=0.842, train_wall=22, gb_free=6.8, wall=6801
2021-03-10 22:40:10 | INFO | train_inner | epoch 027:    922 / 1103 loss=3.493, nll_loss=2.018, ppl=4.05, wps=15891.6, ups=4.46, wpb=3559.2, bsz=132.9, num_updates=29600, lr=0.000183804, gnorm=0.859, train_wall=22, gb_free=6.5, wall=6823
2021-03-10 22:40:32 | INFO | train_inner | epoch 027:   1022 / 1103 loss=3.478, nll_loss=2.003, ppl=4.01, wps=15755.2, ups=4.44, wpb=3549.2, bsz=151.9, num_updates=29700, lr=0.000183494, gnorm=0.856, train_wall=22, gb_free=7, wall=6846
2021-03-10 22:40:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:40:54 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 3.925 | nll_loss 2.373 | ppl 5.18 | wps 48250.1 | wpb 2873.1 | bsz 115.6 | num_updates 29781 | best_loss 3.921
2021-03-10 22:40:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 29781 updates
2021-03-10 22:40:54 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:40:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:40:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 27 @ 29781 updates, score 3.925) (writing took 2.144493907690048 seconds)
2021-03-10 22:40:57 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2021-03-10 22:40:57 | INFO | train | epoch 027 | loss 3.454 | nll_loss 1.974 | ppl 3.93 | wps 15550.1 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 29781 | lr 0.000183244 | gnorm 0.848 | train_wall 246 | gb_free 7.1 | wall 6870
2021-03-10 22:40:57 | INFO | fairseq.trainer | begin training epoch 28
2021-03-10 22:40:57 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:41:01 | INFO | train_inner | epoch 028:     19 / 1103 loss=3.476, nll_loss=2, ppl=4, wps=12591.3, ups=3.51, wpb=3591.8, bsz=141.4, num_updates=29800, lr=0.000183186, gnorm=0.853, train_wall=22, gb_free=6.6, wall=6874
2021-03-10 22:41:24 | INFO | train_inner | epoch 028:    119 / 1103 loss=3.394, nll_loss=1.905, ppl=3.74, wps=15987.2, ups=4.43, wpb=3605.8, bsz=146.4, num_updates=29900, lr=0.000182879, gnorm=0.829, train_wall=22, gb_free=6.9, wall=6897
2021-03-10 22:41:46 | INFO | train_inner | epoch 028:    219 / 1103 loss=3.429, nll_loss=1.945, ppl=3.85, wps=16279.8, ups=4.43, wpb=3675.1, bsz=144.9, num_updates=30000, lr=0.000182574, gnorm=0.816, train_wall=22, gb_free=7, wall=6919
2021-03-10 22:42:08 | INFO | train_inner | epoch 028:    319 / 1103 loss=3.421, nll_loss=1.936, ppl=3.83, wps=16074.5, ups=4.49, wpb=3581.3, bsz=151.3, num_updates=30100, lr=0.000182271, gnorm=0.852, train_wall=22, gb_free=7.2, wall=6942
2021-03-10 22:42:31 | INFO | train_inner | epoch 028:    419 / 1103 loss=3.45, nll_loss=1.967, ppl=3.91, wps=15835.5, ups=4.5, wpb=3517.9, bsz=124.2, num_updates=30200, lr=0.000181969, gnorm=0.897, train_wall=22, gb_free=6.9, wall=6964
2021-03-10 22:42:53 | INFO | train_inner | epoch 028:    519 / 1103 loss=3.444, nll_loss=1.961, ppl=3.89, wps=15737.8, ups=4.47, wpb=3517.8, bsz=136.2, num_updates=30300, lr=0.000181668, gnorm=0.864, train_wall=22, gb_free=7, wall=6986
2021-03-10 22:43:15 | INFO | train_inner | epoch 028:    619 / 1103 loss=3.429, nll_loss=1.946, ppl=3.85, wps=15662.6, ups=4.5, wpb=3481.6, bsz=146.6, num_updates=30400, lr=0.000181369, gnorm=0.864, train_wall=22, gb_free=6.9, wall=7008
2021-03-10 22:43:38 | INFO | train_inner | epoch 028:    719 / 1103 loss=3.43, nll_loss=1.947, ppl=3.86, wps=15856.3, ups=4.43, wpb=3581.9, bsz=155.5, num_updates=30500, lr=0.000181071, gnorm=0.834, train_wall=22, gb_free=7.4, wall=7031
2021-03-10 22:44:00 | INFO | train_inner | epoch 028:    819 / 1103 loss=3.45, nll_loss=1.969, ppl=3.92, wps=15875.8, ups=4.42, wpb=3588.1, bsz=141.7, num_updates=30600, lr=0.000180775, gnorm=0.837, train_wall=22, gb_free=7.2, wall=7054
2021-03-10 22:44:23 | INFO | train_inner | epoch 028:    919 / 1103 loss=3.451, nll_loss=1.971, ppl=3.92, wps=16040.9, ups=4.44, wpb=3613.4, bsz=146.5, num_updates=30700, lr=0.000180481, gnorm=0.835, train_wall=22, gb_free=7.1, wall=7076
2021-03-10 22:44:46 | INFO | train_inner | epoch 028:   1019 / 1103 loss=3.424, nll_loss=1.941, ppl=3.84, wps=15909.7, ups=4.41, wpb=3607.3, bsz=158.5, num_updates=30800, lr=0.000180187, gnorm=0.83, train_wall=23, gb_free=7.1, wall=7099
2021-03-10 22:45:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:45:08 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 3.944 | nll_loss 2.385 | ppl 5.22 | wps 48167.9 | wpb 2873.1 | bsz 115.6 | num_updates 30884 | best_loss 3.921
2021-03-10 22:45:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 30884 updates
2021-03-10 22:45:08 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:45:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:45:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 28 @ 30884 updates, score 3.944) (writing took 2.1298291869461536 seconds)
2021-03-10 22:45:10 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2021-03-10 22:45:10 | INFO | train | epoch 028 | loss 3.435 | nll_loss 1.952 | ppl 3.87 | wps 15544.3 | ups 4.34 | wpb 3577.8 | bsz 145.3 | num_updates 30884 | lr 0.000179942 | gnorm 0.848 | train_wall 247 | gb_free 7.2 | wall 7124
2021-03-10 22:45:10 | INFO | fairseq.trainer | begin training epoch 29
2021-03-10 22:45:10 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:45:14 | INFO | train_inner | epoch 029:     16 / 1103 loss=3.466, nll_loss=1.987, ppl=3.97, wps=12548.8, ups=3.5, wpb=3586.8, bsz=142.8, num_updates=30900, lr=0.000179896, gnorm=0.87, train_wall=22, gb_free=6.8, wall=7127
2021-03-10 22:45:37 | INFO | train_inner | epoch 029:    116 / 1103 loss=3.365, nll_loss=1.872, ppl=3.66, wps=15776.7, ups=4.44, wpb=3550.7, bsz=147.2, num_updates=31000, lr=0.000179605, gnorm=0.828, train_wall=22, gb_free=7.4, wall=7150
2021-03-10 22:45:59 | INFO | train_inner | epoch 029:    216 / 1103 loss=3.4, nll_loss=1.911, ppl=3.76, wps=15986.3, ups=4.44, wpb=3603.5, bsz=138.6, num_updates=31100, lr=0.000179316, gnorm=0.845, train_wall=22, gb_free=6.9, wall=7172
2021-03-10 22:46:22 | INFO | train_inner | epoch 029:    316 / 1103 loss=3.398, nll_loss=1.909, ppl=3.76, wps=15957.9, ups=4.46, wpb=3578.2, bsz=149.1, num_updates=31200, lr=0.000179029, gnorm=0.846, train_wall=22, gb_free=7.3, wall=7195
2021-03-10 22:46:44 | INFO | train_inner | epoch 029:    416 / 1103 loss=3.43, nll_loss=1.945, ppl=3.85, wps=16091.5, ups=4.44, wpb=3622.9, bsz=142.2, num_updates=31300, lr=0.000178743, gnorm=0.857, train_wall=22, gb_free=7.1, wall=7217
2021-03-10 22:47:07 | INFO | train_inner | epoch 029:    516 / 1103 loss=3.37, nll_loss=1.878, ppl=3.68, wps=15779.8, ups=4.44, wpb=3556.6, bsz=160.1, num_updates=31400, lr=0.000178458, gnorm=0.84, train_wall=22, gb_free=7.4, wall=7240
2021-03-10 22:47:29 | INFO | train_inner | epoch 029:    616 / 1103 loss=3.421, nll_loss=1.937, ppl=3.83, wps=16187.8, ups=4.45, wpb=3633.8, bsz=155.2, num_updates=31500, lr=0.000178174, gnorm=0.837, train_wall=22, gb_free=7, wall=7262
2021-03-10 22:47:52 | INFO | train_inner | epoch 029:    716 / 1103 loss=3.47, nll_loss=1.991, ppl=3.98, wps=16084.4, ups=4.46, wpb=3606.6, bsz=137.8, num_updates=31600, lr=0.000177892, gnorm=0.879, train_wall=22, gb_free=7.9, wall=7285
2021-03-10 22:48:14 | INFO | train_inner | epoch 029:    816 / 1103 loss=3.456, nll_loss=1.974, ppl=3.93, wps=15694, ups=4.49, wpb=3491.5, bsz=130.8, num_updates=31700, lr=0.000177611, gnorm=0.897, train_wall=22, gb_free=7.1, wall=7307
2021-03-10 22:48:36 | INFO | train_inner | epoch 029:    916 / 1103 loss=3.427, nll_loss=1.943, ppl=3.85, wps=15926.4, ups=4.45, wpb=3582.6, bsz=152.5, num_updates=31800, lr=0.000177332, gnorm=0.849, train_wall=22, gb_free=7, wall=7329
2021-03-10 22:48:59 | INFO | train_inner | epoch 029:   1016 / 1103 loss=3.42, nll_loss=1.937, ppl=3.83, wps=15904.6, ups=4.43, wpb=3587.6, bsz=155, num_updates=31900, lr=0.000177054, gnorm=0.851, train_wall=22, gb_free=7, wall=7352
2021-03-10 22:49:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:49:22 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 3.917 | nll_loss 2.369 | ppl 5.16 | wps 48175.2 | wpb 2873.1 | bsz 115.6 | num_updates 31987 | best_loss 3.917
2021-03-10 22:49:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 31987 updates
2021-03-10 22:49:22 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:49:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 22:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 29 @ 31987 updates, score 3.917) (writing took 4.64466156065464 seconds)
2021-03-10 22:49:27 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2021-03-10 22:49:27 | INFO | train | epoch 029 | loss 3.419 | nll_loss 1.933 | ppl 3.82 | wps 15389.7 | ups 4.3 | wpb 3577.8 | bsz 145.3 | num_updates 31987 | lr 0.000176813 | gnorm 0.854 | train_wall 247 | gb_free 6.9 | wall 7380
2021-03-10 22:49:27 | INFO | fairseq.trainer | begin training epoch 30
2021-03-10 22:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:49:30 | INFO | train_inner | epoch 030:     13 / 1103 loss=3.442, nll_loss=1.959, ppl=3.89, wps=11372.9, ups=3.22, wpb=3535.5, bsz=132.2, num_updates=32000, lr=0.000176777, gnorm=0.871, train_wall=22, gb_free=7.1, wall=7383
2021-03-10 22:49:53 | INFO | train_inner | epoch 030:    113 / 1103 loss=3.353, nll_loss=1.856, ppl=3.62, wps=15906.3, ups=4.42, wpb=3594.8, bsz=144.1, num_updates=32100, lr=0.000176501, gnorm=0.83, train_wall=22, gb_free=6.5, wall=7406
2021-03-10 22:50:15 | INFO | train_inner | epoch 030:    213 / 1103 loss=3.43, nll_loss=1.944, ppl=3.85, wps=16168.6, ups=4.49, wpb=3604.9, bsz=125.2, num_updates=32200, lr=0.000176227, gnorm=0.845, train_wall=22, gb_free=7.2, wall=7428
2021-03-10 22:50:37 | INFO | train_inner | epoch 030:    313 / 1103 loss=3.33, nll_loss=1.833, ppl=3.56, wps=15818.1, ups=4.45, wpb=3553, bsz=172.3, num_updates=32300, lr=0.000175954, gnorm=0.831, train_wall=22, gb_free=7, wall=7450
2021-03-10 22:51:00 | INFO | train_inner | epoch 030:    413 / 1103 loss=3.402, nll_loss=1.914, ppl=3.77, wps=16086.7, ups=4.44, wpb=3625.1, bsz=152.1, num_updates=32400, lr=0.000175682, gnorm=0.86, train_wall=22, gb_free=6.9, wall=7473
2021-03-10 22:51:22 | INFO | train_inner | epoch 030:    513 / 1103 loss=3.383, nll_loss=1.891, ppl=3.71, wps=15643.2, ups=4.45, wpb=3512.1, bsz=145.2, num_updates=32500, lr=0.000175412, gnorm=0.861, train_wall=22, gb_free=7, wall=7495
2021-03-10 22:51:45 | INFO | train_inner | epoch 030:    613 / 1103 loss=3.398, nll_loss=1.908, ppl=3.75, wps=15824.4, ups=4.45, wpb=3559, bsz=142.4, num_updates=32600, lr=0.000175142, gnorm=0.875, train_wall=22, gb_free=7, wall=7518
2021-03-10 22:52:07 | INFO | train_inner | epoch 030:    713 / 1103 loss=3.435, nll_loss=1.949, ppl=3.86, wps=15666.8, ups=4.44, wpb=3526.4, bsz=126.8, num_updates=32700, lr=0.000174874, gnorm=0.899, train_wall=22, gb_free=7.1, wall=7540
2021-03-10 22:52:30 | INFO | train_inner | epoch 030:    813 / 1103 loss=3.409, nll_loss=1.924, ppl=3.79, wps=16142, ups=4.46, wpb=3623, bsz=158.4, num_updates=32800, lr=0.000174608, gnorm=0.837, train_wall=22, gb_free=6.8, wall=7563
2021-03-10 22:52:52 | INFO | train_inner | epoch 030:    913 / 1103 loss=3.416, nll_loss=1.929, ppl=3.81, wps=15926.6, ups=4.45, wpb=3582.9, bsz=139.4, num_updates=32900, lr=0.000174342, gnorm=0.857, train_wall=22, gb_free=7.6, wall=7585
2021-03-10 22:53:15 | INFO | train_inner | epoch 030:   1013 / 1103 loss=3.426, nll_loss=1.943, ppl=3.84, wps=16289.2, ups=4.41, wpb=3690.2, bsz=152.3, num_updates=33000, lr=0.000174078, gnorm=0.825, train_wall=23, gb_free=7.7, wall=7608
2021-03-10 22:53:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:53:39 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 3.935 | nll_loss 2.382 | ppl 5.21 | wps 48182.3 | wpb 2873.1 | bsz 115.6 | num_updates 33090 | best_loss 3.917
2021-03-10 22:53:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 33090 updates
2021-03-10 22:53:39 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:53:41 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:53:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 30 @ 33090 updates, score 3.935) (writing took 2.337150029838085 seconds)
2021-03-10 22:53:41 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2021-03-10 22:53:41 | INFO | train | epoch 030 | loss 3.402 | nll_loss 1.914 | ppl 3.77 | wps 15534.2 | ups 4.34 | wpb 3577.8 | bsz 145.3 | num_updates 33090 | lr 0.000173841 | gnorm 0.856 | train_wall 246 | gb_free 6.8 | wall 7634
2021-03-10 22:53:41 | INFO | fairseq.trainer | begin training epoch 31
2021-03-10 22:53:41 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:53:43 | INFO | train_inner | epoch 031:     10 / 1103 loss=3.438, nll_loss=1.955, ppl=3.88, wps=12273.2, ups=3.52, wpb=3484.5, bsz=141, num_updates=33100, lr=0.000173814, gnorm=0.898, train_wall=22, gb_free=7.4, wall=7636
2021-03-10 22:54:06 | INFO | train_inner | epoch 031:    110 / 1103 loss=3.33, nll_loss=1.831, ppl=3.56, wps=15921.6, ups=4.46, wpb=3572.6, bsz=149.4, num_updates=33200, lr=0.000173553, gnorm=0.843, train_wall=22, gb_free=6.9, wall=7659
2021-03-10 22:54:28 | INFO | train_inner | epoch 031:    210 / 1103 loss=3.346, nll_loss=1.85, ppl=3.6, wps=16199.7, ups=4.42, wpb=3661.4, bsz=154.8, num_updates=33300, lr=0.000173292, gnorm=0.832, train_wall=22, gb_free=7, wall=7681
2021-03-10 22:54:51 | INFO | train_inner | epoch 031:    310 / 1103 loss=3.354, nll_loss=1.858, ppl=3.62, wps=15949.4, ups=4.45, wpb=3585.4, bsz=145.8, num_updates=33400, lr=0.000173032, gnorm=0.845, train_wall=22, gb_free=6.7, wall=7704
2021-03-10 22:55:13 | INFO | train_inner | epoch 031:    410 / 1103 loss=3.419, nll_loss=1.93, ppl=3.81, wps=15843.1, ups=4.48, wpb=3535.2, bsz=126.2, num_updates=33500, lr=0.000172774, gnorm=0.867, train_wall=22, gb_free=7, wall=7726
2021-03-10 22:55:36 | INFO | train_inner | epoch 031:    510 / 1103 loss=3.416, nll_loss=1.927, ppl=3.8, wps=15914.6, ups=4.45, wpb=3574.3, bsz=132.4, num_updates=33600, lr=0.000172516, gnorm=0.867, train_wall=22, gb_free=7.1, wall=7749
2021-03-10 22:55:58 | INFO | train_inner | epoch 031:    610 / 1103 loss=3.36, nll_loss=1.866, ppl=3.65, wps=15815.8, ups=4.42, wpb=3574.5, bsz=158.1, num_updates=33700, lr=0.00017226, gnorm=0.852, train_wall=22, gb_free=7.1, wall=7771
2021-03-10 22:56:20 | INFO | train_inner | epoch 031:    710 / 1103 loss=3.398, nll_loss=1.909, ppl=3.76, wps=15965.4, ups=4.49, wpb=3559.6, bsz=146.1, num_updates=33800, lr=0.000172005, gnorm=0.881, train_wall=22, gb_free=7.1, wall=7794
2021-03-10 22:56:43 | INFO | train_inner | epoch 031:    810 / 1103 loss=3.399, nll_loss=1.91, ppl=3.76, wps=15738.5, ups=4.45, wpb=3536.3, bsz=145.7, num_updates=33900, lr=0.000171751, gnorm=0.879, train_wall=22, gb_free=7, wall=7816
2021-03-10 22:57:05 | INFO | train_inner | epoch 031:    910 / 1103 loss=3.447, nll_loss=1.965, ppl=3.9, wps=15938.5, ups=4.45, wpb=3578.4, bsz=134.4, num_updates=34000, lr=0.000171499, gnorm=0.875, train_wall=22, gb_free=6.9, wall=7839
2021-03-10 22:57:28 | INFO | train_inner | epoch 031:   1010 / 1103 loss=3.394, nll_loss=1.906, ppl=3.75, wps=15932.3, ups=4.49, wpb=3545.4, bsz=150.8, num_updates=34100, lr=0.000171247, gnorm=0.858, train_wall=22, gb_free=7.1, wall=7861
2021-03-10 22:57:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 22:57:52 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 3.919 | nll_loss 2.366 | ppl 5.15 | wps 48222 | wpb 2873.1 | bsz 115.6 | num_updates 34193 | best_loss 3.917
2021-03-10 22:57:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 34193 updates
2021-03-10 22:57:52 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:57:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 22:57:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 31 @ 34193 updates, score 3.919) (writing took 2.397384572774172 seconds)
2021-03-10 22:57:55 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2021-03-10 22:57:55 | INFO | train | epoch 031 | loss 3.386 | nll_loss 1.895 | ppl 3.72 | wps 15541.5 | ups 4.34 | wpb 3577.8 | bsz 145.3 | num_updates 34193 | lr 0.000171014 | gnorm 0.86 | train_wall 246 | gb_free 7.2 | wall 7888
2021-03-10 22:57:55 | INFO | fairseq.trainer | begin training epoch 32
2021-03-10 22:57:55 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 22:57:57 | INFO | train_inner | epoch 032:      7 / 1103 loss=3.393, nll_loss=1.905, ppl=3.75, wps=12598.8, ups=3.46, wpb=3637.6, bsz=154.6, num_updates=34200, lr=0.000170996, gnorm=0.862, train_wall=22, gb_free=7.3, wall=7890
2021-03-10 22:58:19 | INFO | train_inner | epoch 032:    107 / 1103 loss=3.324, nll_loss=1.823, ppl=3.54, wps=15966.9, ups=4.44, wpb=3597.3, bsz=139.6, num_updates=34300, lr=0.000170747, gnorm=0.855, train_wall=22, gb_free=7.7, wall=7912
2021-03-10 22:58:41 | INFO | train_inner | epoch 032:    207 / 1103 loss=3.345, nll_loss=1.845, ppl=3.59, wps=16112.9, ups=4.5, wpb=3580.2, bsz=134.9, num_updates=34400, lr=0.000170499, gnorm=0.849, train_wall=22, gb_free=6.6, wall=7934
2021-03-10 22:59:04 | INFO | train_inner | epoch 032:    307 / 1103 loss=3.348, nll_loss=1.852, ppl=3.61, wps=16068.2, ups=4.47, wpb=3592.4, bsz=149.3, num_updates=34500, lr=0.000170251, gnorm=0.86, train_wall=22, gb_free=7.1, wall=7957
2021-03-10 22:59:26 | INFO | train_inner | epoch 032:    407 / 1103 loss=3.342, nll_loss=1.846, ppl=3.59, wps=15954, ups=4.46, wpb=3577.8, bsz=158.6, num_updates=34600, lr=0.000170005, gnorm=0.853, train_wall=22, gb_free=6.9, wall=7979
2021-03-10 22:59:48 | INFO | train_inner | epoch 032:    507 / 1103 loss=3.375, nll_loss=1.882, ppl=3.68, wps=15883.2, ups=4.49, wpb=3539.3, bsz=137.4, num_updates=34700, lr=0.00016976, gnorm=0.882, train_wall=22, gb_free=7.1, wall=8001
2021-03-10 23:00:11 | INFO | train_inner | epoch 032:    607 / 1103 loss=3.408, nll_loss=1.916, ppl=3.77, wps=15838.4, ups=4.45, wpb=3561.1, bsz=120.1, num_updates=34800, lr=0.000169516, gnorm=0.883, train_wall=22, gb_free=6.8, wall=8024
2021-03-10 23:00:33 | INFO | train_inner | epoch 032:    707 / 1103 loss=3.367, nll_loss=1.874, ppl=3.67, wps=15809.2, ups=4.48, wpb=3530.9, bsz=153.5, num_updates=34900, lr=0.000169273, gnorm=0.855, train_wall=22, gb_free=7.2, wall=8046
2021-03-10 23:00:56 | INFO | train_inner | epoch 032:    807 / 1103 loss=3.399, nll_loss=1.908, ppl=3.75, wps=15723.9, ups=4.43, wpb=3549.9, bsz=130, num_updates=35000, lr=0.000169031, gnorm=0.886, train_wall=22, gb_free=7.1, wall=8069
2021-03-10 23:01:18 | INFO | train_inner | epoch 032:    907 / 1103 loss=3.376, nll_loss=1.885, ppl=3.69, wps=15927.9, ups=4.44, wpb=3585.3, bsz=149.5, num_updates=35100, lr=0.00016879, gnorm=0.869, train_wall=22, gb_free=7.1, wall=8091
2021-03-10 23:01:41 | INFO | train_inner | epoch 032:   1007 / 1103 loss=3.401, nll_loss=1.914, ppl=3.77, wps=16164, ups=4.48, wpb=3608.2, bsz=160.4, num_updates=35200, lr=0.00016855, gnorm=0.856, train_wall=22, gb_free=7.2, wall=8114
2021-03-10 23:02:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:02:06 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 3.914 | nll_loss 2.36 | ppl 5.14 | wps 48282.3 | wpb 2873.1 | bsz 115.6 | num_updates 35296 | best_loss 3.914
2021-03-10 23:02:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 35296 updates
2021-03-10 23:02:06 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 23:02:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 23:02:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 32 @ 35296 updates, score 3.914) (writing took 4.695395294576883 seconds)
2021-03-10 23:02:10 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2021-03-10 23:02:10 | INFO | train | epoch 032 | loss 3.371 | nll_loss 1.877 | ppl 3.67 | wps 15435.7 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 35296 | lr 0.000168321 | gnorm 0.863 | train_wall 246 | gb_free 7.5 | wall 8144
2021-03-10 23:02:11 | INFO | fairseq.trainer | begin training epoch 33
2021-03-10 23:02:11 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:02:12 | INFO | train_inner | epoch 033:      4 / 1103 loss=3.392, nll_loss=1.905, ppl=3.75, wps=11681.8, ups=3.23, wpb=3617.1, bsz=162.2, num_updates=35300, lr=0.000168311, gnorm=0.843, train_wall=22, gb_free=6.7, wall=8145
2021-03-10 23:02:34 | INFO | train_inner | epoch 033:    104 / 1103 loss=3.292, nll_loss=1.787, ppl=3.45, wps=16071.3, ups=4.48, wpb=3586.2, bsz=150.3, num_updates=35400, lr=0.000168073, gnorm=0.843, train_wall=22, gb_free=6.9, wall=8167
2021-03-10 23:02:56 | INFO | train_inner | epoch 033:    204 / 1103 loss=3.364, nll_loss=1.869, ppl=3.65, wps=16005.6, ups=4.52, wpb=3540.4, bsz=141.8, num_updates=35500, lr=0.000167836, gnorm=0.866, train_wall=22, gb_free=7.2, wall=8189
2021-03-10 23:03:19 | INFO | train_inner | epoch 033:    304 / 1103 loss=3.375, nll_loss=1.88, ppl=3.68, wps=15972.1, ups=4.39, wpb=3639.7, bsz=127.8, num_updates=35600, lr=0.0001676, gnorm=0.858, train_wall=23, gb_free=6.9, wall=8212
2021-03-10 23:03:41 | INFO | train_inner | epoch 033:    404 / 1103 loss=3.32, nll_loss=1.819, ppl=3.53, wps=16024.7, ups=4.43, wpb=3619.9, bsz=153.6, num_updates=35700, lr=0.000167365, gnorm=0.857, train_wall=22, gb_free=7, wall=8234
2021-03-10 23:04:04 | INFO | train_inner | epoch 033:    504 / 1103 loss=3.364, nll_loss=1.869, ppl=3.65, wps=15930, ups=4.42, wpb=3605.6, bsz=137.5, num_updates=35800, lr=0.000167132, gnorm=0.877, train_wall=23, gb_free=6.9, wall=8257
2021-03-10 23:04:26 | INFO | train_inner | epoch 033:    604 / 1103 loss=3.34, nll_loss=1.843, ppl=3.59, wps=15729.5, ups=4.45, wpb=3534.7, bsz=156.6, num_updates=35900, lr=0.000166899, gnorm=0.878, train_wall=22, gb_free=7.2, wall=8280
2021-03-10 23:04:49 | INFO | train_inner | epoch 033:    704 / 1103 loss=3.383, nll_loss=1.89, ppl=3.71, wps=16101.8, ups=4.47, wpb=3600.3, bsz=143.4, num_updates=36000, lr=0.000166667, gnorm=0.87, train_wall=22, gb_free=7.1, wall=8302
2021-03-10 23:05:11 | INFO | train_inner | epoch 033:    804 / 1103 loss=3.35, nll_loss=1.854, ppl=3.61, wps=15992.7, ups=4.43, wpb=3607.7, bsz=153.8, num_updates=36100, lr=0.000166436, gnorm=0.855, train_wall=22, gb_free=7.1, wall=8325
2021-03-10 23:05:33 | INFO | train_inner | epoch 033:    904 / 1103 loss=3.403, nll_loss=1.913, ppl=3.77, wps=15511.5, ups=4.54, wpb=3419.6, bsz=134.2, num_updates=36200, lr=0.000166206, gnorm=0.921, train_wall=22, gb_free=7.2, wall=8347
2021-03-10 23:05:56 | INFO | train_inner | epoch 033:   1004 / 1103 loss=3.401, nll_loss=1.911, ppl=3.76, wps=15927, ups=4.48, wpb=3553.5, bsz=137, num_updates=36300, lr=0.000165977, gnorm=0.889, train_wall=22, gb_free=6.8, wall=8369
2021-03-10 23:06:18 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:06:22 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 3.91 | nll_loss 2.359 | ppl 5.13 | wps 48261.2 | wpb 2873.1 | bsz 115.6 | num_updates 36399 | best_loss 3.91
2021-03-10 23:06:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 36399 updates
2021-03-10 23:06:22 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 23:06:24 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt
2021-03-10 23:06:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_best.pt (epoch 33 @ 36399 updates, score 3.91) (writing took 4.509799540042877 seconds)
2021-03-10 23:06:26 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2021-03-10 23:06:26 | INFO | train | epoch 033 | loss 3.358 | nll_loss 1.862 | ppl 3.63 | wps 15420.4 | ups 4.31 | wpb 3577.8 | bsz 145.3 | num_updates 36399 | lr 0.000165751 | gnorm 0.867 | train_wall 246 | gb_free 6.9 | wall 8400
2021-03-10 23:06:26 | INFO | fairseq.trainer | begin training epoch 34
2021-03-10 23:06:26 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:06:27 | INFO | train_inner | epoch 034:      1 / 1103 loss=3.341, nll_loss=1.846, ppl=3.59, wps=11780.1, ups=3.22, wpb=3660.6, bsz=163.4, num_updates=36400, lr=0.000165748, gnorm=0.825, train_wall=22, gb_free=7.1, wall=8400
2021-03-10 23:06:49 | INFO | train_inner | epoch 034:    101 / 1103 loss=3.286, nll_loss=1.781, ppl=3.44, wps=16030, ups=4.47, wpb=3589.9, bsz=162.9, num_updates=36500, lr=0.000165521, gnorm=0.845, train_wall=22, gb_free=7.3, wall=8422
2021-03-10 23:07:12 | INFO | train_inner | epoch 034:    201 / 1103 loss=3.302, nll_loss=1.798, ppl=3.48, wps=16094, ups=4.45, wpb=3618.7, bsz=155, num_updates=36600, lr=0.000165295, gnorm=0.857, train_wall=22, gb_free=7.3, wall=8445
2021-03-10 23:07:34 | INFO | train_inner | epoch 034:    301 / 1103 loss=3.338, nll_loss=1.839, ppl=3.58, wps=15999.3, ups=4.48, wpb=3571.1, bsz=146.2, num_updates=36700, lr=0.00016507, gnorm=0.881, train_wall=22, gb_free=7.4, wall=8467
2021-03-10 23:07:56 | INFO | train_inner | epoch 034:    401 / 1103 loss=3.359, nll_loss=1.861, ppl=3.63, wps=15670.5, ups=4.46, wpb=3511.3, bsz=124.6, num_updates=36800, lr=0.000164845, gnorm=0.893, train_wall=22, gb_free=7.3, wall=8490
2021-03-10 23:08:19 | INFO | train_inner | epoch 034:    501 / 1103 loss=3.387, nll_loss=1.893, ppl=3.71, wps=16122.4, ups=4.47, wpb=3609.3, bsz=124.5, num_updates=36900, lr=0.000164622, gnorm=0.883, train_wall=22, gb_free=6.9, wall=8512
2021-03-10 23:08:41 | INFO | train_inner | epoch 034:    601 / 1103 loss=3.318, nll_loss=1.819, ppl=3.53, wps=16117.5, ups=4.45, wpb=3624.7, bsz=169.4, num_updates=37000, lr=0.000164399, gnorm=0.844, train_wall=22, gb_free=6.7, wall=8534
2021-03-10 23:09:04 | INFO | train_inner | epoch 034:    701 / 1103 loss=3.344, nll_loss=1.848, ppl=3.6, wps=16007.6, ups=4.47, wpb=3579.8, bsz=152.7, num_updates=37100, lr=0.000164177, gnorm=0.866, train_wall=22, gb_free=6.9, wall=8557
2021-03-10 23:09:26 | INFO | train_inner | epoch 034:    801 / 1103 loss=3.318, nll_loss=1.818, ppl=3.53, wps=15697.4, ups=4.44, wpb=3539.3, bsz=153, num_updates=37200, lr=0.000163956, gnorm=0.873, train_wall=22, gb_free=7.2, wall=8579
2021-03-10 23:09:49 | INFO | train_inner | epoch 034:    901 / 1103 loss=3.35, nll_loss=1.852, ppl=3.61, wps=15710.9, ups=4.48, wpb=3505.9, bsz=140.9, num_updates=37300, lr=0.000163737, gnorm=0.888, train_wall=22, gb_free=6.7, wall=8602
2021-03-10 23:10:11 | INFO | train_inner | epoch 034:   1001 / 1103 loss=3.371, nll_loss=1.876, ppl=3.67, wps=15983.6, ups=4.44, wpb=3597.9, bsz=137.8, num_updates=37400, lr=0.000163517, gnorm=0.868, train_wall=22, gb_free=7, wall=8624
2021-03-10 23:10:33 | INFO | train_inner | epoch 034:   1101 / 1103 loss=3.407, nll_loss=1.918, ppl=3.78, wps=16074, ups=4.46, wpb=3607.1, bsz=132.6, num_updates=37500, lr=0.000163299, gnorm=0.887, train_wall=22, gb_free=6.6, wall=8647
2021-03-10 23:10:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:10:38 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 3.923 | nll_loss 2.369 | ppl 5.17 | wps 48235.2 | wpb 2873.1 | bsz 115.6 | num_updates 37502 | best_loss 3.91
2021-03-10 23:10:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 37502 updates
2021-03-10 23:10:38 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:10:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:10:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 34 @ 37502 updates, score 3.923) (writing took 2.0616847053170204 seconds)
2021-03-10 23:10:40 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2021-03-10 23:10:40 | INFO | train | epoch 034 | loss 3.344 | nll_loss 1.846 | ppl 3.6 | wps 15573.2 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 37502 | lr 0.000163295 | gnorm 0.872 | train_wall 246 | gb_free 6.7 | wall 8653
2021-03-10 23:10:40 | INFO | fairseq.trainer | begin training epoch 35
2021-03-10 23:10:40 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:11:02 | INFO | train_inner | epoch 035:     98 / 1103 loss=3.296, nll_loss=1.79, ppl=3.46, wps=12702.8, ups=3.5, wpb=3625.4, bsz=143.4, num_updates=37600, lr=0.000163082, gnorm=0.846, train_wall=22, gb_free=7.2, wall=8675
2021-03-10 23:11:24 | INFO | train_inner | epoch 035:    198 / 1103 loss=3.3, nll_loss=1.793, ppl=3.47, wps=15808.9, ups=4.45, wpb=3549.4, bsz=136.7, num_updates=37700, lr=0.000162866, gnorm=0.874, train_wall=22, gb_free=7.1, wall=8698
2021-03-10 23:11:47 | INFO | train_inner | epoch 035:    298 / 1103 loss=3.3, nll_loss=1.795, ppl=3.47, wps=15910.6, ups=4.44, wpb=3583.1, bsz=148.9, num_updates=37800, lr=0.00016265, gnorm=0.874, train_wall=22, gb_free=7.2, wall=8720
2021-03-10 23:12:09 | INFO | train_inner | epoch 035:    398 / 1103 loss=3.323, nll_loss=1.82, ppl=3.53, wps=15862.6, ups=4.46, wpb=3560.2, bsz=138.6, num_updates=37900, lr=0.000162435, gnorm=0.882, train_wall=22, gb_free=7.3, wall=8743
2021-03-10 23:12:32 | INFO | train_inner | epoch 035:    498 / 1103 loss=3.332, nll_loss=1.83, ppl=3.56, wps=15961.1, ups=4.49, wpb=3553.3, bsz=137, num_updates=38000, lr=0.000162221, gnorm=0.873, train_wall=22, gb_free=6.9, wall=8765
2021-03-10 23:12:54 | INFO | train_inner | epoch 035:    598 / 1103 loss=3.331, nll_loss=1.832, ppl=3.56, wps=15902.2, ups=4.46, wpb=3567.3, bsz=149.6, num_updates=38100, lr=0.000162008, gnorm=0.866, train_wall=22, gb_free=7.2, wall=8787
2021-03-10 23:13:17 | INFO | train_inner | epoch 035:    698 / 1103 loss=3.343, nll_loss=1.847, ppl=3.6, wps=16058.4, ups=4.46, wpb=3601.8, bsz=159.7, num_updates=38200, lr=0.000161796, gnorm=0.862, train_wall=22, gb_free=7.8, wall=8810
2021-03-10 23:13:39 | INFO | train_inner | epoch 035:    798 / 1103 loss=3.32, nll_loss=1.821, ppl=3.53, wps=16023.3, ups=4.44, wpb=3607.8, bsz=161.1, num_updates=38300, lr=0.000161585, gnorm=0.863, train_wall=22, gb_free=7.3, wall=8832
2021-03-10 23:14:02 | INFO | train_inner | epoch 035:    898 / 1103 loss=3.384, nll_loss=1.891, ppl=3.71, wps=16075.2, ups=4.43, wpb=3631.3, bsz=133.4, num_updates=38400, lr=0.000161374, gnorm=0.886, train_wall=22, gb_free=7.1, wall=8855
2021-03-10 23:14:24 | INFO | train_inner | epoch 035:    998 / 1103 loss=3.338, nll_loss=1.839, ppl=3.58, wps=15693, ups=4.44, wpb=3536.5, bsz=147.4, num_updates=38500, lr=0.000161165, gnorm=0.886, train_wall=22, gb_free=6.9, wall=8877
2021-03-10 23:14:47 | INFO | train_inner | epoch 035:   1098 / 1103 loss=3.354, nll_loss=1.859, ppl=3.63, wps=15915, ups=4.48, wpb=3553.6, bsz=143.6, num_updates=38600, lr=0.000160956, gnorm=0.887, train_wall=22, gb_free=7.4, wall=8900
2021-03-10 23:14:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:14:51 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 3.921 | nll_loss 2.367 | ppl 5.16 | wps 48202.3 | wpb 2873.1 | bsz 115.6 | num_updates 38605 | best_loss 3.91
2021-03-10 23:14:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 38605 updates
2021-03-10 23:14:51 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:14:53 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:14:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 35 @ 38605 updates, score 3.921) (writing took 2.04384508356452 seconds)
2021-03-10 23:14:53 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)
2021-03-10 23:14:53 | INFO | train | epoch 035 | loss 3.33 | nll_loss 1.829 | ppl 3.55 | wps 15558 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 38605 | lr 0.000160945 | gnorm 0.873 | train_wall 246 | gb_free 7.2 | wall 8907
2021-03-10 23:14:54 | INFO | fairseq.trainer | begin training epoch 36
2021-03-10 23:14:54 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:15:15 | INFO | train_inner | epoch 036:     95 / 1103 loss=3.27, nll_loss=1.76, ppl=3.39, wps=12482.9, ups=3.52, wpb=3548.5, bsz=140.5, num_updates=38700, lr=0.000160748, gnorm=0.865, train_wall=22, gb_free=6.9, wall=8928
2021-03-10 23:15:37 | INFO | train_inner | epoch 036:    195 / 1103 loss=3.303, nll_loss=1.798, ppl=3.48, wps=15850.7, ups=4.47, wpb=3542.6, bsz=147.7, num_updates=38800, lr=0.00016054, gnorm=0.902, train_wall=22, gb_free=6.7, wall=8950
2021-03-10 23:16:00 | INFO | train_inner | epoch 036:    295 / 1103 loss=3.294, nll_loss=1.789, ppl=3.46, wps=15912.7, ups=4.45, wpb=3574.8, bsz=156.6, num_updates=38900, lr=0.000160334, gnorm=0.876, train_wall=22, gb_free=7.1, wall=8973
2021-03-10 23:16:22 | INFO | train_inner | epoch 036:    395 / 1103 loss=3.31, nll_loss=1.807, ppl=3.5, wps=15896.3, ups=4.47, wpb=3555.9, bsz=148.2, num_updates=39000, lr=0.000160128, gnorm=0.889, train_wall=22, gb_free=7.1, wall=8995
2021-03-10 23:16:45 | INFO | train_inner | epoch 036:    495 / 1103 loss=3.321, nll_loss=1.816, ppl=3.52, wps=15786.7, ups=4.45, wpb=3548.7, bsz=131, num_updates=39100, lr=0.000159923, gnorm=0.883, train_wall=22, gb_free=6.8, wall=9018
2021-03-10 23:17:07 | INFO | train_inner | epoch 036:    595 / 1103 loss=3.323, nll_loss=1.822, ppl=3.54, wps=16074.7, ups=4.49, wpb=3579.1, bsz=147.3, num_updates=39200, lr=0.000159719, gnorm=0.889, train_wall=22, gb_free=7, wall=9040
2021-03-10 23:17:29 | INFO | train_inner | epoch 036:    695 / 1103 loss=3.348, nll_loss=1.851, ppl=3.61, wps=16097.7, ups=4.47, wpb=3603.7, bsz=146.4, num_updates=39300, lr=0.000159516, gnorm=0.866, train_wall=22, gb_free=7.1, wall=9062
2021-03-10 23:17:52 | INFO | train_inner | epoch 036:    795 / 1103 loss=3.324, nll_loss=1.823, ppl=3.54, wps=15862.4, ups=4.46, wpb=3553.6, bsz=142.5, num_updates=39400, lr=0.000159313, gnorm=0.896, train_wall=22, gb_free=7.1, wall=9085
2021-03-10 23:18:14 | INFO | train_inner | epoch 036:    895 / 1103 loss=3.311, nll_loss=1.809, ppl=3.5, wps=15933.9, ups=4.42, wpb=3608.2, bsz=147.4, num_updates=39500, lr=0.000159111, gnorm=0.863, train_wall=23, gb_free=7.1, wall=9107
2021-03-10 23:18:37 | INFO | train_inner | epoch 036:    995 / 1103 loss=3.346, nll_loss=1.849, ppl=3.6, wps=16078.4, ups=4.44, wpb=3619.8, bsz=146.6, num_updates=39600, lr=0.00015891, gnorm=0.877, train_wall=22, gb_free=7.5, wall=9130
2021-03-10 23:18:59 | INFO | train_inner | epoch 036:   1095 / 1103 loss=3.354, nll_loss=1.857, ppl=3.62, wps=15970.9, ups=4.45, wpb=3590.2, bsz=139.2, num_updates=39700, lr=0.00015871, gnorm=0.887, train_wall=22, gb_free=6.9, wall=9152
2021-03-10 23:19:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:19:05 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 3.919 | nll_loss 2.364 | ppl 5.15 | wps 48199.1 | wpb 2873.1 | bsz 115.6 | num_updates 39708 | best_loss 3.91
2021-03-10 23:19:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 39708 updates
2021-03-10 23:19:05 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:19:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:19:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 36 @ 39708 updates, score 3.919) (writing took 2.1807570084929466 seconds)
2021-03-10 23:19:07 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)
2021-03-10 23:19:07 | INFO | train | epoch 036 | loss 3.318 | nll_loss 1.816 | ppl 3.52 | wps 15560.5 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 39708 | lr 0.000158694 | gnorm 0.881 | train_wall 246 | gb_free 7 | wall 9160
2021-03-10 23:19:07 | INFO | fairseq.trainer | begin training epoch 37
2021-03-10 23:19:07 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:19:28 | INFO | train_inner | epoch 037:     92 / 1103 loss=3.269, nll_loss=1.759, ppl=3.39, wps=12664.4, ups=3.46, wpb=3655.7, bsz=145, num_updates=39800, lr=0.000158511, gnorm=0.859, train_wall=23, gb_free=7, wall=9181
2021-03-10 23:19:51 | INFO | train_inner | epoch 037:    192 / 1103 loss=3.273, nll_loss=1.766, ppl=3.4, wps=16278.6, ups=4.46, wpb=3650.7, bsz=166, num_updates=39900, lr=0.000158312, gnorm=0.839, train_wall=22, gb_free=7.1, wall=9204
2021-03-10 23:20:13 | INFO | train_inner | epoch 037:    292 / 1103 loss=3.272, nll_loss=1.762, ppl=3.39, wps=15658.6, ups=4.45, wpb=3519.3, bsz=147.7, num_updates=40000, lr=0.000158114, gnorm=0.883, train_wall=22, gb_free=7, wall=9226
2021-03-10 23:20:35 | INFO | train_inner | epoch 037:    392 / 1103 loss=3.286, nll_loss=1.778, ppl=3.43, wps=15689.1, ups=4.47, wpb=3506.2, bsz=143.2, num_updates=40100, lr=0.000157917, gnorm=0.887, train_wall=22, gb_free=7.4, wall=9249
2021-03-10 23:20:58 | INFO | train_inner | epoch 037:    492 / 1103 loss=3.285, nll_loss=1.777, ppl=3.43, wps=16026.7, ups=4.39, wpb=3647.5, bsz=144.9, num_updates=40200, lr=0.00015772, gnorm=0.849, train_wall=23, gb_free=7, wall=9271
2021-03-10 23:21:21 | INFO | train_inner | epoch 037:    592 / 1103 loss=3.326, nll_loss=1.824, ppl=3.54, wps=15865.1, ups=4.46, wpb=3557.2, bsz=135.7, num_updates=40300, lr=0.000157524, gnorm=0.9, train_wall=22, gb_free=7.6, wall=9294
2021-03-10 23:21:43 | INFO | train_inner | epoch 037:    692 / 1103 loss=3.314, nll_loss=1.81, ppl=3.51, wps=15793.1, ups=4.47, wpb=3532, bsz=143.9, num_updates=40400, lr=0.000157329, gnorm=0.893, train_wall=22, gb_free=7.1, wall=9316
2021-03-10 23:22:05 | INFO | train_inner | epoch 037:    792 / 1103 loss=3.296, nll_loss=1.791, ppl=3.46, wps=15740.4, ups=4.46, wpb=3526.2, bsz=149.8, num_updates=40500, lr=0.000157135, gnorm=0.886, train_wall=22, gb_free=7, wall=9338
2021-03-10 23:22:28 | INFO | train_inner | epoch 037:    892 / 1103 loss=3.338, nll_loss=1.839, ppl=3.58, wps=15941, ups=4.49, wpb=3553.3, bsz=138.9, num_updates=40600, lr=0.000156941, gnorm=0.909, train_wall=22, gb_free=7, wall=9361
2021-03-10 23:22:50 | INFO | train_inner | epoch 037:    992 / 1103 loss=3.339, nll_loss=1.84, ppl=3.58, wps=15976.2, ups=4.41, wpb=3619.3, bsz=139.4, num_updates=40700, lr=0.000156748, gnorm=0.879, train_wall=23, gb_free=7, wall=9383
2021-03-10 23:23:13 | INFO | train_inner | epoch 037:   1092 / 1103 loss=3.349, nll_loss=1.851, ppl=3.61, wps=16012.5, ups=4.46, wpb=3588.1, bsz=143, num_updates=40800, lr=0.000156556, gnorm=0.898, train_wall=22, gb_free=6.7, wall=9406
2021-03-10 23:23:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:23:19 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 3.911 | nll_loss 2.359 | ppl 5.13 | wps 48196.2 | wpb 2873.1 | bsz 115.6 | num_updates 40811 | best_loss 3.91
2021-03-10 23:23:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 40811 updates
2021-03-10 23:23:19 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:23:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:23:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 37 @ 40811 updates, score 3.911) (writing took 2.109595786780119 seconds)
2021-03-10 23:23:21 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)
2021-03-10 23:23:21 | INFO | train | epoch 037 | loss 3.304 | nll_loss 1.8 | ppl 3.48 | wps 15541 | ups 4.34 | wpb 3577.8 | bsz 145.3 | num_updates 40811 | lr 0.000156535 | gnorm 0.88 | train_wall 247 | gb_free 7.6 | wall 9414
2021-03-10 23:23:21 | INFO | fairseq.trainer | begin training epoch 38
2021-03-10 23:23:21 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:23:41 | INFO | train_inner | epoch 038:     89 / 1103 loss=3.252, nll_loss=1.742, ppl=3.34, wps=12627.5, ups=3.54, wpb=3566.1, bsz=158.2, num_updates=40900, lr=0.000156365, gnorm=0.857, train_wall=22, gb_free=7.2, wall=9434
2021-03-10 23:24:03 | INFO | train_inner | epoch 038:    189 / 1103 loss=3.245, nll_loss=1.732, ppl=3.32, wps=16048.4, ups=4.45, wpb=3609.7, bsz=151.5, num_updates=41000, lr=0.000156174, gnorm=0.852, train_wall=22, gb_free=7, wall=9457
2021-03-10 23:24:26 | INFO | train_inner | epoch 038:    289 / 1103 loss=3.287, nll_loss=1.778, ppl=3.43, wps=15996.9, ups=4.49, wpb=3560.6, bsz=136.6, num_updates=41100, lr=0.000155984, gnorm=0.89, train_wall=22, gb_free=6.7, wall=9479
2021-03-10 23:24:48 | INFO | train_inner | epoch 038:    389 / 1103 loss=3.279, nll_loss=1.771, ppl=3.41, wps=16297, ups=4.44, wpb=3671, bsz=152.3, num_updates=41200, lr=0.000155794, gnorm=0.859, train_wall=22, gb_free=7.3, wall=9501
2021-03-10 23:25:11 | INFO | train_inner | epoch 038:    489 / 1103 loss=3.323, nll_loss=1.82, ppl=3.53, wps=15968.2, ups=4.48, wpb=3568.1, bsz=136.6, num_updates=41300, lr=0.000155606, gnorm=0.894, train_wall=22, gb_free=7, wall=9524
2021-03-10 23:25:33 | INFO | train_inner | epoch 038:    589 / 1103 loss=3.276, nll_loss=1.766, ppl=3.4, wps=15609.2, ups=4.44, wpb=3513.4, bsz=145.5, num_updates=41400, lr=0.000155417, gnorm=0.894, train_wall=22, gb_free=7.4, wall=9546
2021-03-10 23:25:56 | INFO | train_inner | epoch 038:    689 / 1103 loss=3.321, nll_loss=1.818, ppl=3.53, wps=15961.3, ups=4.46, wpb=3578.5, bsz=136.2, num_updates=41500, lr=0.00015523, gnorm=0.894, train_wall=22, gb_free=7.1, wall=9569
2021-03-10 23:26:18 | INFO | train_inner | epoch 038:    789 / 1103 loss=3.285, nll_loss=1.779, ppl=3.43, wps=16109, ups=4.44, wpb=3631.7, bsz=157.9, num_updates=41600, lr=0.000155043, gnorm=0.877, train_wall=22, gb_free=6.8, wall=9591
2021-03-10 23:26:41 | INFO | train_inner | epoch 038:    889 / 1103 loss=3.32, nll_loss=1.818, ppl=3.53, wps=15826.7, ups=4.43, wpb=3574.3, bsz=141.8, num_updates=41700, lr=0.000154857, gnorm=0.885, train_wall=22, gb_free=7, wall=9614
2021-03-10 23:27:03 | INFO | train_inner | epoch 038:    989 / 1103 loss=3.336, nll_loss=1.836, ppl=3.57, wps=15811.7, ups=4.46, wpb=3548.9, bsz=137.1, num_updates=41800, lr=0.000154672, gnorm=0.924, train_wall=22, gb_free=7, wall=9636
2021-03-10 23:27:26 | INFO | train_inner | epoch 038:   1089 / 1103 loss=3.287, nll_loss=1.78, ppl=3.44, wps=15792.3, ups=4.45, wpb=3551.7, bsz=146.6, num_updates=41900, lr=0.000154487, gnorm=0.887, train_wall=22, gb_free=6.9, wall=9659
2021-03-10 23:27:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:27:32 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 3.925 | nll_loss 2.373 | ppl 5.18 | wps 48225.7 | wpb 2873.1 | bsz 115.6 | num_updates 41914 | best_loss 3.91
2021-03-10 23:27:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 41914 updates
2021-03-10 23:27:32 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:27:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:27:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 38 @ 41914 updates, score 3.925) (writing took 2.0347891561686993 seconds)
2021-03-10 23:27:34 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)
2021-03-10 23:27:34 | INFO | train | epoch 038 | loss 3.292 | nll_loss 1.785 | ppl 3.45 | wps 15573.4 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 41914 | lr 0.000154462 | gnorm 0.884 | train_wall 246 | gb_free 7.1 | wall 9668
2021-03-10 23:27:34 | INFO | fairseq.trainer | begin training epoch 39
2021-03-10 23:27:34 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:27:54 | INFO | train_inner | epoch 039:     86 / 1103 loss=3.238, nll_loss=1.723, ppl=3.3, wps=12437.5, ups=3.56, wpb=3496.9, bsz=149.4, num_updates=42000, lr=0.000154303, gnorm=0.88, train_wall=22, gb_free=7.6, wall=9687
2021-03-10 23:28:16 | INFO | train_inner | epoch 039:    186 / 1103 loss=3.247, nll_loss=1.733, ppl=3.32, wps=16005.2, ups=4.47, wpb=3583.5, bsz=145.1, num_updates=42100, lr=0.00015412, gnorm=0.87, train_wall=22, gb_free=7, wall=9709
2021-03-10 23:28:39 | INFO | train_inner | epoch 039:    286 / 1103 loss=3.257, nll_loss=1.744, ppl=3.35, wps=15873.7, ups=4.44, wpb=3576.4, bsz=137.2, num_updates=42200, lr=0.000153937, gnorm=0.885, train_wall=22, gb_free=7, wall=9732
2021-03-10 23:29:01 | INFO | train_inner | epoch 039:    386 / 1103 loss=3.218, nll_loss=1.702, ppl=3.25, wps=15681.8, ups=4.42, wpb=3545.6, bsz=162.2, num_updates=42300, lr=0.000153755, gnorm=0.862, train_wall=22, gb_free=7, wall=9754
2021-03-10 23:29:24 | INFO | train_inner | epoch 039:    486 / 1103 loss=3.271, nll_loss=1.76, ppl=3.39, wps=15895.7, ups=4.42, wpb=3593.9, bsz=144, num_updates=42400, lr=0.000153574, gnorm=0.875, train_wall=22, gb_free=7, wall=9777
2021-03-10 23:29:46 | INFO | train_inner | epoch 039:    586 / 1103 loss=3.28, nll_loss=1.774, ppl=3.42, wps=16471.6, ups=4.47, wpb=3682.1, bsz=164.7, num_updates=42500, lr=0.000153393, gnorm=0.858, train_wall=22, gb_free=7.2, wall=9799
2021-03-10 23:30:09 | INFO | train_inner | epoch 039:    686 / 1103 loss=3.285, nll_loss=1.777, ppl=3.43, wps=16108.5, ups=4.44, wpb=3630.6, bsz=139.1, num_updates=42600, lr=0.000153213, gnorm=0.879, train_wall=22, gb_free=7.1, wall=9822
2021-03-10 23:30:31 | INFO | train_inner | epoch 039:    786 / 1103 loss=3.314, nll_loss=1.812, ppl=3.51, wps=16340.5, ups=4.49, wpb=3638.8, bsz=152.8, num_updates=42700, lr=0.000153033, gnorm=0.889, train_wall=22, gb_free=6.4, wall=9844
2021-03-10 23:30:53 | INFO | train_inner | epoch 039:    886 / 1103 loss=3.331, nll_loss=1.828, ppl=3.55, wps=15927.9, ups=4.48, wpb=3559.2, bsz=132.3, num_updates=42800, lr=0.000152854, gnorm=0.908, train_wall=22, gb_free=7.1, wall=9866
2021-03-10 23:31:16 | INFO | train_inner | epoch 039:    986 / 1103 loss=3.348, nll_loss=1.848, ppl=3.6, wps=15855.5, ups=4.5, wpb=3519.5, bsz=125.4, num_updates=42900, lr=0.000152676, gnorm=0.94, train_wall=22, gb_free=7.1, wall=9889
2021-03-10 23:31:38 | INFO | train_inner | epoch 039:   1086 / 1103 loss=3.311, nll_loss=1.806, ppl=3.5, wps=15783.2, ups=4.43, wpb=3565.4, bsz=141.8, num_updates=43000, lr=0.000152499, gnorm=0.897, train_wall=22, gb_free=7.1, wall=9911
2021-03-10 23:31:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:31:46 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 3.943 | nll_loss 2.385 | ppl 5.22 | wps 48193.8 | wpb 2873.1 | bsz 115.6 | num_updates 43017 | best_loss 3.91
2021-03-10 23:31:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 43017 updates
2021-03-10 23:31:46 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:31:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:31:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 39 @ 43017 updates, score 3.943) (writing took 2.0528496094048023 seconds)
2021-03-10 23:31:48 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)
2021-03-10 23:31:48 | INFO | train | epoch 039 | loss 3.281 | nll_loss 1.772 | ppl 3.42 | wps 15583.2 | ups 4.36 | wpb 3577.8 | bsz 145.3 | num_updates 43017 | lr 0.000152468 | gnorm 0.886 | train_wall 246 | gb_free 7 | wall 9921
2021-03-10 23:31:48 | INFO | fairseq.trainer | begin training epoch 40
2021-03-10 23:31:48 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:32:06 | INFO | train_inner | epoch 040:     83 / 1103 loss=3.235, nll_loss=1.718, ppl=3.29, wps=12356.5, ups=3.53, wpb=3496.8, bsz=138.3, num_updates=43100, lr=0.000152322, gnorm=0.888, train_wall=22, gb_free=6.9, wall=9940
2021-03-10 23:32:29 | INFO | train_inner | epoch 040:    183 / 1103 loss=3.228, nll_loss=1.71, ppl=3.27, wps=15774.2, ups=4.45, wpb=3545.1, bsz=135.8, num_updates=43200, lr=0.000152145, gnorm=0.88, train_wall=22, gb_free=7.5, wall=9962
2021-03-10 23:32:51 | INFO | train_inner | epoch 040:    283 / 1103 loss=3.27, nll_loss=1.759, ppl=3.38, wps=16172.6, ups=4.47, wpb=3621, bsz=138.2, num_updates=43300, lr=0.000151969, gnorm=0.873, train_wall=22, gb_free=7, wall=9984
2021-03-10 23:33:14 | INFO | train_inner | epoch 040:    383 / 1103 loss=3.255, nll_loss=1.74, ppl=3.34, wps=15698.4, ups=4.44, wpb=3535.6, bsz=137.8, num_updates=43400, lr=0.000151794, gnorm=0.899, train_wall=22, gb_free=6.9, wall=10007
2021-03-10 23:33:36 | INFO | train_inner | epoch 040:    483 / 1103 loss=3.254, nll_loss=1.741, ppl=3.34, wps=15837.3, ups=4.46, wpb=3550.3, bsz=145.2, num_updates=43500, lr=0.00015162, gnorm=0.892, train_wall=22, gb_free=7.1, wall=10029
2021-03-10 23:33:59 | INFO | train_inner | epoch 040:    583 / 1103 loss=3.261, nll_loss=1.749, ppl=3.36, wps=15790, ups=4.48, wpb=3522.8, bsz=145.1, num_updates=43600, lr=0.000151446, gnorm=0.927, train_wall=22, gb_free=7.2, wall=10052
2021-03-10 23:34:21 | INFO | train_inner | epoch 040:    683 / 1103 loss=3.284, nll_loss=1.775, ppl=3.42, wps=16050.5, ups=4.44, wpb=3617.8, bsz=143.6, num_updates=43700, lr=0.000151272, gnorm=0.892, train_wall=22, gb_free=7.6, wall=10074
2021-03-10 23:34:44 | INFO | train_inner | epoch 040:    783 / 1103 loss=3.283, nll_loss=1.776, ppl=3.42, wps=16032.1, ups=4.44, wpb=3610.5, bsz=152.4, num_updates=43800, lr=0.000151099, gnorm=0.891, train_wall=22, gb_free=6.9, wall=10097
2021-03-10 23:35:06 | INFO | train_inner | epoch 040:    883 / 1103 loss=3.314, nll_loss=1.811, ppl=3.51, wps=16047.4, ups=4.48, wpb=3582.9, bsz=142.6, num_updates=43900, lr=0.000150927, gnorm=0.901, train_wall=22, gb_free=7.2, wall=10119
2021-03-10 23:35:28 | INFO | train_inner | epoch 040:    983 / 1103 loss=3.277, nll_loss=1.772, ppl=3.41, wps=16117.8, ups=4.44, wpb=3630, bsz=170.6, num_updates=44000, lr=0.000150756, gnorm=0.866, train_wall=22, gb_free=6.9, wall=10142
2021-03-10 23:35:51 | INFO | train_inner | epoch 040:   1083 / 1103 loss=3.28, nll_loss=1.772, ppl=3.42, wps=15828.5, ups=4.44, wpb=3565.3, bsz=152.3, num_updates=44100, lr=0.000150585, gnorm=0.881, train_wall=22, gb_free=7, wall=10164
2021-03-10 23:35:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:35:59 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 3.92 | nll_loss 2.364 | ppl 5.15 | wps 48206.5 | wpb 2873.1 | bsz 115.6 | num_updates 44120 | best_loss 3.91
2021-03-10 23:35:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 44120 updates
2021-03-10 23:35:59 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:36:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:36:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 40 @ 44120 updates, score 3.92) (writing took 2.043198984116316 seconds)
2021-03-10 23:36:01 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)
2021-03-10 23:36:01 | INFO | train | epoch 040 | loss 3.269 | nll_loss 1.759 | ppl 3.38 | wps 15559.5 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 44120 | lr 0.000150551 | gnorm 0.889 | train_wall 246 | gb_free 6.8 | wall 10174
2021-03-10 23:36:01 | INFO | fairseq.trainer | begin training epoch 41
2021-03-10 23:36:01 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:36:19 | INFO | train_inner | epoch 041:     80 / 1103 loss=3.261, nll_loss=1.749, ppl=3.36, wps=12836.9, ups=3.51, wpb=3662.4, bsz=143.3, num_updates=44200, lr=0.000150414, gnorm=0.87, train_wall=22, gb_free=6.6, wall=10193
2021-03-10 23:36:42 | INFO | train_inner | epoch 041:    180 / 1103 loss=3.243, nll_loss=1.728, ppl=3.31, wps=15987, ups=4.48, wpb=3566.9, bsz=145.6, num_updates=44300, lr=0.000150244, gnorm=0.887, train_wall=22, gb_free=6.9, wall=10215
2021-03-10 23:37:04 | INFO | train_inner | epoch 041:    280 / 1103 loss=3.217, nll_loss=1.699, ppl=3.25, wps=15811.9, ups=4.46, wpb=3548.3, bsz=154.2, num_updates=44400, lr=0.000150075, gnorm=0.877, train_wall=22, gb_free=6.9, wall=10237
2021-03-10 23:37:27 | INFO | train_inner | epoch 041:    380 / 1103 loss=3.265, nll_loss=1.755, ppl=3.37, wps=16208.4, ups=4.48, wpb=3618.4, bsz=149.6, num_updates=44500, lr=0.000149906, gnorm=0.866, train_wall=22, gb_free=7.1, wall=10260
2021-03-10 23:37:49 | INFO | train_inner | epoch 041:    480 / 1103 loss=3.248, nll_loss=1.733, ppl=3.33, wps=15568, ups=4.5, wpb=3463.2, bsz=140.2, num_updates=44600, lr=0.000149738, gnorm=0.919, train_wall=22, gb_free=7.2, wall=10282
2021-03-10 23:38:11 | INFO | train_inner | epoch 041:    580 / 1103 loss=3.264, nll_loss=1.753, ppl=3.37, wps=16256, ups=4.42, wpb=3677.4, bsz=150, num_updates=44700, lr=0.000149571, gnorm=0.878, train_wall=23, gb_free=7, wall=10305
2021-03-10 23:38:34 | INFO | train_inner | epoch 041:    680 / 1103 loss=3.247, nll_loss=1.734, ppl=3.33, wps=15972, ups=4.43, wpb=3602.3, bsz=155.1, num_updates=44800, lr=0.000149404, gnorm=0.883, train_wall=22, gb_free=7.2, wall=10327
2021-03-10 23:38:56 | INFO | train_inner | epoch 041:    780 / 1103 loss=3.261, nll_loss=1.749, ppl=3.36, wps=15799.9, ups=4.46, wpb=3543.7, bsz=145.2, num_updates=44900, lr=0.000149237, gnorm=0.912, train_wall=22, gb_free=7.2, wall=10350
2021-03-10 23:39:19 | INFO | train_inner | epoch 041:    880 / 1103 loss=3.28, nll_loss=1.77, ppl=3.41, wps=15951.2, ups=4.46, wpb=3575.8, bsz=136.5, num_updates=45000, lr=0.000149071, gnorm=0.908, train_wall=22, gb_free=7.5, wall=10372
2021-03-10 23:39:41 | INFO | train_inner | epoch 041:    980 / 1103 loss=3.3, nll_loss=1.793, ppl=3.47, wps=15869.7, ups=4.44, wpb=3575.3, bsz=126.3, num_updates=45100, lr=0.000148906, gnorm=0.908, train_wall=22, gb_free=6.8, wall=10395
2021-03-10 23:40:04 | INFO | train_inner | epoch 041:   1080 / 1103 loss=3.272, nll_loss=1.762, ppl=3.39, wps=15837.6, ups=4.46, wpb=3550.8, bsz=149.8, num_updates=45200, lr=0.000148741, gnorm=0.91, train_wall=22, gb_free=6.9, wall=10417
2021-03-10 23:40:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:40:13 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 3.933 | nll_loss 2.375 | ppl 5.19 | wps 48195.9 | wpb 2873.1 | bsz 115.6 | num_updates 45223 | best_loss 3.91
2021-03-10 23:40:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 45223 updates
2021-03-10 23:40:13 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:40:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 41 @ 45223 updates, score 3.933) (writing took 2.0460567511618137 seconds)
2021-03-10 23:40:15 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)
2021-03-10 23:40:15 | INFO | train | epoch 041 | loss 3.259 | nll_loss 1.747 | ppl 3.36 | wps 15569.3 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 45223 | lr 0.000148703 | gnorm 0.893 | train_wall 246 | gb_free 7.1 | wall 10428
2021-03-10 23:40:15 | INFO | fairseq.trainer | begin training epoch 42
2021-03-10 23:40:15 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:40:32 | INFO | train_inner | epoch 042:     77 / 1103 loss=3.232, nll_loss=1.716, ppl=3.28, wps=12796, ups=3.53, wpb=3625.1, bsz=146.5, num_updates=45300, lr=0.000148577, gnorm=0.89, train_wall=22, gb_free=6.9, wall=10445
2021-03-10 23:40:55 | INFO | train_inner | epoch 042:    177 / 1103 loss=3.243, nll_loss=1.727, ppl=3.31, wps=16024.1, ups=4.45, wpb=3603.7, bsz=139.2, num_updates=45400, lr=0.000148413, gnorm=0.878, train_wall=22, gb_free=6.9, wall=10468
2021-03-10 23:41:17 | INFO | train_inner | epoch 042:    277 / 1103 loss=3.201, nll_loss=1.679, ppl=3.2, wps=15814.8, ups=4.45, wpb=3553.1, bsz=144.5, num_updates=45500, lr=0.00014825, gnorm=0.892, train_wall=22, gb_free=6.9, wall=10490
2021-03-10 23:41:39 | INFO | train_inner | epoch 042:    377 / 1103 loss=3.262, nll_loss=1.749, ppl=3.36, wps=15972.3, ups=4.47, wpb=3570.6, bsz=131.7, num_updates=45600, lr=0.000148087, gnorm=0.896, train_wall=22, gb_free=7.2, wall=10513
2021-03-10 23:42:02 | INFO | train_inner | epoch 042:    477 / 1103 loss=3.254, nll_loss=1.741, ppl=3.34, wps=16006.3, ups=4.48, wpb=3570.6, bsz=145.9, num_updates=45700, lr=0.000147925, gnorm=0.901, train_wall=22, gb_free=7, wall=10535
2021-03-10 23:42:24 | INFO | train_inner | epoch 042:    577 / 1103 loss=3.264, nll_loss=1.751, ppl=3.37, wps=15862.8, ups=4.48, wpb=3540.5, bsz=135.1, num_updates=45800, lr=0.000147764, gnorm=0.905, train_wall=22, gb_free=7.1, wall=10557
2021-03-10 23:42:47 | INFO | train_inner | epoch 042:    677 / 1103 loss=3.242, nll_loss=1.729, ppl=3.31, wps=15996.4, ups=4.44, wpb=3600.3, bsz=156.6, num_updates=45900, lr=0.000147602, gnorm=0.881, train_wall=22, gb_free=6.9, wall=10580
2021-03-10 23:43:09 | INFO | train_inner | epoch 042:    777 / 1103 loss=3.245, nll_loss=1.731, ppl=3.32, wps=15935.8, ups=4.42, wpb=3604, bsz=147, num_updates=46000, lr=0.000147442, gnorm=0.89, train_wall=23, gb_free=6.9, wall=10602
2021-03-10 23:43:32 | INFO | train_inner | epoch 042:    877 / 1103 loss=3.25, nll_loss=1.738, ppl=3.34, wps=15676.4, ups=4.46, wpb=3512.3, bsz=155.8, num_updates=46100, lr=0.000147282, gnorm=0.911, train_wall=22, gb_free=7.8, wall=10625
2021-03-10 23:43:54 | INFO | train_inner | epoch 042:    977 / 1103 loss=3.265, nll_loss=1.755, ppl=3.38, wps=15872.6, ups=4.49, wpb=3536.3, bsz=148.8, num_updates=46200, lr=0.000147122, gnorm=0.917, train_wall=22, gb_free=7.7, wall=10647
2021-03-10 23:44:16 | INFO | train_inner | epoch 042:   1077 / 1103 loss=3.279, nll_loss=1.77, ppl=3.41, wps=16025.3, ups=4.42, wpb=3625.7, bsz=140.3, num_updates=46300, lr=0.000146964, gnorm=0.907, train_wall=23, gb_free=7.1, wall=10670
2021-03-10 23:44:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:44:26 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 3.93 | nll_loss 2.374 | ppl 5.18 | wps 48232 | wpb 2873.1 | bsz 115.6 | num_updates 46326 | best_loss 3.91
2021-03-10 23:44:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 46326 updates
2021-03-10 23:44:26 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:44:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:44:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 42 @ 46326 updates, score 3.93) (writing took 2.134742960333824 seconds)
2021-03-10 23:44:28 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)
2021-03-10 23:44:28 | INFO | train | epoch 042 | loss 3.248 | nll_loss 1.734 | ppl 3.33 | wps 15561.6 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 46326 | lr 0.000146922 | gnorm 0.896 | train_wall 246 | gb_free 7.1 | wall 10681
2021-03-10 23:44:28 | INFO | fairseq.trainer | begin training epoch 43
2021-03-10 23:44:28 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:44:44 | INFO | train_inner | epoch 043:     74 / 1103 loss=3.209, nll_loss=1.692, ppl=3.23, wps=13500.2, ups=3.68, wpb=3666.3, bsz=163.1, num_updates=46400, lr=0.000146805, gnorm=0.864, train_wall=21, gb_free=7.2, wall=10697
2021-03-10 23:45:04 | INFO | train_inner | epoch 043:    174 / 1103 loss=3.198, nll_loss=1.678, ppl=3.2, wps=17377.9, ups=4.93, wpb=3527.7, bsz=164.4, num_updates=46500, lr=0.000146647, gnorm=0.884, train_wall=20, gb_free=7.1, wall=10717
2021-03-10 23:45:24 | INFO | train_inner | epoch 043:    274 / 1103 loss=3.238, nll_loss=1.721, ppl=3.3, wps=17521.7, ups=4.87, wpb=3597.2, bsz=131.4, num_updates=46600, lr=0.00014649, gnorm=0.903, train_wall=20, gb_free=7.1, wall=10738
2021-03-10 23:45:45 | INFO | train_inner | epoch 043:    374 / 1103 loss=3.249, nll_loss=1.732, ppl=3.32, wps=17431, ups=4.85, wpb=3592.6, bsz=129.9, num_updates=46700, lr=0.000146333, gnorm=0.915, train_wall=21, gb_free=6.5, wall=10758
2021-03-10 23:46:06 | INFO | train_inner | epoch 043:    474 / 1103 loss=3.243, nll_loss=1.727, ppl=3.31, wps=17286.8, ups=4.88, wpb=3542.2, bsz=134.3, num_updates=46800, lr=0.000146176, gnorm=0.91, train_wall=20, gb_free=7.4, wall=10779
2021-03-10 23:46:26 | INFO | train_inner | epoch 043:    574 / 1103 loss=3.211, nll_loss=1.693, ppl=3.23, wps=17505.8, ups=4.83, wpb=3624.9, bsz=156.8, num_updates=46900, lr=0.00014602, gnorm=0.877, train_wall=21, gb_free=7.1, wall=10799
2021-03-10 23:46:47 | INFO | train_inner | epoch 043:    674 / 1103 loss=3.244, nll_loss=1.728, ppl=3.31, wps=17126.2, ups=4.86, wpb=3522.8, bsz=142.2, num_updates=47000, lr=0.000145865, gnorm=0.921, train_wall=20, gb_free=7, wall=10820
2021-03-10 23:47:08 | INFO | train_inner | epoch 043:    774 / 1103 loss=3.233, nll_loss=1.718, ppl=3.29, wps=16832.4, ups=4.65, wpb=3619.6, bsz=155.6, num_updates=47100, lr=0.00014571, gnorm=0.889, train_wall=21, gb_free=7.1, wall=10842
2021-03-10 23:47:31 | INFO | train_inner | epoch 043:    874 / 1103 loss=3.248, nll_loss=1.735, ppl=3.33, wps=15977.1, ups=4.43, wpb=3609.6, bsz=143.6, num_updates=47200, lr=0.000145556, gnorm=0.886, train_wall=22, gb_free=7.7, wall=10864
2021-03-10 23:47:53 | INFO | train_inner | epoch 043:    974 / 1103 loss=3.271, nll_loss=1.76, ppl=3.39, wps=15884.5, ups=4.47, wpb=3557.6, bsz=141.4, num_updates=47300, lr=0.000145402, gnorm=0.921, train_wall=22, gb_free=7, wall=10886
2021-03-10 23:48:16 | INFO | train_inner | epoch 043:   1074 / 1103 loss=3.253, nll_loss=1.74, ppl=3.34, wps=15808.5, ups=4.44, wpb=3557, bsz=145.3, num_updates=47400, lr=0.000145248, gnorm=0.9, train_wall=22, gb_free=7.2, wall=10909
2021-03-10 23:48:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:48:26 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 3.943 | nll_loss 2.387 | ppl 5.23 | wps 48222.1 | wpb 2873.1 | bsz 115.6 | num_updates 47429 | best_loss 3.91
2021-03-10 23:48:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 47429 updates
2021-03-10 23:48:26 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:48:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:48:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 43 @ 47429 updates, score 3.943) (writing took 2.394577607512474 seconds)
2021-03-10 23:48:28 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)
2021-03-10 23:48:28 | INFO | train | epoch 043 | loss 3.237 | nll_loss 1.721 | ppl 3.3 | wps 16436.3 | ups 4.59 | wpb 3577.8 | bsz 145.3 | num_updates 47429 | lr 0.000145204 | gnorm 0.899 | train_wall 233 | gb_free 6.9 | wall 10922
2021-03-10 23:48:28 | INFO | fairseq.trainer | begin training epoch 44
2021-03-10 23:48:28 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:48:45 | INFO | train_inner | epoch 044:     71 / 1103 loss=3.204, nll_loss=1.683, ppl=3.21, wps=12382.2, ups=3.46, wpb=3576.8, bsz=143.8, num_updates=47500, lr=0.000145095, gnorm=0.89, train_wall=22, gb_free=7.1, wall=10938
2021-03-10 23:49:07 | INFO | train_inner | epoch 044:    171 / 1103 loss=3.215, nll_loss=1.693, ppl=3.23, wps=15959, ups=4.48, wpb=3566.2, bsz=132, num_updates=47600, lr=0.000144943, gnorm=0.906, train_wall=22, gb_free=6.9, wall=10960
2021-03-10 23:49:29 | INFO | train_inner | epoch 044:    271 / 1103 loss=3.21, nll_loss=1.688, ppl=3.22, wps=15960.2, ups=4.49, wpb=3558.2, bsz=138.5, num_updates=47700, lr=0.000144791, gnorm=0.916, train_wall=22, gb_free=6.8, wall=10983
2021-03-10 23:49:52 | INFO | train_inner | epoch 044:    371 / 1103 loss=3.147, nll_loss=1.622, ppl=3.08, wps=15978.7, ups=4.41, wpb=3622.3, bsz=187.9, num_updates=47800, lr=0.000144639, gnorm=0.869, train_wall=23, gb_free=7.1, wall=11005
2021-03-10 23:50:15 | INFO | train_inner | epoch 044:    471 / 1103 loss=3.226, nll_loss=1.708, ppl=3.27, wps=16082.9, ups=4.43, wpb=3632.9, bsz=140, num_updates=47900, lr=0.000144488, gnorm=0.887, train_wall=22, gb_free=6.9, wall=11028
2021-03-10 23:50:37 | INFO | train_inner | epoch 044:    571 / 1103 loss=3.232, nll_loss=1.717, ppl=3.29, wps=16046.5, ups=4.5, wpb=3562.6, bsz=151.1, num_updates=48000, lr=0.000144338, gnorm=0.903, train_wall=22, gb_free=7.7, wall=11050
2021-03-10 23:50:59 | INFO | train_inner | epoch 044:    671 / 1103 loss=3.267, nll_loss=1.756, ppl=3.38, wps=16255.4, ups=4.46, wpb=3642.5, bsz=146.6, num_updates=48100, lr=0.000144187, gnorm=0.892, train_wall=22, gb_free=7.4, wall=11072
2021-03-10 23:51:22 | INFO | train_inner | epoch 044:    771 / 1103 loss=3.245, nll_loss=1.729, ppl=3.31, wps=16011.1, ups=4.47, wpb=3584.9, bsz=136.5, num_updates=48200, lr=0.000144038, gnorm=0.902, train_wall=22, gb_free=6.9, wall=11095
2021-03-10 23:51:44 | INFO | train_inner | epoch 044:    871 / 1103 loss=3.222, nll_loss=1.705, ppl=3.26, wps=15779.5, ups=4.5, wpb=3510.1, bsz=147.3, num_updates=48300, lr=0.000143889, gnorm=0.918, train_wall=22, gb_free=7.1, wall=11117
2021-03-10 23:52:07 | INFO | train_inner | epoch 044:    971 / 1103 loss=3.256, nll_loss=1.742, ppl=3.34, wps=15873.6, ups=4.41, wpb=3596.8, bsz=137.4, num_updates=48400, lr=0.00014374, gnorm=0.908, train_wall=23, gb_free=7.1, wall=11140
2021-03-10 23:52:29 | INFO | train_inner | epoch 044:   1071 / 1103 loss=3.29, nll_loss=1.779, ppl=3.43, wps=15651.9, ups=4.53, wpb=3458.6, bsz=125.2, num_updates=48500, lr=0.000143592, gnorm=0.963, train_wall=22, gb_free=7.1, wall=11162
2021-03-10 23:52:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:52:40 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 3.944 | nll_loss 2.388 | ppl 5.24 | wps 48223.5 | wpb 2873.1 | bsz 115.6 | num_updates 48532 | best_loss 3.91
2021-03-10 23:52:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 48532 updates
2021-03-10 23:52:40 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:52:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:52:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 44 @ 48532 updates, score 3.944) (writing took 2.215151432901621 seconds)
2021-03-10 23:52:42 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)
2021-03-10 23:52:42 | INFO | train | epoch 044 | loss 3.227 | nll_loss 1.71 | ppl 3.27 | wps 15568.8 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 48532 | lr 0.000143544 | gnorm 0.904 | train_wall 246 | gb_free 6.9 | wall 11175
2021-03-10 23:52:42 | INFO | fairseq.trainer | begin training epoch 45
2021-03-10 23:52:42 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:52:57 | INFO | train_inner | epoch 045:     68 / 1103 loss=3.19, nll_loss=1.669, ppl=3.18, wps=12650.7, ups=3.47, wpb=3640.8, bsz=161.8, num_updates=48600, lr=0.000143444, gnorm=0.868, train_wall=22, gb_free=7, wall=11191
2021-03-10 23:53:20 | INFO | train_inner | epoch 045:    168 / 1103 loss=3.203, nll_loss=1.682, ppl=3.21, wps=16028, ups=4.48, wpb=3581.5, bsz=144.6, num_updates=48700, lr=0.000143296, gnorm=0.915, train_wall=22, gb_free=7.1, wall=11213
2021-03-10 23:53:42 | INFO | train_inner | epoch 045:    268 / 1103 loss=3.179, nll_loss=1.655, ppl=3.15, wps=15981.1, ups=4.42, wpb=3611.6, bsz=159.2, num_updates=48800, lr=0.00014315, gnorm=0.899, train_wall=22, gb_free=7, wall=11236
2021-03-10 23:54:04 | INFO | train_inner | epoch 045:    368 / 1103 loss=3.202, nll_loss=1.678, ppl=3.2, wps=15630.8, ups=4.54, wpb=3440, bsz=135.4, num_updates=48900, lr=0.000143003, gnorm=0.944, train_wall=22, gb_free=7.3, wall=11258
2021-03-10 23:54:27 | INFO | train_inner | epoch 045:    468 / 1103 loss=3.199, nll_loss=1.677, ppl=3.2, wps=16105.8, ups=4.47, wpb=3602.7, bsz=152.5, num_updates=49000, lr=0.000142857, gnorm=0.886, train_wall=22, gb_free=7.1, wall=11280
2021-03-10 23:54:49 | INFO | train_inner | epoch 045:    568 / 1103 loss=3.205, nll_loss=1.685, ppl=3.22, wps=16019.2, ups=4.43, wpb=3619.3, bsz=152.1, num_updates=49100, lr=0.000142712, gnorm=0.902, train_wall=22, gb_free=7, wall=11302
2021-03-10 23:55:12 | INFO | train_inner | epoch 045:    668 / 1103 loss=3.237, nll_loss=1.722, ppl=3.3, wps=16291.7, ups=4.49, wpb=3630.8, bsz=150.3, num_updates=49200, lr=0.000142566, gnorm=0.894, train_wall=22, gb_free=7.1, wall=11325
2021-03-10 23:55:34 | INFO | train_inner | epoch 045:    768 / 1103 loss=3.223, nll_loss=1.704, ppl=3.26, wps=15897.2, ups=4.44, wpb=3578.5, bsz=147, num_updates=49300, lr=0.000142422, gnorm=0.904, train_wall=22, gb_free=7.2, wall=11347
2021-03-10 23:55:57 | INFO | train_inner | epoch 045:    868 / 1103 loss=3.273, nll_loss=1.761, ppl=3.39, wps=16010.1, ups=4.47, wpb=3585.4, bsz=128.3, num_updates=49400, lr=0.000142278, gnorm=0.927, train_wall=22, gb_free=7.4, wall=11370
2021-03-10 23:56:19 | INFO | train_inner | epoch 045:    968 / 1103 loss=3.247, nll_loss=1.732, ppl=3.32, wps=15831.9, ups=4.48, wpb=3537.5, bsz=138.6, num_updates=49500, lr=0.000142134, gnorm=0.928, train_wall=22, gb_free=6.5, wall=11392
2021-03-10 23:56:42 | INFO | train_inner | epoch 045:   1068 / 1103 loss=3.237, nll_loss=1.721, ppl=3.3, wps=15951, ups=4.42, wpb=3609.2, bsz=144.6, num_updates=49600, lr=0.00014199, gnorm=0.9, train_wall=23, gb_free=7.1, wall=11415
2021-03-10 23:56:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-10 23:56:53 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 3.936 | nll_loss 2.378 | ppl 5.2 | wps 48204.5 | wpb 2873.1 | bsz 115.6 | num_updates 49635 | best_loss 3.91
2021-03-10 23:56:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 49635 updates
2021-03-10 23:56:53 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:56:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-10 23:56:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 45 @ 49635 updates, score 3.936) (writing took 2.081395745277405 seconds)
2021-03-10 23:56:55 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)
2021-03-10 23:56:55 | INFO | train | epoch 045 | loss 3.219 | nll_loss 1.7 | ppl 3.25 | wps 15590.4 | ups 4.36 | wpb 3577.8 | bsz 145.3 | num_updates 49635 | lr 0.00014194 | gnorm 0.909 | train_wall 246 | gb_free 7.4 | wall 11428
2021-03-10 23:56:55 | INFO | fairseq.trainer | begin training epoch 46
2021-03-10 23:56:55 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-10 23:57:10 | INFO | train_inner | epoch 046:     65 / 1103 loss=3.177, nll_loss=1.651, ppl=3.14, wps=12401, ups=3.52, wpb=3524.2, bsz=144.9, num_updates=49700, lr=0.000141848, gnorm=0.905, train_wall=22, gb_free=7.1, wall=11443
2021-03-10 23:57:32 | INFO | train_inner | epoch 046:    165 / 1103 loss=3.175, nll_loss=1.647, ppl=3.13, wps=15553.9, ups=4.54, wpb=3428.9, bsz=135.4, num_updates=49800, lr=0.000141705, gnorm=0.939, train_wall=22, gb_free=6.9, wall=11465
2021-03-10 23:57:54 | INFO | train_inner | epoch 046:    265 / 1103 loss=3.192, nll_loss=1.667, ppl=3.18, wps=15762, ups=4.45, wpb=3539.7, bsz=137.8, num_updates=49900, lr=0.000141563, gnorm=0.908, train_wall=22, gb_free=6.8, wall=11488
2021-03-10 23:58:17 | INFO | train_inner | epoch 046:    365 / 1103 loss=3.21, nll_loss=1.688, ppl=3.22, wps=15653, ups=4.46, wpb=3511, bsz=133.8, num_updates=50000, lr=0.000141421, gnorm=0.938, train_wall=22, gb_free=7.2, wall=11510
2021-03-10 23:58:39 | INFO | train_inner | epoch 046:    465 / 1103 loss=3.2, nll_loss=1.678, ppl=3.2, wps=16056.8, ups=4.42, wpb=3633.6, bsz=141.3, num_updates=50100, lr=0.00014128, gnorm=0.897, train_wall=23, gb_free=7, wall=11533
2021-03-10 23:59:02 | INFO | train_inner | epoch 046:    565 / 1103 loss=3.229, nll_loss=1.712, ppl=3.28, wps=16112.3, ups=4.46, wpb=3610.6, bsz=143.3, num_updates=50200, lr=0.000141139, gnorm=0.9, train_wall=22, gb_free=7.2, wall=11555
2021-03-10 23:59:25 | INFO | train_inner | epoch 046:    665 / 1103 loss=3.189, nll_loss=1.667, ppl=3.17, wps=16029.1, ups=4.4, wpb=3639.4, bsz=153, num_updates=50300, lr=0.000140999, gnorm=0.898, train_wall=23, gb_free=6.9, wall=11578
2021-03-10 23:59:47 | INFO | train_inner | epoch 046:    765 / 1103 loss=3.227, nll_loss=1.711, ppl=3.27, wps=16008.6, ups=4.46, wpb=3587.7, bsz=145, num_updates=50400, lr=0.000140859, gnorm=0.903, train_wall=22, gb_free=6.9, wall=11600
2021-03-11 00:00:09 | INFO | train_inner | epoch 046:    865 / 1103 loss=3.185, nll_loss=1.664, ppl=3.17, wps=15897.4, ups=4.46, wpb=3566.9, bsz=172.6, num_updates=50500, lr=0.00014072, gnorm=0.905, train_wall=22, gb_free=7.4, wall=11623
2021-03-11 00:00:32 | INFO | train_inner | epoch 046:    965 / 1103 loss=3.216, nll_loss=1.698, ppl=3.24, wps=16197.7, ups=4.41, wpb=3672.1, bsz=151.6, num_updates=50600, lr=0.00014058, gnorm=0.894, train_wall=23, gb_free=7.2, wall=11645
2021-03-11 00:00:55 | INFO | train_inner | epoch 046:   1065 / 1103 loss=3.269, nll_loss=1.757, ppl=3.38, wps=15928.7, ups=4.47, wpb=3567.2, bsz=135.8, num_updates=50700, lr=0.000140442, gnorm=0.937, train_wall=22, gb_free=7.2, wall=11668
2021-03-11 00:01:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:01:07 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 3.931 | nll_loss 2.377 | ppl 5.2 | wps 48195.7 | wpb 2873.1 | bsz 115.6 | num_updates 50738 | best_loss 3.91
2021-03-11 00:01:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 50738 updates
2021-03-11 00:01:07 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:01:09 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:01:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 46 @ 50738 updates, score 3.931) (writing took 2.0852617993950844 seconds)
2021-03-11 00:01:09 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)
2021-03-11 00:01:09 | INFO | train | epoch 046 | loss 3.208 | nll_loss 1.687 | ppl 3.22 | wps 15551.1 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 50738 | lr 0.000140389 | gnorm 0.91 | train_wall 246 | gb_free 6.9 | wall 11682
2021-03-11 00:01:09 | INFO | fairseq.trainer | begin training epoch 47
2021-03-11 00:01:09 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:01:23 | INFO | train_inner | epoch 047:     62 / 1103 loss=3.209, nll_loss=1.687, ppl=3.22, wps=12651.1, ups=3.53, wpb=3580.2, bsz=141.9, num_updates=50800, lr=0.000140303, gnorm=0.915, train_wall=22, gb_free=6.6, wall=11696
2021-03-11 00:01:45 | INFO | train_inner | epoch 047:    162 / 1103 loss=3.192, nll_loss=1.666, ppl=3.17, wps=15942.2, ups=4.56, wpb=3498.3, bsz=133.3, num_updates=50900, lr=0.000140165, gnorm=0.937, train_wall=22, gb_free=6.9, wall=11718
2021-03-11 00:02:07 | INFO | train_inner | epoch 047:    262 / 1103 loss=3.197, nll_loss=1.674, ppl=3.19, wps=15981.8, ups=4.45, wpb=3591.4, bsz=146.6, num_updates=51000, lr=0.000140028, gnorm=0.907, train_wall=22, gb_free=7.3, wall=11740
2021-03-11 00:02:30 | INFO | train_inner | epoch 047:    362 / 1103 loss=3.174, nll_loss=1.648, ppl=3.13, wps=16123.5, ups=4.41, wpb=3653.9, bsz=151.7, num_updates=51100, lr=0.000139891, gnorm=0.898, train_wall=23, gb_free=7.2, wall=11763
2021-03-11 00:02:53 | INFO | train_inner | epoch 047:    462 / 1103 loss=3.156, nll_loss=1.627, ppl=3.09, wps=15764.5, ups=4.42, wpb=3566.8, bsz=153.9, num_updates=51200, lr=0.000139754, gnorm=0.894, train_wall=23, gb_free=7.4, wall=11786
2021-03-11 00:03:15 | INFO | train_inner | epoch 047:    562 / 1103 loss=3.226, nll_loss=1.707, ppl=3.26, wps=15802.3, ups=4.46, wpb=3544.5, bsz=138.1, num_updates=51300, lr=0.000139618, gnorm=0.93, train_wall=22, gb_free=6.9, wall=11808
2021-03-11 00:03:37 | INFO | train_inner | epoch 047:    662 / 1103 loss=3.2, nll_loss=1.679, ppl=3.2, wps=16232.4, ups=4.51, wpb=3597.7, bsz=153, num_updates=51400, lr=0.000139482, gnorm=0.905, train_wall=22, gb_free=6.8, wall=11830
2021-03-11 00:04:00 | INFO | train_inner | epoch 047:    762 / 1103 loss=3.202, nll_loss=1.68, ppl=3.2, wps=16008.1, ups=4.44, wpb=3605.8, bsz=141.8, num_updates=51500, lr=0.000139347, gnorm=0.903, train_wall=22, gb_free=6.9, wall=11853
2021-03-11 00:04:22 | INFO | train_inner | epoch 047:    862 / 1103 loss=3.209, nll_loss=1.689, ppl=3.22, wps=15777.3, ups=4.48, wpb=3523.5, bsz=144.2, num_updates=51600, lr=0.000139212, gnorm=0.929, train_wall=22, gb_free=7, wall=11875
2021-03-11 00:04:44 | INFO | train_inner | epoch 047:    962 / 1103 loss=3.205, nll_loss=1.685, ppl=3.22, wps=15979.6, ups=4.48, wpb=3564.5, bsz=151, num_updates=51700, lr=0.000139077, gnorm=0.915, train_wall=22, gb_free=7.1, wall=11897
2021-03-11 00:05:07 | INFO | train_inner | epoch 047:   1062 / 1103 loss=3.226, nll_loss=1.708, ppl=3.27, wps=16025.9, ups=4.48, wpb=3577.3, bsz=134.4, num_updates=51800, lr=0.000138943, gnorm=0.921, train_wall=22, gb_free=7, wall=11920
2021-03-11 00:05:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:05:20 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 3.932 | nll_loss 2.378 | ppl 5.2 | wps 48189.4 | wpb 2873.1 | bsz 115.6 | num_updates 51841 | best_loss 3.91
2021-03-11 00:05:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 51841 updates
2021-03-11 00:05:20 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:05:22 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:05:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 47 @ 51841 updates, score 3.932) (writing took 2.1811452992260456 seconds)
2021-03-11 00:05:22 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)
2021-03-11 00:05:22 | INFO | train | epoch 047 | loss 3.199 | nll_loss 1.677 | ppl 3.2 | wps 15599.3 | ups 4.36 | wpb 3577.8 | bsz 145.3 | num_updates 51841 | lr 0.000138888 | gnorm 0.912 | train_wall 246 | gb_free 7.2 | wall 11935
2021-03-11 00:05:22 | INFO | fairseq.trainer | begin training epoch 48
2021-03-11 00:05:22 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:05:35 | INFO | train_inner | epoch 048:     59 / 1103 loss=3.193, nll_loss=1.671, ppl=3.18, wps=12678.4, ups=3.51, wpb=3615.6, bsz=145.1, num_updates=51900, lr=0.000138809, gnorm=0.89, train_wall=22, gb_free=7, wall=11948
2021-03-11 00:05:58 | INFO | train_inner | epoch 048:    159 / 1103 loss=3.18, nll_loss=1.654, ppl=3.15, wps=16237.2, ups=4.47, wpb=3635.6, bsz=151.1, num_updates=52000, lr=0.000138675, gnorm=0.918, train_wall=22, gb_free=6.9, wall=11971
2021-03-11 00:06:20 | INFO | train_inner | epoch 048:    259 / 1103 loss=3.162, nll_loss=1.632, ppl=3.1, wps=15861.9, ups=4.43, wpb=3583.7, bsz=139.2, num_updates=52100, lr=0.000138542, gnorm=0.913, train_wall=22, gb_free=6.9, wall=11993
2021-03-11 00:06:43 | INFO | train_inner | epoch 048:    359 / 1103 loss=3.156, nll_loss=1.626, ppl=3.09, wps=15668.5, ups=4.44, wpb=3530.6, bsz=135.7, num_updates=52200, lr=0.000138409, gnorm=0.918, train_wall=22, gb_free=6.6, wall=12016
2021-03-11 00:07:05 | INFO | train_inner | epoch 048:    459 / 1103 loss=3.202, nll_loss=1.679, ppl=3.2, wps=15872, ups=4.43, wpb=3580.8, bsz=134, num_updates=52300, lr=0.000138277, gnorm=0.921, train_wall=22, gb_free=6.8, wall=12038
2021-03-11 00:07:28 | INFO | train_inner | epoch 048:    559 / 1103 loss=3.181, nll_loss=1.657, ppl=3.15, wps=16063.1, ups=4.41, wpb=3640.6, bsz=151.5, num_updates=52400, lr=0.000138145, gnorm=0.895, train_wall=23, gb_free=6.7, wall=12061
2021-03-11 00:07:50 | INFO | train_inner | epoch 048:    659 / 1103 loss=3.204, nll_loss=1.683, ppl=3.21, wps=16115.1, ups=4.45, wpb=3624.7, bsz=148.5, num_updates=52500, lr=0.000138013, gnorm=0.907, train_wall=22, gb_free=7, wall=12083
2021-03-11 00:08:13 | INFO | train_inner | epoch 048:    759 / 1103 loss=3.223, nll_loss=1.702, ppl=3.25, wps=15737.1, ups=4.47, wpb=3524.2, bsz=128.7, num_updates=52600, lr=0.000137882, gnorm=0.941, train_wall=22, gb_free=7.1, wall=12106
2021-03-11 00:08:35 | INFO | train_inner | epoch 048:    859 / 1103 loss=3.201, nll_loss=1.678, ppl=3.2, wps=15687.1, ups=4.48, wpb=3499.4, bsz=144.9, num_updates=52700, lr=0.000137751, gnorm=0.968, train_wall=22, gb_free=6.8, wall=12128
2021-03-11 00:08:57 | INFO | train_inner | epoch 048:    959 / 1103 loss=3.2, nll_loss=1.68, ppl=3.2, wps=16010.1, ups=4.48, wpb=3571.8, bsz=156.7, num_updates=52800, lr=0.00013762, gnorm=0.909, train_wall=22, gb_free=7, wall=12151
2021-03-11 00:09:20 | INFO | train_inner | epoch 048:   1059 / 1103 loss=3.197, nll_loss=1.676, ppl=3.2, wps=15921.3, ups=4.42, wpb=3605.5, bsz=162.2, num_updates=52900, lr=0.00013749, gnorm=0.907, train_wall=23, gb_free=6.8, wall=12173
2021-03-11 00:09:30 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:09:34 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 3.941 | nll_loss 2.382 | ppl 5.21 | wps 48176.8 | wpb 2873.1 | bsz 115.6 | num_updates 52944 | best_loss 3.91
2021-03-11 00:09:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 52944 updates
2021-03-11 00:09:34 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:09:36 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:09:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 48 @ 52944 updates, score 3.941) (writing took 2.0426352694630623 seconds)
2021-03-11 00:09:36 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)
2021-03-11 00:09:36 | INFO | train | epoch 048 | loss 3.189 | nll_loss 1.666 | ppl 3.17 | wps 15537.3 | ups 4.34 | wpb 3577.8 | bsz 145.3 | num_updates 52944 | lr 0.000137433 | gnorm 0.918 | train_wall 247 | gb_free 7 | wall 12189
2021-03-11 00:09:36 | INFO | fairseq.trainer | begin training epoch 49
2021-03-11 00:09:36 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:09:49 | INFO | train_inner | epoch 049:     56 / 1103 loss=3.167, nll_loss=1.642, ppl=3.12, wps=12683, ups=3.5, wpb=3627.3, bsz=157, num_updates=53000, lr=0.000137361, gnorm=0.884, train_wall=22, gb_free=6.9, wall=12202
2021-03-11 00:10:11 | INFO | train_inner | epoch 049:    156 / 1103 loss=3.156, nll_loss=1.626, ppl=3.09, wps=16090.4, ups=4.43, wpb=3634.8, bsz=139, num_updates=53100, lr=0.000137231, gnorm=0.897, train_wall=22, gb_free=6.9, wall=12224
2021-03-11 00:10:34 | INFO | train_inner | epoch 049:    256 / 1103 loss=3.159, nll_loss=1.63, ppl=3.1, wps=15923.7, ups=4.45, wpb=3575.2, bsz=146.5, num_updates=53200, lr=0.000137102, gnorm=0.909, train_wall=22, gb_free=7.2, wall=12247
2021-03-11 00:10:56 | INFO | train_inner | epoch 049:    356 / 1103 loss=3.137, nll_loss=1.605, ppl=3.04, wps=15683.9, ups=4.44, wpb=3530.6, bsz=150.3, num_updates=53300, lr=0.000136973, gnorm=0.91, train_wall=22, gb_free=7, wall=12269
2021-03-11 00:11:19 | INFO | train_inner | epoch 049:    456 / 1103 loss=3.198, nll_loss=1.673, ppl=3.19, wps=15891.5, ups=4.42, wpb=3597.4, bsz=137.6, num_updates=53400, lr=0.000136845, gnorm=0.922, train_wall=23, gb_free=6.9, wall=12292
2021-03-11 00:11:41 | INFO | train_inner | epoch 049:    556 / 1103 loss=3.159, nll_loss=1.631, ppl=3.1, wps=15839.5, ups=4.47, wpb=3546.4, bsz=147.5, num_updates=53500, lr=0.000136717, gnorm=0.909, train_wall=22, gb_free=6.9, wall=12314
2021-03-11 00:12:02 | INFO | train_inner | epoch 049:    656 / 1103 loss=3.158, nll_loss=1.63, ppl=3.09, wps=16772.5, ups=4.77, wpb=3519.3, bsz=152.6, num_updates=53600, lr=0.00013659, gnorm=0.934, train_wall=21, gb_free=6.9, wall=12335
2021-03-11 00:12:23 | INFO | train_inner | epoch 049:    756 / 1103 loss=3.218, nll_loss=1.697, ppl=3.24, wps=17288.6, ups=4.86, wpb=3558.8, bsz=129.8, num_updates=53700, lr=0.000136462, gnorm=0.953, train_wall=20, gb_free=6.9, wall=12356
2021-03-11 00:12:43 | INFO | train_inner | epoch 049:    856 / 1103 loss=3.207, nll_loss=1.686, ppl=3.22, wps=17430.7, ups=4.83, wpb=3608.1, bsz=145.5, num_updates=53800, lr=0.000136335, gnorm=0.917, train_wall=21, gb_free=7.9, wall=12377
2021-03-11 00:13:04 | INFO | train_inner | epoch 049:    956 / 1103 loss=3.221, nll_loss=1.702, ppl=3.25, wps=17347.6, ups=4.86, wpb=3571.6, bsz=140.9, num_updates=53900, lr=0.000136209, gnorm=0.938, train_wall=20, gb_free=7, wall=12397
2021-03-11 00:13:24 | INFO | train_inner | epoch 049:   1056 / 1103 loss=3.22, nll_loss=1.7, ppl=3.25, wps=17383.2, ups=4.9, wpb=3546, bsz=138.5, num_updates=54000, lr=0.000136083, gnorm=0.953, train_wall=20, gb_free=6.9, wall=12418
2021-03-11 00:13:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:13:38 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 3.934 | nll_loss 2.378 | ppl 5.2 | wps 48055.6 | wpb 2873.1 | bsz 115.6 | num_updates 54047 | best_loss 3.91
2021-03-11 00:13:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 54047 updates
2021-03-11 00:13:38 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:13:40 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 49 @ 54047 updates, score 3.934) (writing took 2.134369295090437 seconds)
2021-03-11 00:13:40 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)
2021-03-11 00:13:40 | INFO | train | epoch 049 | loss 3.182 | nll_loss 1.657 | ppl 3.15 | wps 16150.3 | ups 4.51 | wpb 3577.8 | bsz 145.3 | num_updates 54047 | lr 0.000136024 | gnorm 0.922 | train_wall 237 | gb_free 7.2 | wall 12433
2021-03-11 00:13:40 | INFO | fairseq.trainer | begin training epoch 50
2021-03-11 00:13:40 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:13:51 | INFO | train_inner | epoch 050:     53 / 1103 loss=3.168, nll_loss=1.643, ppl=3.12, wps=13399.5, ups=3.74, wpb=3586.7, bsz=159, num_updates=54100, lr=0.000135957, gnorm=0.931, train_wall=21, gb_free=6.9, wall=12444
2021-03-11 00:14:12 | INFO | train_inner | epoch 050:    153 / 1103 loss=3.139, nll_loss=1.607, ppl=3.05, wps=17478.9, ups=4.81, wpb=3636.2, bsz=147.4, num_updates=54200, lr=0.000135831, gnorm=0.889, train_wall=21, gb_free=7.1, wall=12465
2021-03-11 00:14:33 | INFO | train_inner | epoch 050:    253 / 1103 loss=3.145, nll_loss=1.616, ppl=3.07, wps=17464.3, ups=4.85, wpb=3602.3, bsz=162.8, num_updates=54300, lr=0.000135706, gnorm=0.906, train_wall=21, gb_free=7.5, wall=12486
2021-03-11 00:14:53 | INFO | train_inner | epoch 050:    353 / 1103 loss=3.183, nll_loss=1.654, ppl=3.15, wps=17145.4, ups=4.95, wpb=3461.3, bsz=118.6, num_updates=54400, lr=0.000135582, gnorm=0.988, train_wall=20, gb_free=7.2, wall=12506
2021-03-11 00:15:13 | INFO | train_inner | epoch 050:    453 / 1103 loss=3.198, nll_loss=1.674, ppl=3.19, wps=17685.8, ups=4.84, wpb=3652, bsz=136.9, num_updates=54500, lr=0.000135457, gnorm=0.908, train_wall=21, gb_free=7, wall=12527
2021-03-11 00:15:34 | INFO | train_inner | epoch 050:    553 / 1103 loss=3.168, nll_loss=1.64, ppl=3.12, wps=17276.7, ups=4.8, wpb=3598.2, bsz=142.6, num_updates=54600, lr=0.000135333, gnorm=0.918, train_wall=21, gb_free=6.6, wall=12547
2021-03-11 00:15:55 | INFO | train_inner | epoch 050:    653 / 1103 loss=3.166, nll_loss=1.639, ppl=3.11, wps=17352, ups=4.84, wpb=3585, bsz=152.5, num_updates=54700, lr=0.000135209, gnorm=0.92, train_wall=21, gb_free=6.9, wall=12568
2021-03-11 00:16:16 | INFO | train_inner | epoch 050:    753 / 1103 loss=3.176, nll_loss=1.651, ppl=3.14, wps=17327.6, ups=4.86, wpb=3566.7, bsz=155.9, num_updates=54800, lr=0.000135086, gnorm=0.918, train_wall=20, gb_free=7, wall=12589
2021-03-11 00:16:36 | INFO | train_inner | epoch 050:    853 / 1103 loss=3.182, nll_loss=1.658, ppl=3.16, wps=17385.8, ups=4.84, wpb=3590.8, bsz=149.4, num_updates=54900, lr=0.000134963, gnorm=0.916, train_wall=21, gb_free=6.9, wall=12609
2021-03-11 00:16:57 | INFO | train_inner | epoch 050:    953 / 1103 loss=3.185, nll_loss=1.663, ppl=3.17, wps=17561.5, ups=4.83, wpb=3634, bsz=161.8, num_updates=55000, lr=0.00013484, gnorm=0.916, train_wall=21, gb_free=6.8, wall=12630
2021-03-11 00:17:18 | INFO | train_inner | epoch 050:   1053 / 1103 loss=3.206, nll_loss=1.683, ppl=3.21, wps=16816.3, ups=4.82, wpb=3489.9, bsz=133.8, num_updates=55100, lr=0.000134718, gnorm=0.949, train_wall=21, gb_free=7.2, wall=12651
2021-03-11 00:17:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:17:33 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 3.95 | nll_loss 2.395 | ppl 5.26 | wps 48118.1 | wpb 2873.1 | bsz 115.6 | num_updates 55150 | best_loss 3.91
2021-03-11 00:17:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 55150 updates
2021-03-11 00:17:33 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:17:35 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:17:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 50 @ 55150 updates, score 3.95) (writing took 2.1663844250142574 seconds)
2021-03-11 00:17:35 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)
2021-03-11 00:17:35 | INFO | train | epoch 050 | loss 3.173 | nll_loss 1.647 | ppl 3.13 | wps 16812.6 | ups 4.7 | wpb 3577.8 | bsz 145.3 | num_updates 55150 | lr 0.000134656 | gnorm 0.923 | train_wall 227 | gb_free 6.9 | wall 12668
2021-03-11 00:17:35 | INFO | fairseq.trainer | begin training epoch 51
2021-03-11 00:17:35 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:17:46 | INFO | train_inner | epoch 051:     50 / 1103 loss=3.168, nll_loss=1.641, ppl=3.12, wps=12552.4, ups=3.51, wpb=3580.2, bsz=145, num_updates=55200, lr=0.000134595, gnorm=0.919, train_wall=22, gb_free=7, wall=12679
2021-03-11 00:18:09 | INFO | train_inner | epoch 051:    150 / 1103 loss=3.109, nll_loss=1.572, ppl=2.97, wps=15811.7, ups=4.47, wpb=3535.6, bsz=152.2, num_updates=55300, lr=0.000134474, gnorm=0.898, train_wall=22, gb_free=6.8, wall=12702
2021-03-11 00:18:31 | INFO | train_inner | epoch 051:    250 / 1103 loss=3.118, nll_loss=1.583, ppl=3, wps=15876.3, ups=4.47, wpb=3554.4, bsz=150.6, num_updates=55400, lr=0.000134352, gnorm=0.918, train_wall=22, gb_free=7.1, wall=12724
2021-03-11 00:18:53 | INFO | train_inner | epoch 051:    350 / 1103 loss=3.162, nll_loss=1.634, ppl=3.1, wps=16161.6, ups=4.47, wpb=3617.3, bsz=149, num_updates=55500, lr=0.000134231, gnorm=0.912, train_wall=22, gb_free=6.9, wall=12746
2021-03-11 00:19:16 | INFO | train_inner | epoch 051:    450 / 1103 loss=3.184, nll_loss=1.658, ppl=3.16, wps=16041, ups=4.45, wpb=3604.7, bsz=134.3, num_updates=55600, lr=0.00013411, gnorm=0.934, train_wall=22, gb_free=6.9, wall=12769
2021-03-11 00:19:38 | INFO | train_inner | epoch 051:    550 / 1103 loss=3.155, nll_loss=1.625, ppl=3.08, wps=15712.6, ups=4.45, wpb=3534.5, bsz=142.3, num_updates=55700, lr=0.00013399, gnorm=0.92, train_wall=22, gb_free=7, wall=12791
2021-03-11 00:20:01 | INFO | train_inner | epoch 051:    650 / 1103 loss=3.198, nll_loss=1.676, ppl=3.2, wps=15982.4, ups=4.48, wpb=3566.3, bsz=150.9, num_updates=55800, lr=0.00013387, gnorm=0.945, train_wall=22, gb_free=7, wall=12814
2021-03-11 00:20:23 | INFO | train_inner | epoch 051:    750 / 1103 loss=3.166, nll_loss=1.636, ppl=3.11, wps=15864.8, ups=4.41, wpb=3601.5, bsz=136.4, num_updates=55900, lr=0.00013375, gnorm=0.924, train_wall=23, gb_free=7, wall=12836
2021-03-11 00:20:46 | INFO | train_inner | epoch 051:    850 / 1103 loss=3.194, nll_loss=1.671, ppl=3.19, wps=15962.4, ups=4.46, wpb=3581.7, bsz=143.1, num_updates=56000, lr=0.000133631, gnorm=0.942, train_wall=22, gb_free=6.9, wall=12859
2021-03-11 00:21:08 | INFO | train_inner | epoch 051:    950 / 1103 loss=3.195, nll_loss=1.672, ppl=3.19, wps=15922.4, ups=4.45, wpb=3578.5, bsz=145.8, num_updates=56100, lr=0.000133511, gnorm=0.935, train_wall=22, gb_free=7.1, wall=12881
2021-03-11 00:21:31 | INFO | train_inner | epoch 051:   1050 / 1103 loss=3.164, nll_loss=1.637, ppl=3.11, wps=15838.9, ups=4.41, wpb=3589.1, bsz=147, num_updates=56200, lr=0.000133393, gnorm=0.919, train_wall=23, gb_free=6.8, wall=12904
2021-03-11 00:21:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:21:47 | INFO | valid | epoch 051 | valid on 'valid' subset | loss 3.954 | nll_loss 2.397 | ppl 5.27 | wps 48211.3 | wpb 2873.1 | bsz 115.6 | num_updates 56253 | best_loss 3.91
2021-03-11 00:21:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 51 @ 56253 updates
2021-03-11 00:21:47 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:21:49 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:21:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 51 @ 56253 updates, score 3.954) (writing took 2.3085732460021973 seconds)
2021-03-11 00:21:49 | INFO | fairseq_cli.train | end of epoch 51 (average epoch stats below)
2021-03-11 00:21:49 | INFO | train | epoch 051 | loss 3.164 | nll_loss 1.636 | ppl 3.11 | wps 15528.4 | ups 4.34 | wpb 3577.8 | bsz 145.3 | num_updates 56253 | lr 0.00013333 | gnorm 0.924 | train_wall 247 | gb_free 6.9 | wall 12922
2021-03-11 00:21:49 | INFO | fairseq.trainer | begin training epoch 52
2021-03-11 00:21:49 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:22:00 | INFO | train_inner | epoch 052:     47 / 1103 loss=3.155, nll_loss=1.624, ppl=3.08, wps=12518.6, ups=3.47, wpb=3610.6, bsz=140.6, num_updates=56300, lr=0.000133274, gnorm=0.904, train_wall=22, gb_free=7, wall=12933
2021-03-11 00:22:22 | INFO | train_inner | epoch 052:    147 / 1103 loss=3.119, nll_loss=1.583, ppl=3, wps=15785.7, ups=4.44, wpb=3559, bsz=142.1, num_updates=56400, lr=0.000133156, gnorm=0.931, train_wall=22, gb_free=7.1, wall=12955
2021-03-11 00:22:45 | INFO | train_inner | epoch 052:    247 / 1103 loss=3.125, nll_loss=1.59, ppl=3.01, wps=15862.4, ups=4.47, wpb=3546.4, bsz=141.4, num_updates=56500, lr=0.000133038, gnorm=0.918, train_wall=22, gb_free=6.9, wall=12978
2021-03-11 00:23:07 | INFO | train_inner | epoch 052:    347 / 1103 loss=3.14, nll_loss=1.608, ppl=3.05, wps=16047, ups=4.47, wpb=3593.7, bsz=146.6, num_updates=56600, lr=0.00013292, gnorm=0.917, train_wall=22, gb_free=7, wall=13000
2021-03-11 00:23:29 | INFO | train_inner | epoch 052:    447 / 1103 loss=3.138, nll_loss=1.606, ppl=3.04, wps=15958.2, ups=4.45, wpb=3586.7, bsz=149.4, num_updates=56700, lr=0.000132803, gnorm=0.917, train_wall=22, gb_free=6.9, wall=13023
2021-03-11 00:23:52 | INFO | train_inner | epoch 052:    547 / 1103 loss=3.169, nll_loss=1.639, ppl=3.11, wps=15806, ups=4.49, wpb=3516.5, bsz=124.4, num_updates=56800, lr=0.000132686, gnorm=0.964, train_wall=22, gb_free=7, wall=13045
2021-03-11 00:24:14 | INFO | train_inner | epoch 052:    647 / 1103 loss=3.164, nll_loss=1.638, ppl=3.11, wps=15985.6, ups=4.4, wpb=3634.2, bsz=151.4, num_updates=56900, lr=0.00013257, gnorm=0.914, train_wall=23, gb_free=6.9, wall=13068
2021-03-11 00:24:37 | INFO | train_inner | epoch 052:    747 / 1103 loss=3.211, nll_loss=1.689, ppl=3.22, wps=15948.6, ups=4.47, wpb=3571.5, bsz=134.3, num_updates=57000, lr=0.000132453, gnorm=0.957, train_wall=22, gb_free=6.9, wall=13090
2021-03-11 00:24:59 | INFO | train_inner | epoch 052:    847 / 1103 loss=3.156, nll_loss=1.628, ppl=3.09, wps=15895.4, ups=4.49, wpb=3541.9, bsz=152.2, num_updates=57100, lr=0.000132337, gnorm=0.934, train_wall=22, gb_free=7.2, wall=13112
2021-03-11 00:25:22 | INFO | train_inner | epoch 052:    947 / 1103 loss=3.162, nll_loss=1.635, ppl=3.11, wps=15854.1, ups=4.45, wpb=3562.5, bsz=158.5, num_updates=57200, lr=0.000132221, gnorm=0.935, train_wall=22, gb_free=7.4, wall=13135
2021-03-11 00:25:44 | INFO | train_inner | epoch 052:   1047 / 1103 loss=3.177, nll_loss=1.652, ppl=3.14, wps=15900.8, ups=4.41, wpb=3603.8, bsz=147.5, num_updates=57300, lr=0.000132106, gnorm=0.918, train_wall=23, gb_free=6.9, wall=13157
2021-03-11 00:25:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:26:01 | INFO | valid | epoch 052 | valid on 'valid' subset | loss 3.95 | nll_loss 2.394 | ppl 5.26 | wps 48191.8 | wpb 2873.1 | bsz 115.6 | num_updates 57356 | best_loss 3.91
2021-03-11 00:26:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 52 @ 57356 updates
2021-03-11 00:26:01 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:26:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:26:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 52 @ 57356 updates, score 3.95) (writing took 2.1354785561561584 seconds)
2021-03-11 00:26:03 | INFO | fairseq_cli.train | end of epoch 52 (average epoch stats below)
2021-03-11 00:26:03 | INFO | train | epoch 052 | loss 3.156 | nll_loss 1.626 | ppl 3.09 | wps 15551.7 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 57356 | lr 0.000132042 | gnorm 0.927 | train_wall 246 | gb_free 6.9 | wall 13176
2021-03-11 00:26:03 | INFO | fairseq.trainer | begin training epoch 53
2021-03-11 00:26:03 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:26:13 | INFO | train_inner | epoch 053:     44 / 1103 loss=3.154, nll_loss=1.625, ppl=3.08, wps=12655.9, ups=3.51, wpb=3606.3, bsz=147.2, num_updates=57400, lr=0.000131991, gnorm=0.913, train_wall=22, gb_free=6.9, wall=13186
2021-03-11 00:26:36 | INFO | train_inner | epoch 053:    144 / 1103 loss=3.131, nll_loss=1.597, ppl=3.03, wps=16168.1, ups=4.4, wpb=3676.9, bsz=144.5, num_updates=57500, lr=0.000131876, gnorm=0.9, train_wall=23, gb_free=7, wall=13209
2021-03-11 00:26:58 | INFO | train_inner | epoch 053:    244 / 1103 loss=3.125, nll_loss=1.592, ppl=3.01, wps=16294.9, ups=4.44, wpb=3666.8, bsz=154.4, num_updates=57600, lr=0.000131762, gnorm=0.908, train_wall=22, gb_free=7.3, wall=13231
2021-03-11 00:27:20 | INFO | train_inner | epoch 053:    344 / 1103 loss=3.161, nll_loss=1.631, ppl=3.1, wps=15929.8, ups=4.48, wpb=3559.3, bsz=131.4, num_updates=57700, lr=0.000131647, gnorm=0.944, train_wall=22, gb_free=6.5, wall=13253
2021-03-11 00:27:43 | INFO | train_inner | epoch 053:    444 / 1103 loss=3.131, nll_loss=1.597, ppl=3.03, wps=15655.4, ups=4.46, wpb=3507.5, bsz=146.8, num_updates=57800, lr=0.000131533, gnorm=0.937, train_wall=22, gb_free=7.3, wall=13276
2021-03-11 00:28:05 | INFO | train_inner | epoch 053:    544 / 1103 loss=3.157, nll_loss=1.627, ppl=3.09, wps=15999.7, ups=4.48, wpb=3571.2, bsz=139.8, num_updates=57900, lr=0.00013142, gnorm=0.938, train_wall=22, gb_free=6.9, wall=13298
2021-03-11 00:28:28 | INFO | train_inner | epoch 053:    644 / 1103 loss=3.131, nll_loss=1.597, ppl=3.03, wps=15694.8, ups=4.45, wpb=3525.8, bsz=150, num_updates=58000, lr=0.000131306, gnorm=0.929, train_wall=22, gb_free=7, wall=13321
2021-03-11 00:28:50 | INFO | train_inner | epoch 053:    744 / 1103 loss=3.167, nll_loss=1.64, ppl=3.12, wps=15978.7, ups=4.44, wpb=3597.9, bsz=153, num_updates=58100, lr=0.000131193, gnorm=0.936, train_wall=22, gb_free=6.9, wall=13343
2021-03-11 00:29:12 | INFO | train_inner | epoch 053:    844 / 1103 loss=3.161, nll_loss=1.631, ppl=3.1, wps=16071.7, ups=4.46, wpb=3602.5, bsz=138.2, num_updates=58200, lr=0.000131081, gnorm=0.936, train_wall=22, gb_free=6.9, wall=13366
2021-03-11 00:29:35 | INFO | train_inner | epoch 053:    944 / 1103 loss=3.155, nll_loss=1.625, ppl=3.08, wps=15963.4, ups=4.45, wpb=3590.8, bsz=145.8, num_updates=58300, lr=0.000130968, gnorm=0.933, train_wall=22, gb_free=6.9, wall=13388
2021-03-11 00:29:57 | INFO | train_inner | epoch 053:   1044 / 1103 loss=3.171, nll_loss=1.645, ppl=3.13, wps=15864.7, ups=4.52, wpb=3512.2, bsz=146.2, num_updates=58400, lr=0.000130856, gnorm=0.968, train_wall=22, gb_free=7.2, wall=13410
2021-03-11 00:30:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:30:14 | INFO | valid | epoch 053 | valid on 'valid' subset | loss 3.954 | nll_loss 2.396 | ppl 5.26 | wps 48183.4 | wpb 2873.1 | bsz 115.6 | num_updates 58459 | best_loss 3.91
2021-03-11 00:30:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 53 @ 58459 updates
2021-03-11 00:30:14 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:30:16 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:30:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 53 @ 58459 updates, score 3.954) (writing took 2.014250200241804 seconds)
2021-03-11 00:30:16 | INFO | fairseq_cli.train | end of epoch 53 (average epoch stats below)
2021-03-11 00:30:16 | INFO | train | epoch 053 | loss 3.149 | nll_loss 1.618 | ppl 3.07 | wps 15584.8 | ups 4.36 | wpb 3577.8 | bsz 145.3 | num_updates 58459 | lr 0.00013079 | gnorm 0.933 | train_wall 246 | gb_free 6.8 | wall 13429
2021-03-11 00:30:16 | INFO | fairseq.trainer | begin training epoch 54
2021-03-11 00:30:16 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:30:25 | INFO | train_inner | epoch 054:     41 / 1103 loss=3.152, nll_loss=1.62, ppl=3.07, wps=12459.3, ups=3.55, wpb=3510.7, bsz=134.2, num_updates=58500, lr=0.000130744, gnorm=0.943, train_wall=22, gb_free=7, wall=13438
2021-03-11 00:30:48 | INFO | train_inner | epoch 054:    141 / 1103 loss=3.088, nll_loss=1.546, ppl=2.92, wps=15657.8, ups=4.47, wpb=3502, bsz=140, num_updates=58600, lr=0.000130632, gnorm=0.92, train_wall=22, gb_free=6.9, wall=13461
2021-03-11 00:31:10 | INFO | train_inner | epoch 054:    241 / 1103 loss=3.1, nll_loss=1.564, ppl=2.96, wps=16134.4, ups=4.42, wpb=3650.8, bsz=165, num_updates=58700, lr=0.000130521, gnorm=0.912, train_wall=23, gb_free=6.9, wall=13483
2021-03-11 00:31:33 | INFO | train_inner | epoch 054:    341 / 1103 loss=3.119, nll_loss=1.585, ppl=3, wps=16105.9, ups=4.4, wpb=3661.6, bsz=155.4, num_updates=58800, lr=0.00013041, gnorm=0.923, train_wall=23, gb_free=6.7, wall=13506
2021-03-11 00:31:56 | INFO | train_inner | epoch 054:    441 / 1103 loss=3.117, nll_loss=1.581, ppl=2.99, wps=15806.5, ups=4.45, wpb=3553.9, bsz=150.3, num_updates=58900, lr=0.000130299, gnorm=0.929, train_wall=22, gb_free=7.3, wall=13529
2021-03-11 00:32:18 | INFO | train_inner | epoch 054:    541 / 1103 loss=3.177, nll_loss=1.649, ppl=3.14, wps=15945.2, ups=4.44, wpb=3589.8, bsz=131.8, num_updates=59000, lr=0.000130189, gnorm=0.948, train_wall=22, gb_free=7, wall=13551
2021-03-11 00:32:40 | INFO | train_inner | epoch 054:    641 / 1103 loss=3.139, nll_loss=1.607, ppl=3.05, wps=16045.1, ups=4.45, wpb=3604.9, bsz=151, num_updates=59100, lr=0.000130079, gnorm=0.921, train_wall=22, gb_free=6.9, wall=13574
2021-03-11 00:33:03 | INFO | train_inner | epoch 054:    741 / 1103 loss=3.138, nll_loss=1.606, ppl=3.04, wps=16110.4, ups=4.46, wpb=3612.2, bsz=154.1, num_updates=59200, lr=0.000129969, gnorm=0.927, train_wall=22, gb_free=7.5, wall=13596
2021-03-11 00:33:25 | INFO | train_inner | epoch 054:    841 / 1103 loss=3.187, nll_loss=1.66, ppl=3.16, wps=15921.4, ups=4.5, wpb=3538.6, bsz=126, num_updates=59300, lr=0.000129859, gnorm=0.957, train_wall=22, gb_free=6.4, wall=13618
2021-03-11 00:33:47 | INFO | train_inner | epoch 054:    941 / 1103 loss=3.177, nll_loss=1.65, ppl=3.14, wps=15848.3, ups=4.47, wpb=3544.7, bsz=135.9, num_updates=59400, lr=0.00012975, gnorm=0.96, train_wall=22, gb_free=7.5, wall=13641
2021-03-11 00:34:10 | INFO | train_inner | epoch 054:   1041 / 1103 loss=3.157, nll_loss=1.629, ppl=3.09, wps=16082.7, ups=4.44, wpb=3624.6, bsz=160.6, num_updates=59500, lr=0.000129641, gnorm=0.926, train_wall=22, gb_free=7.1, wall=13663
2021-03-11 00:34:24 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:34:28 | INFO | valid | epoch 054 | valid on 'valid' subset | loss 3.965 | nll_loss 2.405 | ppl 5.3 | wps 48198.3 | wpb 2873.1 | bsz 115.6 | num_updates 59562 | best_loss 3.91
2021-03-11 00:34:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 54 @ 59562 updates
2021-03-11 00:34:28 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:34:30 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:34:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 54 @ 59562 updates, score 3.965) (writing took 2.0704017244279385 seconds)
2021-03-11 00:34:30 | INFO | fairseq_cli.train | end of epoch 54 (average epoch stats below)
2021-03-11 00:34:30 | INFO | train | epoch 054 | loss 3.14 | nll_loss 1.608 | ppl 3.05 | wps 15557.8 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 59562 | lr 0.000129573 | gnorm 0.934 | train_wall 246 | gb_free 7 | wall 13683
2021-03-11 00:34:30 | INFO | fairseq.trainer | begin training epoch 55
2021-03-11 00:34:30 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:34:38 | INFO | train_inner | epoch 055:     38 / 1103 loss=3.127, nll_loss=1.592, ppl=3.02, wps=12386.1, ups=3.53, wpb=3508.9, bsz=146.2, num_updates=59600, lr=0.000129532, gnorm=0.944, train_wall=22, gb_free=7.2, wall=13692
2021-03-11 00:35:01 | INFO | train_inner | epoch 055:    138 / 1103 loss=3.086, nll_loss=1.545, ppl=2.92, wps=16127.6, ups=4.43, wpb=3641.7, bsz=154.6, num_updates=59700, lr=0.000129423, gnorm=0.898, train_wall=22, gb_free=6.8, wall=13714
2021-03-11 00:35:23 | INFO | train_inner | epoch 055:    238 / 1103 loss=3.115, nll_loss=1.577, ppl=2.98, wps=15828.6, ups=4.47, wpb=3544, bsz=129.7, num_updates=59800, lr=0.000129315, gnorm=0.964, train_wall=22, gb_free=7, wall=13736
2021-03-11 00:35:46 | INFO | train_inner | epoch 055:    338 / 1103 loss=3.1, nll_loss=1.56, ppl=2.95, wps=15910.1, ups=4.4, wpb=3618.1, bsz=148.6, num_updates=59900, lr=0.000129207, gnorm=0.911, train_wall=23, gb_free=6.8, wall=13759
2021-03-11 00:36:09 | INFO | train_inner | epoch 055:    438 / 1103 loss=3.121, nll_loss=1.586, ppl=3, wps=15913.4, ups=4.45, wpb=3575.5, bsz=145.7, num_updates=60000, lr=0.000129099, gnorm=0.934, train_wall=22, gb_free=7.1, wall=13782
2021-03-11 00:36:31 | INFO | train_inner | epoch 055:    538 / 1103 loss=3.142, nll_loss=1.606, ppl=3.04, wps=15693.7, ups=4.46, wpb=3522.1, bsz=125.4, num_updates=60100, lr=0.000128992, gnorm=0.958, train_wall=22, gb_free=7.3, wall=13804
2021-03-11 00:36:54 | INFO | train_inner | epoch 055:    638 / 1103 loss=3.138, nll_loss=1.605, ppl=3.04, wps=15980.9, ups=4.42, wpb=3614.5, bsz=137.8, num_updates=60200, lr=0.000128885, gnorm=0.922, train_wall=23, gb_free=6.9, wall=13827
2021-03-11 00:37:16 | INFO | train_inner | epoch 055:    738 / 1103 loss=3.118, nll_loss=1.586, ppl=3, wps=15957.9, ups=4.43, wpb=3599.9, bsz=167.4, num_updates=60300, lr=0.000128778, gnorm=0.944, train_wall=22, gb_free=6.9, wall=13849
2021-03-11 00:37:38 | INFO | train_inner | epoch 055:    838 / 1103 loss=3.19, nll_loss=1.666, ppl=3.17, wps=16223.1, ups=4.5, wpb=3603, bsz=140.8, num_updates=60400, lr=0.000128671, gnorm=0.97, train_wall=22, gb_free=6.8, wall=13872
2021-03-11 00:38:01 | INFO | train_inner | epoch 055:    938 / 1103 loss=3.125, nll_loss=1.593, ppl=3.02, wps=15847.3, ups=4.41, wpb=3590.4, bsz=163.8, num_updates=60500, lr=0.000128565, gnorm=0.925, train_wall=23, gb_free=6.7, wall=13894
2021-03-11 00:38:23 | INFO | train_inner | epoch 055:   1038 / 1103 loss=3.184, nll_loss=1.658, ppl=3.16, wps=15850.7, ups=4.5, wpb=3519.7, bsz=139.5, num_updates=60600, lr=0.000128459, gnorm=0.964, train_wall=22, gb_free=6.9, wall=13916
2021-03-11 00:38:37 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:38:41 | INFO | valid | epoch 055 | valid on 'valid' subset | loss 3.949 | nll_loss 2.396 | ppl 5.26 | wps 48176.4 | wpb 2873.1 | bsz 115.6 | num_updates 60665 | best_loss 3.91
2021-03-11 00:38:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 55 @ 60665 updates
2021-03-11 00:38:41 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:38:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:38:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 55 @ 60665 updates, score 3.949) (writing took 2.04968399181962 seconds)
2021-03-11 00:38:43 | INFO | fairseq_cli.train | end of epoch 55 (average epoch stats below)
2021-03-11 00:38:43 | INFO | train | epoch 055 | loss 3.133 | nll_loss 1.6 | ppl 3.03 | wps 15553.7 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 60665 | lr 0.00012839 | gnorm 0.94 | train_wall 246 | gb_free 7 | wall 13936
2021-03-11 00:38:43 | INFO | fairseq.trainer | begin training epoch 56
2021-03-11 00:38:43 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:38:51 | INFO | train_inner | epoch 056:     35 / 1103 loss=3.157, nll_loss=1.628, ppl=3.09, wps=12678.8, ups=3.55, wpb=3570, bsz=140.2, num_updates=60700, lr=0.000128353, gnorm=0.945, train_wall=22, gb_free=6.9, wall=13945
2021-03-11 00:39:14 | INFO | train_inner | epoch 056:    135 / 1103 loss=3.055, nll_loss=1.51, ppl=2.85, wps=15892.2, ups=4.42, wpb=3592.3, bsz=156.1, num_updates=60800, lr=0.000128247, gnorm=0.906, train_wall=22, gb_free=6.7, wall=13967
2021-03-11 00:39:36 | INFO | train_inner | epoch 056:    235 / 1103 loss=3.111, nll_loss=1.574, ppl=2.98, wps=15933.3, ups=4.49, wpb=3545.4, bsz=145.7, num_updates=60900, lr=0.000128142, gnorm=0.928, train_wall=22, gb_free=7, wall=13989
2021-03-11 00:39:59 | INFO | train_inner | epoch 056:    335 / 1103 loss=3.115, nll_loss=1.578, ppl=2.98, wps=15941.1, ups=4.45, wpb=3578.7, bsz=139, num_updates=61000, lr=0.000128037, gnorm=0.938, train_wall=22, gb_free=7.2, wall=14012
2021-03-11 00:40:21 | INFO | train_inner | epoch 056:    435 / 1103 loss=3.141, nll_loss=1.607, ppl=3.05, wps=15966.5, ups=4.45, wpb=3589.5, bsz=136.2, num_updates=61100, lr=0.000127932, gnorm=0.959, train_wall=22, gb_free=7, wall=14034
2021-03-11 00:40:44 | INFO | train_inner | epoch 056:    535 / 1103 loss=3.135, nll_loss=1.602, ppl=3.04, wps=16009.2, ups=4.45, wpb=3597.4, bsz=143.3, num_updates=61200, lr=0.000127827, gnorm=0.943, train_wall=22, gb_free=6.9, wall=14057
2021-03-11 00:41:06 | INFO | train_inner | epoch 056:    635 / 1103 loss=3.127, nll_loss=1.592, ppl=3.02, wps=15787.3, ups=4.43, wpb=3560.9, bsz=151.8, num_updates=61300, lr=0.000127723, gnorm=0.939, train_wall=22, gb_free=7.1, wall=14079
2021-03-11 00:41:29 | INFO | train_inner | epoch 056:    735 / 1103 loss=3.143, nll_loss=1.61, ppl=3.05, wps=15859.4, ups=4.46, wpb=3554.6, bsz=134.7, num_updates=61400, lr=0.000127619, gnorm=0.971, train_wall=22, gb_free=7.2, wall=14102
2021-03-11 00:41:51 | INFO | train_inner | epoch 056:    835 / 1103 loss=3.163, nll_loss=1.634, ppl=3.1, wps=15974.6, ups=4.49, wpb=3555.1, bsz=140.5, num_updates=61500, lr=0.000127515, gnorm=0.96, train_wall=22, gb_free=6.9, wall=14124
2021-03-11 00:42:13 | INFO | train_inner | epoch 056:    935 / 1103 loss=3.133, nll_loss=1.6, ppl=3.03, wps=15898.1, ups=4.44, wpb=3580, bsz=148.2, num_updates=61600, lr=0.000127412, gnorm=0.954, train_wall=22, gb_free=7.1, wall=14147
2021-03-11 00:42:36 | INFO | train_inner | epoch 056:   1035 / 1103 loss=3.128, nll_loss=1.596, ppl=3.02, wps=15986.2, ups=4.4, wpb=3631.5, bsz=156.2, num_updates=61700, lr=0.000127309, gnorm=0.922, train_wall=23, gb_free=7.2, wall=14169
2021-03-11 00:42:51 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:42:55 | INFO | valid | epoch 056 | valid on 'valid' subset | loss 3.957 | nll_loss 2.403 | ppl 5.29 | wps 48235 | wpb 2873.1 | bsz 115.6 | num_updates 61768 | best_loss 3.91
2021-03-11 00:42:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 56 @ 61768 updates
2021-03-11 00:42:55 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:42:57 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:42:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 56 @ 61768 updates, score 3.957) (writing took 2.15019054338336 seconds)
2021-03-11 00:42:57 | INFO | fairseq_cli.train | end of epoch 56 (average epoch stats below)
2021-03-11 00:42:57 | INFO | train | epoch 056 | loss 3.126 | nll_loss 1.591 | ppl 3.01 | wps 15551.7 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 61768 | lr 0.000127238 | gnorm 0.941 | train_wall 246 | gb_free 6.9 | wall 14190
2021-03-11 00:42:57 | INFO | fairseq.trainer | begin training epoch 57
2021-03-11 00:42:57 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:43:05 | INFO | train_inner | epoch 057:     32 / 1103 loss=3.116, nll_loss=1.581, ppl=2.99, wps=12504.6, ups=3.51, wpb=3558.4, bsz=147.9, num_updates=61800, lr=0.000127205, gnorm=0.936, train_wall=22, gb_free=7.2, wall=14198
2021-03-11 00:43:27 | INFO | train_inner | epoch 057:    132 / 1103 loss=3.08, nll_loss=1.536, ppl=2.9, wps=15890.6, ups=4.45, wpb=3573.8, bsz=143.5, num_updates=61900, lr=0.000127103, gnorm=0.922, train_wall=22, gb_free=7.8, wall=14220
2021-03-11 00:43:49 | INFO | train_inner | epoch 057:    232 / 1103 loss=3.072, nll_loss=1.528, ppl=2.88, wps=15603.6, ups=4.46, wpb=3496.2, bsz=147, num_updates=62000, lr=0.000127, gnorm=0.951, train_wall=22, gb_free=7.1, wall=14243
2021-03-11 00:44:12 | INFO | train_inner | epoch 057:    332 / 1103 loss=3.112, nll_loss=1.575, ppl=2.98, wps=16012, ups=4.48, wpb=3572.9, bsz=140.6, num_updates=62100, lr=0.000126898, gnorm=0.956, train_wall=22, gb_free=6.9, wall=14265
2021-03-11 00:44:34 | INFO | train_inner | epoch 057:    432 / 1103 loss=3.1, nll_loss=1.56, ppl=2.95, wps=15912.9, ups=4.42, wpb=3600.6, bsz=144.6, num_updates=62200, lr=0.000126796, gnorm=0.923, train_wall=23, gb_free=7.4, wall=14288
2021-03-11 00:44:57 | INFO | train_inner | epoch 057:    532 / 1103 loss=3.105, nll_loss=1.568, ppl=2.96, wps=15873.8, ups=4.44, wpb=3575, bsz=150.9, num_updates=62300, lr=0.000126694, gnorm=0.948, train_wall=22, gb_free=7, wall=14310
2021-03-11 00:45:19 | INFO | train_inner | epoch 057:    632 / 1103 loss=3.122, nll_loss=1.587, ppl=3, wps=16040.6, ups=4.46, wpb=3594.4, bsz=152.4, num_updates=62400, lr=0.000126592, gnorm=0.939, train_wall=22, gb_free=7.2, wall=14332
2021-03-11 00:45:42 | INFO | train_inner | epoch 057:    732 / 1103 loss=3.124, nll_loss=1.59, ppl=3.01, wps=15948.8, ups=4.44, wpb=3593.5, bsz=151.7, num_updates=62500, lr=0.000126491, gnorm=0.933, train_wall=22, gb_free=7, wall=14355
2021-03-11 00:46:04 | INFO | train_inner | epoch 057:    832 / 1103 loss=3.157, nll_loss=1.627, ppl=3.09, wps=15973.4, ups=4.49, wpb=3561.5, bsz=135.6, num_updates=62600, lr=0.00012639, gnorm=0.967, train_wall=22, gb_free=7.2, wall=14377
2021-03-11 00:46:27 | INFO | train_inner | epoch 057:    932 / 1103 loss=3.164, nll_loss=1.636, ppl=3.11, wps=16166.4, ups=4.45, wpb=3634.1, bsz=142, num_updates=62700, lr=0.000126289, gnorm=0.949, train_wall=22, gb_free=7.3, wall=14400
2021-03-11 00:46:49 | INFO | train_inner | epoch 057:   1032 / 1103 loss=3.141, nll_loss=1.608, ppl=3.05, wps=15896.1, ups=4.47, wpb=3560, bsz=146.1, num_updates=62800, lr=0.000126189, gnorm=0.946, train_wall=22, gb_free=6.9, wall=14422
2021-03-11 00:47:05 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:47:09 | INFO | valid | epoch 057 | valid on 'valid' subset | loss 3.96 | nll_loss 2.41 | ppl 5.31 | wps 48309.9 | wpb 2873.1 | bsz 115.6 | num_updates 62871 | best_loss 3.91
2021-03-11 00:47:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 57 @ 62871 updates
2021-03-11 00:47:09 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:47:11 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:47:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 57 @ 62871 updates, score 3.96) (writing took 2.063488654792309 seconds)
2021-03-11 00:47:11 | INFO | fairseq_cli.train | end of epoch 57 (average epoch stats below)
2021-03-11 00:47:11 | INFO | train | epoch 057 | loss 3.118 | nll_loss 1.582 | ppl 2.99 | wps 15564.4 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 62871 | lr 0.000126117 | gnorm 0.944 | train_wall 246 | gb_free 7.4 | wall 14444
2021-03-11 00:47:11 | INFO | fairseq.trainer | begin training epoch 58
2021-03-11 00:47:11 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:47:17 | INFO | train_inner | epoch 058:     29 / 1103 loss=3.134, nll_loss=1.6, ppl=3.03, wps=12897, ups=3.61, wpb=3569.8, bsz=141.8, num_updates=62900, lr=0.000126088, gnorm=0.951, train_wall=22, gb_free=7.1, wall=14450
2021-03-11 00:47:37 | INFO | train_inner | epoch 058:    129 / 1103 loss=3.083, nll_loss=1.541, ppl=2.91, wps=17733.2, ups=4.86, wpb=3646.4, bsz=147.1, num_updates=63000, lr=0.000125988, gnorm=0.912, train_wall=20, gb_free=6.9, wall=14470
2021-03-11 00:47:58 | INFO | train_inner | epoch 058:    229 / 1103 loss=3.06, nll_loss=1.517, ppl=2.86, wps=17584.6, ups=4.86, wpb=3615.9, bsz=166, num_updates=63100, lr=0.000125888, gnorm=0.912, train_wall=20, gb_free=7, wall=14491
2021-03-11 00:48:18 | INFO | train_inner | epoch 058:    329 / 1103 loss=3.101, nll_loss=1.561, ppl=2.95, wps=17323.9, ups=4.86, wpb=3561.6, bsz=142.4, num_updates=63200, lr=0.000125789, gnorm=0.954, train_wall=20, gb_free=7.1, wall=14512
2021-03-11 00:48:39 | INFO | train_inner | epoch 058:    429 / 1103 loss=3.127, nll_loss=1.591, ppl=3.01, wps=17794.5, ups=4.94, wpb=3601.1, bsz=135.8, num_updates=63300, lr=0.000125689, gnorm=0.948, train_wall=20, gb_free=6.7, wall=14532
2021-03-11 00:48:59 | INFO | train_inner | epoch 058:    529 / 1103 loss=3.116, nll_loss=1.578, ppl=2.99, wps=17548.5, ups=4.94, wpb=3550, bsz=139.7, num_updates=63400, lr=0.00012559, gnorm=0.95, train_wall=20, gb_free=7.3, wall=14552
2021-03-11 00:49:19 | INFO | train_inner | epoch 058:    629 / 1103 loss=3.12, nll_loss=1.585, ppl=3, wps=17482, ups=4.87, wpb=3587.9, bsz=146.2, num_updates=63500, lr=0.000125491, gnorm=0.949, train_wall=20, gb_free=7.4, wall=14573
2021-03-11 00:49:40 | INFO | train_inner | epoch 058:    729 / 1103 loss=3.125, nll_loss=1.59, ppl=3.01, wps=17418.7, ups=4.87, wpb=3577.9, bsz=143.5, num_updates=63600, lr=0.000125392, gnorm=0.949, train_wall=20, gb_free=7, wall=14593
2021-03-11 00:50:00 | INFO | train_inner | epoch 058:    829 / 1103 loss=3.13, nll_loss=1.596, ppl=3.02, wps=17465.4, ups=4.9, wpb=3563.3, bsz=140.5, num_updates=63700, lr=0.000125294, gnorm=0.963, train_wall=20, gb_free=7, wall=14613
2021-03-11 00:50:21 | INFO | train_inner | epoch 058:    929 / 1103 loss=3.128, nll_loss=1.593, ppl=3.02, wps=17239.5, ups=4.9, wpb=3517.6, bsz=136.4, num_updates=63800, lr=0.000125196, gnorm=0.979, train_wall=20, gb_free=7.1, wall=14634
2021-03-11 00:50:41 | INFO | train_inner | epoch 058:   1029 / 1103 loss=3.148, nll_loss=1.616, ppl=3.07, wps=17483.6, ups=4.88, wpb=3580.4, bsz=142.5, num_updates=63900, lr=0.000125098, gnorm=0.951, train_wall=20, gb_free=7, wall=14654
2021-03-11 00:50:56 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:51:00 | INFO | valid | epoch 058 | valid on 'valid' subset | loss 3.983 | nll_loss 2.423 | ppl 5.36 | wps 48223.2 | wpb 2873.1 | bsz 115.6 | num_updates 63974 | best_loss 3.91
2021-03-11 00:51:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 58 @ 63974 updates
2021-03-11 00:51:00 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:51:03 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:51:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 58 @ 63974 updates, score 3.983) (writing took 2.516845263540745 seconds)
2021-03-11 00:51:03 | INFO | fairseq_cli.train | end of epoch 58 (average epoch stats below)
2021-03-11 00:51:03 | INFO | train | epoch 058 | loss 3.111 | nll_loss 1.574 | ppl 2.98 | wps 17001.6 | ups 4.75 | wpb 3577.8 | bsz 145.3 | num_updates 63974 | lr 0.000125025 | gnorm 0.946 | train_wall 225 | gb_free 7 | wall 14676
2021-03-11 00:51:03 | INFO | fairseq.trainer | begin training epoch 59
2021-03-11 00:51:03 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:51:08 | INFO | train_inner | epoch 059:     26 / 1103 loss=3.08, nll_loss=1.54, ppl=2.91, wps=13052.1, ups=3.72, wpb=3511.9, bsz=157.6, num_updates=64000, lr=0.000125, gnorm=0.945, train_wall=20, gb_free=7.3, wall=14681
2021-03-11 00:51:29 | INFO | train_inner | epoch 059:    126 / 1103 loss=3.085, nll_loss=1.542, ppl=2.91, wps=17588.9, ups=4.87, wpb=3611.8, bsz=140.5, num_updates=64100, lr=0.000124902, gnorm=0.924, train_wall=20, gb_free=7, wall=14702
2021-03-11 00:51:49 | INFO | train_inner | epoch 059:    226 / 1103 loss=3.066, nll_loss=1.521, ppl=2.87, wps=17329, ups=4.88, wpb=3549.8, bsz=147.9, num_updates=64200, lr=0.000124805, gnorm=0.948, train_wall=20, gb_free=6.7, wall=14722
2021-03-11 00:52:09 | INFO | train_inner | epoch 059:    326 / 1103 loss=3.09, nll_loss=1.546, ppl=2.92, wps=17253.8, ups=4.92, wpb=3506.6, bsz=126.4, num_updates=64300, lr=0.000124708, gnorm=0.968, train_wall=20, gb_free=7, wall=14743
2021-03-11 00:52:30 | INFO | train_inner | epoch 059:    426 / 1103 loss=3.082, nll_loss=1.541, ppl=2.91, wps=17484.3, ups=4.86, wpb=3598.1, bsz=153.5, num_updates=64400, lr=0.000124611, gnorm=0.937, train_wall=20, gb_free=7.4, wall=14763
2021-03-11 00:52:50 | INFO | train_inner | epoch 059:    526 / 1103 loss=3.119, nll_loss=1.58, ppl=2.99, wps=17175.7, ups=4.96, wpb=3464.3, bsz=132.8, num_updates=64500, lr=0.000124515, gnorm=0.996, train_wall=20, gb_free=7.2, wall=14783
2021-03-11 00:53:11 | INFO | train_inner | epoch 059:    626 / 1103 loss=3.113, nll_loss=1.575, ppl=2.98, wps=17411.1, ups=4.88, wpb=3568.9, bsz=139.7, num_updates=64600, lr=0.000124418, gnorm=0.944, train_wall=20, gb_free=7.1, wall=14804
2021-03-11 00:53:31 | INFO | train_inner | epoch 059:    726 / 1103 loss=3.13, nll_loss=1.595, ppl=3.02, wps=17658.4, ups=4.94, wpb=3576.4, bsz=145.4, num_updates=64700, lr=0.000124322, gnorm=0.96, train_wall=20, gb_free=7.8, wall=14824
2021-03-11 00:53:52 | INFO | train_inner | epoch 059:    826 / 1103 loss=3.1, nll_loss=1.563, ppl=2.95, wps=17608, ups=4.84, wpb=3636.8, bsz=154.9, num_updates=64800, lr=0.000124226, gnorm=0.932, train_wall=21, gb_free=7, wall=14845
2021-03-11 00:54:12 | INFO | train_inner | epoch 059:    926 / 1103 loss=3.131, nll_loss=1.598, ppl=3.03, wps=17749.1, ups=4.83, wpb=3675.5, bsz=154.6, num_updates=64900, lr=0.00012413, gnorm=0.932, train_wall=21, gb_free=7.2, wall=14865
2021-03-11 00:54:33 | INFO | train_inner | epoch 059:   1026 / 1103 loss=3.118, nll_loss=1.584, ppl=3, wps=17584.1, ups=4.86, wpb=3620.1, bsz=150.3, num_updates=65000, lr=0.000124035, gnorm=0.933, train_wall=20, gb_free=6.8, wall=14886
2021-03-11 00:54:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:54:52 | INFO | valid | epoch 059 | valid on 'valid' subset | loss 3.964 | nll_loss 2.408 | ppl 5.31 | wps 48243.1 | wpb 2873.1 | bsz 115.6 | num_updates 65077 | best_loss 3.91
2021-03-11 00:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 59 @ 65077 updates
2021-03-11 00:54:52 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:54:55 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:54:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 59 @ 65077 updates, score 3.964) (writing took 2.308833599090576 seconds)
2021-03-11 00:54:55 | INFO | fairseq_cli.train | end of epoch 59 (average epoch stats below)
2021-03-11 00:54:55 | INFO | train | epoch 059 | loss 3.104 | nll_loss 1.565 | ppl 2.96 | wps 17007.3 | ups 4.75 | wpb 3577.8 | bsz 145.3 | num_updates 65077 | lr 0.000123961 | gnorm 0.948 | train_wall 225 | gb_free 7.2 | wall 14908
2021-03-11 00:54:55 | INFO | fairseq.trainer | begin training epoch 60
2021-03-11 00:54:55 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:55:00 | INFO | train_inner | epoch 060:     23 / 1103 loss=3.097, nll_loss=1.56, ppl=2.95, wps=13377.8, ups=3.7, wpb=3616.3, bsz=164.2, num_updates=65100, lr=0.000123939, gnorm=0.932, train_wall=21, gb_free=7, wall=14913
2021-03-11 00:55:22 | INFO | train_inner | epoch 060:    123 / 1103 loss=3.023, nll_loss=1.472, ppl=2.77, wps=15687.4, ups=4.48, wpb=3502.6, bsz=158.6, num_updates=65200, lr=0.000123844, gnorm=0.937, train_wall=22, gb_free=7.4, wall=14935
2021-03-11 00:55:45 | INFO | train_inner | epoch 060:    223 / 1103 loss=3.072, nll_loss=1.53, ppl=2.89, wps=16081.3, ups=4.45, wpb=3615.5, bsz=157.3, num_updates=65300, lr=0.000123749, gnorm=0.926, train_wall=22, gb_free=7, wall=14958
2021-03-11 00:56:07 | INFO | train_inner | epoch 060:    323 / 1103 loss=3.097, nll_loss=1.555, ppl=2.94, wps=15795.4, ups=4.48, wpb=3525.3, bsz=124.4, num_updates=65400, lr=0.000123655, gnorm=0.97, train_wall=22, gb_free=7.3, wall=14980
2021-03-11 00:56:29 | INFO | train_inner | epoch 060:    423 / 1103 loss=3.102, nll_loss=1.561, ppl=2.95, wps=16019.6, ups=4.5, wpb=3561.4, bsz=135, num_updates=65500, lr=0.00012356, gnorm=0.957, train_wall=22, gb_free=7, wall=15002
2021-03-11 00:56:52 | INFO | train_inner | epoch 060:    523 / 1103 loss=3.089, nll_loss=1.548, ppl=2.92, wps=16126.4, ups=4.46, wpb=3618, bsz=147.5, num_updates=65600, lr=0.000123466, gnorm=0.94, train_wall=22, gb_free=6.9, wall=15025
2021-03-11 00:57:14 | INFO | train_inner | epoch 060:    623 / 1103 loss=3.085, nll_loss=1.544, ppl=2.92, wps=15813, ups=4.46, wpb=3544.1, bsz=155.4, num_updates=65700, lr=0.000123372, gnorm=0.954, train_wall=22, gb_free=7.6, wall=15047
2021-03-11 00:57:37 | INFO | train_inner | epoch 060:    723 / 1103 loss=3.147, nll_loss=1.615, ppl=3.06, wps=16198.9, ups=4.47, wpb=3622.7, bsz=141.4, num_updates=65800, lr=0.000123278, gnorm=0.965, train_wall=22, gb_free=7.4, wall=15070
2021-03-11 00:57:59 | INFO | train_inner | epoch 060:    823 / 1103 loss=3.125, nll_loss=1.59, ppl=3.01, wps=16184.6, ups=4.45, wpb=3639.2, bsz=136.1, num_updates=65900, lr=0.000123185, gnorm=0.95, train_wall=22, gb_free=7.4, wall=15092
2021-03-11 00:58:22 | INFO | train_inner | epoch 060:    923 / 1103 loss=3.128, nll_loss=1.594, ppl=3.02, wps=16073.7, ups=4.42, wpb=3634.9, bsz=146.2, num_updates=66000, lr=0.000123091, gnorm=0.964, train_wall=23, gb_free=7.4, wall=15115
2021-03-11 00:58:44 | INFO | train_inner | epoch 060:   1023 / 1103 loss=3.128, nll_loss=1.593, ppl=3.02, wps=15850.5, ups=4.47, wpb=3549.6, bsz=141.4, num_updates=66100, lr=0.000122998, gnorm=0.986, train_wall=22, gb_free=6.9, wall=15137
2021-03-11 00:59:02 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 00:59:06 | INFO | valid | epoch 060 | valid on 'valid' subset | loss 3.971 | nll_loss 2.416 | ppl 5.34 | wps 48271.8 | wpb 2873.1 | bsz 115.6 | num_updates 66180 | best_loss 3.91
2021-03-11 00:59:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 60 @ 66180 updates
2021-03-11 00:59:06 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:59:08 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 00:59:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 60 @ 66180 updates, score 3.971) (writing took 2.254071157425642 seconds)
2021-03-11 00:59:08 | INFO | fairseq_cli.train | end of epoch 60 (average epoch stats below)
2021-03-11 00:59:08 | INFO | train | epoch 060 | loss 3.098 | nll_loss 1.558 | ppl 2.95 | wps 15587.8 | ups 4.36 | wpb 3577.8 | bsz 145.3 | num_updates 66180 | lr 0.000122924 | gnorm 0.954 | train_wall 246 | gb_free 7.1 | wall 15161
2021-03-11 00:59:08 | INFO | fairseq.trainer | begin training epoch 61
2021-03-11 00:59:08 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 00:59:12 | INFO | train_inner | epoch 061:     20 / 1103 loss=3.099, nll_loss=1.56, ppl=2.95, wps=12419.9, ups=3.55, wpb=3496.9, bsz=143.3, num_updates=66200, lr=0.000122905, gnorm=0.975, train_wall=22, gb_free=7.6, wall=15165
2021-03-11 00:59:33 | INFO | train_inner | epoch 061:    120 / 1103 loss=3.075, nll_loss=1.529, ppl=2.89, wps=17533.1, ups=4.92, wpb=3567, bsz=128.5, num_updates=66300, lr=0.000122813, gnorm=0.953, train_wall=20, gb_free=6.9, wall=15186
2021-03-11 00:59:53 | INFO | train_inner | epoch 061:    220 / 1103 loss=3.065, nll_loss=1.519, ppl=2.87, wps=17646.9, ups=4.85, wpb=3637.3, bsz=148.2, num_updates=66400, lr=0.00012272, gnorm=0.925, train_wall=21, gb_free=7, wall=15206
2021-03-11 01:00:14 | INFO | train_inner | epoch 061:    320 / 1103 loss=3.072, nll_loss=1.528, ppl=2.88, wps=17455.7, ups=4.87, wpb=3583.7, bsz=145.4, num_updates=66500, lr=0.000122628, gnorm=0.937, train_wall=20, gb_free=6.9, wall=15227
2021-03-11 01:00:34 | INFO | train_inner | epoch 061:    420 / 1103 loss=3.067, nll_loss=1.523, ppl=2.87, wps=17624, ups=4.86, wpb=3625.5, bsz=148.3, num_updates=66600, lr=0.000122536, gnorm=0.948, train_wall=20, gb_free=6.9, wall=15247
2021-03-11 01:00:55 | INFO | train_inner | epoch 061:    520 / 1103 loss=3.073, nll_loss=1.53, ppl=2.89, wps=17475.5, ups=4.89, wpb=3576.6, bsz=158.5, num_updates=66700, lr=0.000122444, gnorm=0.944, train_wall=20, gb_free=7, wall=15268
2021-03-11 01:01:15 | INFO | train_inner | epoch 061:    620 / 1103 loss=3.117, nll_loss=1.58, ppl=2.99, wps=17577, ups=4.9, wpb=3584.1, bsz=138.2, num_updates=66800, lr=0.000122352, gnorm=0.959, train_wall=20, gb_free=7.1, wall=15288
2021-03-11 01:01:35 | INFO | train_inner | epoch 061:    720 / 1103 loss=3.123, nll_loss=1.587, ppl=3, wps=17598.2, ups=4.93, wpb=3570.2, bsz=143.8, num_updates=66900, lr=0.000122261, gnorm=0.967, train_wall=20, gb_free=7, wall=15309
2021-03-11 01:01:56 | INFO | train_inner | epoch 061:    820 / 1103 loss=3.088, nll_loss=1.548, ppl=2.92, wps=17355.4, ups=4.86, wpb=3568.6, bsz=152, num_updates=67000, lr=0.000122169, gnorm=0.945, train_wall=20, gb_free=7, wall=15329
2021-03-11 01:02:17 | INFO | train_inner | epoch 061:    920 / 1103 loss=3.099, nll_loss=1.56, ppl=2.95, wps=17461.2, ups=4.85, wpb=3598.5, bsz=147.4, num_updates=67100, lr=0.000122078, gnorm=0.953, train_wall=21, gb_free=7, wall=15350
2021-03-11 01:02:37 | INFO | train_inner | epoch 061:   1020 / 1103 loss=3.109, nll_loss=1.571, ppl=2.97, wps=17288.6, ups=4.9, wpb=3527.8, bsz=142.1, num_updates=67200, lr=0.000121988, gnorm=0.979, train_wall=20, gb_free=6.8, wall=15370
2021-03-11 01:02:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:02:59 | INFO | valid | epoch 061 | valid on 'valid' subset | loss 3.984 | nll_loss 2.423 | ppl 5.36 | wps 48193 | wpb 2873.1 | bsz 115.6 | num_updates 67283 | best_loss 3.91
2021-03-11 01:02:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 61 @ 67283 updates
2021-03-11 01:02:59 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:03:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:03:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 61 @ 67283 updates, score 3.984) (writing took 2.185986455529928 seconds)
2021-03-11 01:03:01 | INFO | fairseq_cli.train | end of epoch 61 (average epoch stats below)
2021-03-11 01:03:01 | INFO | train | epoch 061 | loss 3.09 | nll_loss 1.55 | ppl 2.93 | wps 16906 | ups 4.73 | wpb 3577.8 | bsz 145.3 | num_updates 67283 | lr 0.000121912 | gnorm 0.954 | train_wall 226 | gb_free 7 | wall 15395
2021-03-11 01:03:01 | INFO | fairseq.trainer | begin training epoch 62
2021-03-11 01:03:01 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:03:05 | INFO | train_inner | epoch 062:     17 / 1103 loss=3.104, nll_loss=1.566, ppl=2.96, wps=12452.7, ups=3.53, wpb=3532.4, bsz=148.2, num_updates=67300, lr=0.000121897, gnorm=0.969, train_wall=22, gb_free=6.8, wall=15398
2021-03-11 01:03:26 | INFO | train_inner | epoch 062:    117 / 1103 loss=3.034, nll_loss=1.485, ppl=2.8, wps=17211.9, ups=4.92, wpb=3495.8, bsz=156.6, num_updates=67400, lr=0.000121806, gnorm=0.958, train_wall=20, gb_free=7.4, wall=15419
2021-03-11 01:03:46 | INFO | train_inner | epoch 062:    217 / 1103 loss=3.052, nll_loss=1.505, ppl=2.84, wps=17209.9, ups=4.86, wpb=3542.9, bsz=151.8, num_updates=67500, lr=0.000121716, gnorm=0.942, train_wall=20, gb_free=6.9, wall=15439
2021-03-11 01:04:09 | INFO | train_inner | epoch 062:    317 / 1103 loss=3.062, nll_loss=1.519, ppl=2.87, wps=16428.4, ups=4.44, wpb=3702.7, bsz=166.6, num_updates=67600, lr=0.000121626, gnorm=0.908, train_wall=22, gb_free=6.8, wall=15462
2021-03-11 01:04:31 | INFO | train_inner | epoch 062:    417 / 1103 loss=3.093, nll_loss=1.551, ppl=2.93, wps=15910.2, ups=4.45, wpb=3571.5, bsz=131, num_updates=67700, lr=0.000121536, gnorm=0.969, train_wall=22, gb_free=6.8, wall=15484
2021-03-11 01:04:54 | INFO | train_inner | epoch 062:    517 / 1103 loss=3.082, nll_loss=1.539, ppl=2.91, wps=16105.4, ups=4.41, wpb=3651.1, bsz=145, num_updates=67800, lr=0.000121447, gnorm=0.941, train_wall=23, gb_free=7.1, wall=15507
2021-03-11 01:05:15 | INFO | train_inner | epoch 062:    617 / 1103 loss=3.126, nll_loss=1.59, ppl=3.01, wps=16563.8, ups=4.64, wpb=3573.2, bsz=131, num_updates=67900, lr=0.000121357, gnorm=0.981, train_wall=21, gb_free=6.9, wall=15529
2021-03-11 01:05:36 | INFO | train_inner | epoch 062:    717 / 1103 loss=3.105, nll_loss=1.566, ppl=2.96, wps=17392.7, ups=4.95, wpb=3516.9, bsz=135.8, num_updates=68000, lr=0.000121268, gnorm=0.98, train_wall=20, gb_free=7.1, wall=15549
2021-03-11 01:05:56 | INFO | train_inner | epoch 062:    817 / 1103 loss=3.092, nll_loss=1.551, ppl=2.93, wps=17323.5, ups=4.89, wpb=3543.6, bsz=145, num_updates=68100, lr=0.000121179, gnorm=0.975, train_wall=20, gb_free=7.1, wall=15569
2021-03-11 01:06:18 | INFO | train_inner | epoch 062:    917 / 1103 loss=3.108, nll_loss=1.567, ppl=2.96, wps=16017.5, ups=4.47, wpb=3581.1, bsz=132.4, num_updates=68200, lr=0.00012109, gnorm=0.964, train_wall=22, gb_free=6.9, wall=15592
2021-03-11 01:06:41 | INFO | train_inner | epoch 062:   1017 / 1103 loss=3.085, nll_loss=1.546, ppl=2.92, wps=16056.8, ups=4.41, wpb=3642.6, bsz=161.7, num_updates=68300, lr=0.000121001, gnorm=0.935, train_wall=23, gb_free=7, wall=15614
2021-03-11 01:07:00 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:07:04 | INFO | valid | epoch 062 | valid on 'valid' subset | loss 3.977 | nll_loss 2.42 | ppl 5.35 | wps 48211.7 | wpb 2873.1 | bsz 115.6 | num_updates 68386 | best_loss 3.91
2021-03-11 01:07:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 62 @ 68386 updates
2021-03-11 01:07:04 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:07:06 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:07:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 62 @ 68386 updates, score 3.977) (writing took 2.163457367569208 seconds)
2021-03-11 01:07:06 | INFO | fairseq_cli.train | end of epoch 62 (average epoch stats below)
2021-03-11 01:07:06 | INFO | train | epoch 062 | loss 3.085 | nll_loss 1.543 | ppl 2.91 | wps 16110.2 | ups 4.5 | wpb 3577.8 | bsz 145.3 | num_updates 68386 | lr 0.000120925 | gnorm 0.957 | train_wall 238 | gb_free 7 | wall 15639
2021-03-11 01:07:06 | INFO | fairseq.trainer | begin training epoch 63
2021-03-11 01:07:06 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:07:10 | INFO | train_inner | epoch 063:     14 / 1103 loss=3.089, nll_loss=1.547, ppl=2.92, wps=12423.6, ups=3.5, wpb=3544.8, bsz=138.6, num_updates=68400, lr=0.000120913, gnorm=0.968, train_wall=22, gb_free=7.1, wall=15643
2021-03-11 01:07:32 | INFO | train_inner | epoch 063:    114 / 1103 loss=3.059, nll_loss=1.512, ppl=2.85, wps=15965, ups=4.5, wpb=3551.4, bsz=137.2, num_updates=68500, lr=0.000120824, gnorm=0.954, train_wall=22, gb_free=7.2, wall=15665
2021-03-11 01:07:54 | INFO | train_inner | epoch 063:    214 / 1103 loss=3.058, nll_loss=1.511, ppl=2.85, wps=16006.2, ups=4.47, wpb=3578.8, bsz=145.4, num_updates=68600, lr=0.000120736, gnorm=0.96, train_wall=22, gb_free=6.9, wall=15687
2021-03-11 01:08:17 | INFO | train_inner | epoch 063:    314 / 1103 loss=3.054, nll_loss=1.507, ppl=2.84, wps=15780.8, ups=4.47, wpb=3527.5, bsz=145.3, num_updates=68700, lr=0.000120648, gnorm=0.961, train_wall=22, gb_free=6.9, wall=15710
2021-03-11 01:08:39 | INFO | train_inner | epoch 063:    414 / 1103 loss=3.055, nll_loss=1.51, ppl=2.85, wps=15924.9, ups=4.43, wpb=3597.7, bsz=154.9, num_updates=68800, lr=0.000120561, gnorm=0.971, train_wall=22, gb_free=6.9, wall=15732
2021-03-11 01:09:02 | INFO | train_inner | epoch 063:    514 / 1103 loss=3.085, nll_loss=1.542, ppl=2.91, wps=15955.7, ups=4.46, wpb=3577.9, bsz=131.8, num_updates=68900, lr=0.000120473, gnorm=0.974, train_wall=22, gb_free=7.2, wall=15755
2021-03-11 01:09:24 | INFO | train_inner | epoch 063:    614 / 1103 loss=3.108, nll_loss=1.57, ppl=2.97, wps=16105.1, ups=4.46, wpb=3613.4, bsz=146.7, num_updates=69000, lr=0.000120386, gnorm=0.954, train_wall=22, gb_free=7, wall=15777
2021-03-11 01:09:46 | INFO | train_inner | epoch 063:    714 / 1103 loss=3.095, nll_loss=1.554, ppl=2.94, wps=15892.9, ups=4.49, wpb=3538.3, bsz=143.1, num_updates=69100, lr=0.000120299, gnorm=0.973, train_wall=22, gb_free=7, wall=15800
2021-03-11 01:10:09 | INFO | train_inner | epoch 063:    814 / 1103 loss=3.07, nll_loss=1.528, ppl=2.88, wps=15941.3, ups=4.43, wpb=3601.8, bsz=158.9, num_updates=69200, lr=0.000120212, gnorm=0.961, train_wall=22, gb_free=7, wall=15822
2021-03-11 01:10:32 | INFO | train_inner | epoch 063:    914 / 1103 loss=3.085, nll_loss=1.544, ppl=2.92, wps=15996.9, ups=4.4, wpb=3639.1, bsz=153.6, num_updates=69300, lr=0.000120125, gnorm=0.936, train_wall=23, gb_free=6.7, wall=15845
2021-03-11 01:10:54 | INFO | train_inner | epoch 063:   1014 / 1103 loss=3.093, nll_loss=1.554, ppl=2.94, wps=16084, ups=4.43, wpb=3628.3, bsz=156.6, num_updates=69400, lr=0.000120038, gnorm=0.952, train_wall=22, gb_free=7.4, wall=15867
2021-03-11 01:11:14 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:11:18 | INFO | valid | epoch 063 | valid on 'valid' subset | loss 3.979 | nll_loss 2.424 | ppl 5.37 | wps 48285.1 | wpb 2873.1 | bsz 115.6 | num_updates 69489 | best_loss 3.91
2021-03-11 01:11:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 63 @ 69489 updates
2021-03-11 01:11:18 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:11:20 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:11:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 63 @ 69489 updates, score 3.979) (writing took 2.149413112550974 seconds)
2021-03-11 01:11:20 | INFO | fairseq_cli.train | end of epoch 63 (average epoch stats below)
2021-03-11 01:11:20 | INFO | train | epoch 063 | loss 3.078 | nll_loss 1.535 | ppl 2.9 | wps 15552.7 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 69489 | lr 0.000119962 | gnorm 0.962 | train_wall 246 | gb_free 7.1 | wall 15893
2021-03-11 01:11:20 | INFO | fairseq.trainer | begin training epoch 64
2021-03-11 01:11:20 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:11:23 | INFO | train_inner | epoch 064:     11 / 1103 loss=3.083, nll_loss=1.54, ppl=2.91, wps=12230.7, ups=3.51, wpb=3484.1, bsz=130.7, num_updates=69500, lr=0.000119952, gnorm=0.986, train_wall=22, gb_free=7, wall=15896
2021-03-11 01:11:45 | INFO | train_inner | epoch 064:    111 / 1103 loss=3.061, nll_loss=1.512, ppl=2.85, wps=16043.2, ups=4.46, wpb=3595.4, bsz=130, num_updates=69600, lr=0.000119866, gnorm=0.958, train_wall=22, gb_free=6.9, wall=15918
2021-03-11 01:12:07 | INFO | train_inner | epoch 064:    211 / 1103 loss=3.046, nll_loss=1.498, ppl=2.82, wps=16036.8, ups=4.48, wpb=3579.9, bsz=154.2, num_updates=69700, lr=0.00011978, gnorm=0.967, train_wall=22, gb_free=7.1, wall=15941
2021-03-11 01:12:30 | INFO | train_inner | epoch 064:    311 / 1103 loss=3.038, nll_loss=1.488, ppl=2.81, wps=15878.2, ups=4.47, wpb=3554.9, bsz=147, num_updates=69800, lr=0.000119694, gnorm=0.964, train_wall=22, gb_free=7, wall=15963
2021-03-11 01:12:52 | INFO | train_inner | epoch 064:    411 / 1103 loss=3.062, nll_loss=1.517, ppl=2.86, wps=15986.9, ups=4.47, wpb=3580.5, bsz=150.2, num_updates=69900, lr=0.000119608, gnorm=0.957, train_wall=22, gb_free=6.9, wall=15985
2021-03-11 01:13:15 | INFO | train_inner | epoch 064:    511 / 1103 loss=3.049, nll_loss=1.504, ppl=2.84, wps=16028.6, ups=4.45, wpb=3602.1, bsz=157.1, num_updates=70000, lr=0.000119523, gnorm=0.934, train_wall=22, gb_free=6.9, wall=16008
2021-03-11 01:13:37 | INFO | train_inner | epoch 064:    611 / 1103 loss=3.072, nll_loss=1.526, ppl=2.88, wps=15814.6, ups=4.47, wpb=3540, bsz=143, num_updates=70100, lr=0.000119438, gnorm=0.981, train_wall=22, gb_free=7, wall=16030
2021-03-11 01:13:59 | INFO | train_inner | epoch 064:    711 / 1103 loss=3.102, nll_loss=1.561, ppl=2.95, wps=15953.1, ups=4.48, wpb=3563.7, bsz=133.8, num_updates=70200, lr=0.000119352, gnorm=0.986, train_wall=22, gb_free=6.9, wall=16053
2021-03-11 01:14:22 | INFO | train_inner | epoch 064:    811 / 1103 loss=3.087, nll_loss=1.546, ppl=2.92, wps=15964.9, ups=4.46, wpb=3583.2, bsz=139.6, num_updates=70300, lr=0.000119268, gnorm=0.955, train_wall=22, gb_free=7.1, wall=16075
2021-03-11 01:14:45 | INFO | train_inner | epoch 064:    911 / 1103 loss=3.1, nll_loss=1.561, ppl=2.95, wps=16010.4, ups=4.42, wpb=3624.2, bsz=142.3, num_updates=70400, lr=0.000119183, gnorm=0.972, train_wall=23, gb_free=7.5, wall=16098
2021-03-11 01:15:07 | INFO | train_inner | epoch 064:   1011 / 1103 loss=3.104, nll_loss=1.564, ppl=2.96, wps=15939.5, ups=4.48, wpb=3558.5, bsz=142.9, num_updates=70500, lr=0.000119098, gnorm=0.966, train_wall=22, gb_free=7.7, wall=16120
2021-03-11 01:15:27 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:15:31 | INFO | valid | epoch 064 | valid on 'valid' subset | loss 3.983 | nll_loss 2.425 | ppl 5.37 | wps 48181.5 | wpb 2873.1 | bsz 115.6 | num_updates 70592 | best_loss 3.91
2021-03-11 01:15:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 64 @ 70592 updates
2021-03-11 01:15:31 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:15:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:15:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 64 @ 70592 updates, score 3.983) (writing took 2.3449752256274223 seconds)
2021-03-11 01:15:34 | INFO | fairseq_cli.train | end of epoch 64 (average epoch stats below)
2021-03-11 01:15:34 | INFO | train | epoch 064 | loss 3.072 | nll_loss 1.527 | ppl 2.88 | wps 15564.1 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 70592 | lr 0.000119021 | gnorm 0.964 | train_wall 246 | gb_free 7.1 | wall 16147
2021-03-11 01:15:34 | INFO | fairseq.trainer | begin training epoch 65
2021-03-11 01:15:34 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:15:36 | INFO | train_inner | epoch 065:      8 / 1103 loss=3.078, nll_loss=1.537, ppl=2.9, wps=12459.1, ups=3.47, wpb=3589.8, bsz=152.4, num_updates=70600, lr=0.000119014, gnorm=0.963, train_wall=22, gb_free=7.1, wall=16149
2021-03-11 01:15:58 | INFO | train_inner | epoch 065:    108 / 1103 loss=3.035, nll_loss=1.482, ppl=2.79, wps=15773.5, ups=4.47, wpb=3526.4, bsz=129.7, num_updates=70700, lr=0.00011893, gnorm=0.968, train_wall=22, gb_free=6.9, wall=16171
2021-03-11 01:16:21 | INFO | train_inner | epoch 065:    208 / 1103 loss=3.045, nll_loss=1.496, ppl=2.82, wps=15925.1, ups=4.43, wpb=3598.4, bsz=146.8, num_updates=70800, lr=0.000118846, gnorm=0.966, train_wall=22, gb_free=6.8, wall=16194
2021-03-11 01:16:43 | INFO | train_inner | epoch 065:    308 / 1103 loss=3.055, nll_loss=1.507, ppl=2.84, wps=15971, ups=4.41, wpb=3617.7, bsz=139.2, num_updates=70900, lr=0.000118762, gnorm=0.959, train_wall=23, gb_free=6.9, wall=16216
2021-03-11 01:17:06 | INFO | train_inner | epoch 065:    408 / 1103 loss=3.047, nll_loss=1.496, ppl=2.82, wps=15761.5, ups=4.45, wpb=3545.5, bsz=135.7, num_updates=71000, lr=0.000118678, gnorm=0.962, train_wall=22, gb_free=6.8, wall=16239
2021-03-11 01:17:28 | INFO | train_inner | epoch 065:    508 / 1103 loss=3.051, nll_loss=1.505, ppl=2.84, wps=15779.4, ups=4.49, wpb=3515.3, bsz=151, num_updates=71100, lr=0.000118595, gnorm=0.964, train_wall=22, gb_free=6.8, wall=16261
2021-03-11 01:17:50 | INFO | train_inner | epoch 065:    608 / 1103 loss=3.087, nll_loss=1.546, ppl=2.92, wps=16290.7, ups=4.5, wpb=3619.7, bsz=148.1, num_updates=71200, lr=0.000118511, gnorm=0.96, train_wall=22, gb_free=6.9, wall=16283
2021-03-11 01:18:13 | INFO | train_inner | epoch 065:    708 / 1103 loss=3.066, nll_loss=1.521, ppl=2.87, wps=15930.8, ups=4.48, wpb=3555.9, bsz=151.8, num_updates=71300, lr=0.000118428, gnorm=0.966, train_wall=22, gb_free=7, wall=16306
2021-03-11 01:18:35 | INFO | train_inner | epoch 065:    808 / 1103 loss=3.085, nll_loss=1.542, ppl=2.91, wps=15792.3, ups=4.46, wpb=3542.4, bsz=136.2, num_updates=71400, lr=0.000118345, gnorm=0.99, train_wall=22, gb_free=7.5, wall=16328
2021-03-11 01:18:58 | INFO | train_inner | epoch 065:    908 / 1103 loss=3.068, nll_loss=1.526, ppl=2.88, wps=16142.5, ups=4.42, wpb=3655.8, bsz=158.9, num_updates=71500, lr=0.000118262, gnorm=0.947, train_wall=23, gb_free=7.2, wall=16351
2021-03-11 01:19:20 | INFO | train_inner | epoch 065:   1008 / 1103 loss=3.105, nll_loss=1.568, ppl=2.96, wps=15899.3, ups=4.47, wpb=3555.4, bsz=155, num_updates=71600, lr=0.00011818, gnorm=0.984, train_wall=22, gb_free=7.1, wall=16373
2021-03-11 01:19:41 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:19:45 | INFO | valid | epoch 065 | valid on 'valid' subset | loss 3.972 | nll_loss 2.419 | ppl 5.35 | wps 48213.4 | wpb 2873.1 | bsz 115.6 | num_updates 71695 | best_loss 3.91
2021-03-11 01:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 65 @ 71695 updates
2021-03-11 01:19:45 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:19:47 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:19:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 65 @ 71695 updates, score 3.972) (writing took 2.2210671454668045 seconds)
2021-03-11 01:19:47 | INFO | fairseq_cli.train | end of epoch 65 (average epoch stats below)
2021-03-11 01:19:47 | INFO | train | epoch 065 | loss 3.066 | nll_loss 1.521 | ppl 2.87 | wps 15549.3 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 71695 | lr 0.000118102 | gnorm 0.965 | train_wall 246 | gb_free 7.5 | wall 16401
2021-03-11 01:19:47 | INFO | fairseq.trainer | begin training epoch 66
2021-03-11 01:19:47 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:19:49 | INFO | train_inner | epoch 066:      5 / 1103 loss=3.09, nll_loss=1.549, ppl=2.93, wps=12629.6, ups=3.49, wpb=3617.5, bsz=141.8, num_updates=71700, lr=0.000118097, gnorm=0.951, train_wall=22, gb_free=6.8, wall=16402
2021-03-11 01:20:11 | INFO | train_inner | epoch 066:    105 / 1103 loss=2.989, nll_loss=1.436, ppl=2.7, wps=16063.5, ups=4.44, wpb=3616.4, bsz=175.1, num_updates=71800, lr=0.000118015, gnorm=0.934, train_wall=22, gb_free=7.1, wall=16424
2021-03-11 01:20:34 | INFO | train_inner | epoch 066:    205 / 1103 loss=3.015, nll_loss=1.461, ppl=2.75, wps=15742.2, ups=4.44, wpb=3549.1, bsz=140.9, num_updates=71900, lr=0.000117933, gnorm=0.957, train_wall=22, gb_free=7, wall=16447
2021-03-11 01:20:56 | INFO | train_inner | epoch 066:    305 / 1103 loss=3.027, nll_loss=1.477, ppl=2.78, wps=15953.3, ups=4.44, wpb=3594.9, bsz=153.7, num_updates=72000, lr=0.000117851, gnorm=0.957, train_wall=22, gb_free=6.9, wall=16469
2021-03-11 01:21:19 | INFO | train_inner | epoch 066:    405 / 1103 loss=3.066, nll_loss=1.518, ppl=2.86, wps=15705, ups=4.44, wpb=3541, bsz=130.2, num_updates=72100, lr=0.000117769, gnorm=0.977, train_wall=22, gb_free=7.1, wall=16492
2021-03-11 01:21:41 | INFO | train_inner | epoch 066:    505 / 1103 loss=3.066, nll_loss=1.519, ppl=2.87, wps=15938.3, ups=4.5, wpb=3544.7, bsz=134.4, num_updates=72200, lr=0.000117688, gnorm=0.981, train_wall=22, gb_free=7, wall=16514
2021-03-11 01:22:03 | INFO | train_inner | epoch 066:    605 / 1103 loss=3.094, nll_loss=1.551, ppl=2.93, wps=16108.9, ups=4.48, wpb=3599.1, bsz=127.5, num_updates=72300, lr=0.000117606, gnorm=0.985, train_wall=22, gb_free=6.9, wall=16537
2021-03-11 01:22:26 | INFO | train_inner | epoch 066:    705 / 1103 loss=3.067, nll_loss=1.522, ppl=2.87, wps=16004.6, ups=4.44, wpb=3604.8, bsz=148.2, num_updates=72400, lr=0.000117525, gnorm=0.977, train_wall=22, gb_free=7, wall=16559
2021-03-11 01:22:48 | INFO | train_inner | epoch 066:    805 / 1103 loss=3.077, nll_loss=1.534, ppl=2.9, wps=16118.3, ups=4.45, wpb=3623.7, bsz=149, num_updates=72500, lr=0.000117444, gnorm=0.954, train_wall=22, gb_free=7, wall=16582
2021-03-11 01:23:11 | INFO | train_inner | epoch 066:    905 / 1103 loss=3.078, nll_loss=1.535, ppl=2.9, wps=15931.6, ups=4.45, wpb=3576.7, bsz=144.9, num_updates=72600, lr=0.000117363, gnorm=0.975, train_wall=22, gb_free=7, wall=16604
2021-03-11 01:23:33 | INFO | train_inner | epoch 066:   1005 / 1103 loss=3.074, nll_loss=1.531, ppl=2.89, wps=15855.7, ups=4.48, wpb=3537, bsz=148, num_updates=72700, lr=0.000117282, gnorm=0.964, train_wall=22, gb_free=6.9, wall=16626
2021-03-11 01:23:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:23:59 | INFO | valid | epoch 066 | valid on 'valid' subset | loss 3.986 | nll_loss 2.43 | ppl 5.39 | wps 48154.4 | wpb 2873.1 | bsz 115.6 | num_updates 72798 | best_loss 3.91
2021-03-11 01:23:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 66 @ 72798 updates
2021-03-11 01:23:59 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:24:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:24:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 66 @ 72798 updates, score 3.986) (writing took 2.222533155232668 seconds)
2021-03-11 01:24:01 | INFO | fairseq_cli.train | end of epoch 66 (average epoch stats below)
2021-03-11 01:24:01 | INFO | train | epoch 066 | loss 3.059 | nll_loss 1.513 | ppl 2.85 | wps 15558.3 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 72798 | lr 0.000117203 | gnorm 0.967 | train_wall 246 | gb_free 7 | wall 16654
2021-03-11 01:24:01 | INFO | fairseq.trainer | begin training epoch 67
2021-03-11 01:24:01 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:24:02 | INFO | train_inner | epoch 067:      2 / 1103 loss=3.088, nll_loss=1.548, ppl=2.92, wps=12521, ups=3.51, wpb=3568, bsz=148, num_updates=72800, lr=0.000117202, gnorm=0.974, train_wall=22, gb_free=7, wall=16655
2021-03-11 01:24:24 | INFO | train_inner | epoch 067:    102 / 1103 loss=2.992, nll_loss=1.436, ppl=2.71, wps=15940, ups=4.47, wpb=3566.3, bsz=157, num_updates=72900, lr=0.000117121, gnorm=0.945, train_wall=22, gb_free=6.9, wall=16677
2021-03-11 01:24:46 | INFO | train_inner | epoch 067:    202 / 1103 loss=3.058, nll_loss=1.51, ppl=2.85, wps=16198.2, ups=4.46, wpb=3628.5, bsz=139.5, num_updates=73000, lr=0.000117041, gnorm=0.953, train_wall=22, gb_free=6.9, wall=16700
2021-03-11 01:25:09 | INFO | train_inner | epoch 067:    302 / 1103 loss=3.028, nll_loss=1.476, ppl=2.78, wps=15854.9, ups=4.44, wpb=3567.7, bsz=147.2, num_updates=73100, lr=0.000116961, gnorm=0.962, train_wall=22, gb_free=7.1, wall=16722
2021-03-11 01:25:31 | INFO | train_inner | epoch 067:    402 / 1103 loss=3.01, nll_loss=1.457, ppl=2.74, wps=15809.2, ups=4.44, wpb=3563.1, bsz=149.7, num_updates=73200, lr=0.000116881, gnorm=0.958, train_wall=22, gb_free=7.2, wall=16745
2021-03-11 01:25:54 | INFO | train_inner | epoch 067:    502 / 1103 loss=3.053, nll_loss=1.505, ppl=2.84, wps=15964.3, ups=4.42, wpb=3613.2, bsz=141, num_updates=73300, lr=0.000116801, gnorm=0.968, train_wall=23, gb_free=7, wall=16767
2021-03-11 01:26:16 | INFO | train_inner | epoch 067:    602 / 1103 loss=3.056, nll_loss=1.509, ppl=2.85, wps=15764.3, ups=4.49, wpb=3514.4, bsz=147.3, num_updates=73400, lr=0.000116722, gnorm=0.988, train_wall=22, gb_free=7.8, wall=16790
2021-03-11 01:26:39 | INFO | train_inner | epoch 067:    702 / 1103 loss=3.07, nll_loss=1.525, ppl=2.88, wps=15897.2, ups=4.46, wpb=3564.9, bsz=142.1, num_updates=73500, lr=0.000116642, gnorm=0.972, train_wall=22, gb_free=7.1, wall=16812
2021-03-11 01:27:01 | INFO | train_inner | epoch 067:    802 / 1103 loss=3.06, nll_loss=1.517, ppl=2.86, wps=16037.6, ups=4.44, wpb=3611.1, bsz=159.3, num_updates=73600, lr=0.000116563, gnorm=0.972, train_wall=22, gb_free=7, wall=16835
2021-03-11 01:27:24 | INFO | train_inner | epoch 067:    902 / 1103 loss=3.078, nll_loss=1.534, ppl=2.9, wps=15736.6, ups=4.49, wpb=3501, bsz=136, num_updates=73700, lr=0.000116484, gnorm=1, train_wall=22, gb_free=7, wall=16857
2021-03-11 01:27:46 | INFO | train_inner | epoch 067:   1002 / 1103 loss=3.097, nll_loss=1.556, ppl=2.94, wps=16063.5, ups=4.44, wpb=3614.1, bsz=140.4, num_updates=73800, lr=0.000116405, gnorm=0.968, train_wall=22, gb_free=7.2, wall=16879
2021-03-11 01:28:09 | INFO | train_inner | epoch 067:   1102 / 1103 loss=3.087, nll_loss=1.545, ppl=2.92, wps=16091.9, ups=4.47, wpb=3603.5, bsz=138, num_updates=73900, lr=0.000116326, gnorm=0.964, train_wall=22, gb_free=6.8, wall=16902
2021-03-11 01:28:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:28:13 | INFO | valid | epoch 067 | valid on 'valid' subset | loss 3.992 | nll_loss 2.436 | ppl 5.41 | wps 48179.7 | wpb 2873.1 | bsz 115.6 | num_updates 73901 | best_loss 3.91
2021-03-11 01:28:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 67 @ 73901 updates
2021-03-11 01:28:13 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:28:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:28:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 67 @ 73901 updates, score 3.992) (writing took 2.182992424815893 seconds)
2021-03-11 01:28:15 | INFO | fairseq_cli.train | end of epoch 67 (average epoch stats below)
2021-03-11 01:28:15 | INFO | train | epoch 067 | loss 3.053 | nll_loss 1.506 | ppl 2.84 | wps 15555.6 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 73901 | lr 0.000116325 | gnorm 0.968 | train_wall 246 | gb_free 6.8 | wall 16908
2021-03-11 01:28:15 | INFO | fairseq.trainer | begin training epoch 68
2021-03-11 01:28:15 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:28:37 | INFO | train_inner | epoch 068:     99 / 1103 loss=3.003, nll_loss=1.447, ppl=2.73, wps=12445.4, ups=3.5, wpb=3555.8, bsz=148.8, num_updates=74000, lr=0.000116248, gnorm=0.963, train_wall=22, gb_free=6.9, wall=16930
2021-03-11 01:29:00 | INFO | train_inner | epoch 068:    199 / 1103 loss=3.021, nll_loss=1.469, ppl=2.77, wps=16085.2, ups=4.44, wpb=3626.3, bsz=155.6, num_updates=74100, lr=0.000116169, gnorm=0.947, train_wall=22, gb_free=7.1, wall=16953
2021-03-11 01:29:22 | INFO | train_inner | epoch 068:    299 / 1103 loss=3.05, nll_loss=1.501, ppl=2.83, wps=16039.2, ups=4.48, wpb=3579.5, bsz=142.3, num_updates=74200, lr=0.000116091, gnorm=0.976, train_wall=22, gb_free=7, wall=16975
2021-03-11 01:29:45 | INFO | train_inner | epoch 068:    399 / 1103 loss=3.027, nll_loss=1.475, ppl=2.78, wps=15926.8, ups=4.42, wpb=3604, bsz=138.9, num_updates=74300, lr=0.000116013, gnorm=0.962, train_wall=23, gb_free=6.9, wall=16998
2021-03-11 01:30:07 | INFO | train_inner | epoch 068:    499 / 1103 loss=3.014, nll_loss=1.462, ppl=2.76, wps=15824.6, ups=4.49, wpb=3528.1, bsz=156.8, num_updates=74400, lr=0.000115935, gnorm=0.983, train_wall=22, gb_free=7.5, wall=17020
2021-03-11 01:30:29 | INFO | train_inner | epoch 068:    599 / 1103 loss=3.054, nll_loss=1.506, ppl=2.84, wps=15807.6, ups=4.47, wpb=3535.2, bsz=144.2, num_updates=74500, lr=0.000115857, gnorm=0.977, train_wall=22, gb_free=7.3, wall=17042
2021-03-11 01:30:52 | INFO | train_inner | epoch 068:    699 / 1103 loss=3.025, nll_loss=1.476, ppl=2.78, wps=15928, ups=4.46, wpb=3572, bsz=161.6, num_updates=74600, lr=0.000115779, gnorm=0.96, train_wall=22, gb_free=6.9, wall=17065
2021-03-11 01:31:14 | INFO | train_inner | epoch 068:    799 / 1103 loss=3.066, nll_loss=1.523, ppl=2.87, wps=16121.3, ups=4.47, wpb=3607, bsz=152.7, num_updates=74700, lr=0.000115702, gnorm=0.975, train_wall=22, gb_free=7.1, wall=17087
2021-03-11 01:31:36 | INFO | train_inner | epoch 068:    899 / 1103 loss=3.073, nll_loss=1.527, ppl=2.88, wps=15757.1, ups=4.48, wpb=3513.8, bsz=128.2, num_updates=74800, lr=0.000115624, gnorm=0.99, train_wall=22, gb_free=7.1, wall=17109
2021-03-11 01:31:59 | INFO | train_inner | epoch 068:    999 / 1103 loss=3.123, nll_loss=1.584, ppl=3, wps=16163.1, ups=4.46, wpb=3623, bsz=125.4, num_updates=74900, lr=0.000115547, gnorm=0.995, train_wall=22, gb_free=7.3, wall=17132
2021-03-11 01:32:21 | INFO | train_inner | epoch 068:   1099 / 1103 loss=3.069, nll_loss=1.525, ppl=2.88, wps=16068.5, ups=4.45, wpb=3609.9, bsz=144.2, num_updates=75000, lr=0.00011547, gnorm=0.992, train_wall=22, gb_free=7, wall=17154
2021-03-11 01:32:22 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:32:26 | INFO | valid | epoch 068 | valid on 'valid' subset | loss 3.975 | nll_loss 2.419 | ppl 5.35 | wps 48175.2 | wpb 2873.1 | bsz 115.6 | num_updates 75004 | best_loss 3.91
2021-03-11 01:32:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 68 @ 75004 updates
2021-03-11 01:32:26 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:32:28 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:32:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 68 @ 75004 updates, score 3.975) (writing took 2.073491208255291 seconds)
2021-03-11 01:32:28 | INFO | fairseq_cli.train | end of epoch 68 (average epoch stats below)
2021-03-11 01:32:28 | INFO | train | epoch 068 | loss 3.048 | nll_loss 1.5 | ppl 2.83 | wps 15584.2 | ups 4.36 | wpb 3577.8 | bsz 145.3 | num_updates 75004 | lr 0.000115467 | gnorm 0.975 | train_wall 246 | gb_free 6.6 | wall 17161
2021-03-11 01:32:28 | INFO | fairseq.trainer | begin training epoch 69
2021-03-11 01:32:28 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:32:50 | INFO | train_inner | epoch 069:     96 / 1103 loss=3.027, nll_loss=1.473, ppl=2.78, wps=12598.1, ups=3.49, wpb=3608.3, bsz=133.9, num_updates=75100, lr=0.000115393, gnorm=0.959, train_wall=22, gb_free=7.2, wall=17183
2021-03-11 01:33:12 | INFO | train_inner | epoch 069:    196 / 1103 loss=2.996, nll_loss=1.442, ppl=2.72, wps=16008.2, ups=4.49, wpb=3563.2, bsz=164.2, num_updates=75200, lr=0.000115316, gnorm=0.953, train_wall=22, gb_free=6.8, wall=17205
2021-03-11 01:33:35 | INFO | train_inner | epoch 069:    296 / 1103 loss=3.036, nll_loss=1.484, ppl=2.8, wps=16045.1, ups=4.46, wpb=3599.1, bsz=138.2, num_updates=75300, lr=0.00011524, gnorm=0.972, train_wall=22, gb_free=7.5, wall=17228
2021-03-11 01:33:57 | INFO | train_inner | epoch 069:    396 / 1103 loss=3.028, nll_loss=1.475, ppl=2.78, wps=15746.2, ups=4.48, wpb=3512.9, bsz=140.4, num_updates=75400, lr=0.000115163, gnorm=0.986, train_wall=22, gb_free=6.9, wall=17250
2021-03-11 01:34:20 | INFO | train_inner | epoch 069:    496 / 1103 loss=3.034, nll_loss=1.483, ppl=2.8, wps=15823.3, ups=4.4, wpb=3593.4, bsz=145, num_updates=75500, lr=0.000115087, gnorm=0.969, train_wall=23, gb_free=7.2, wall=17273
2021-03-11 01:34:42 | INFO | train_inner | epoch 069:    596 / 1103 loss=3.042, nll_loss=1.494, ppl=2.82, wps=16007.3, ups=4.43, wpb=3611.1, bsz=155, num_updates=75600, lr=0.000115011, gnorm=0.967, train_wall=22, gb_free=7.1, wall=17295
2021-03-11 01:35:05 | INFO | train_inner | epoch 069:    696 / 1103 loss=3.052, nll_loss=1.503, ppl=2.83, wps=15926.1, ups=4.45, wpb=3580.3, bsz=136.6, num_updates=75700, lr=0.000114935, gnorm=0.974, train_wall=22, gb_free=6.8, wall=17318
2021-03-11 01:35:27 | INFO | train_inner | epoch 069:    796 / 1103 loss=3.063, nll_loss=1.516, ppl=2.86, wps=15896.8, ups=4.45, wpb=3569.5, bsz=141.6, num_updates=75800, lr=0.000114859, gnorm=0.984, train_wall=22, gb_free=7.1, wall=17340
2021-03-11 01:35:50 | INFO | train_inner | epoch 069:    896 / 1103 loss=3.038, nll_loss=1.49, ppl=2.81, wps=15887, ups=4.45, wpb=3566.8, bsz=149.7, num_updates=75900, lr=0.000114783, gnorm=0.997, train_wall=22, gb_free=6.9, wall=17363
2021-03-11 01:36:12 | INFO | train_inner | epoch 069:    996 / 1103 loss=3.079, nll_loss=1.537, ppl=2.9, wps=15986, ups=4.46, wpb=3583.3, bsz=144.4, num_updates=76000, lr=0.000114708, gnorm=0.986, train_wall=22, gb_free=7.2, wall=17385
2021-03-11 01:36:34 | INFO | train_inner | epoch 069:   1096 / 1103 loss=3.073, nll_loss=1.529, ppl=2.89, wps=16032.4, ups=4.47, wpb=3587.8, bsz=148.5, num_updates=76100, lr=0.000114632, gnorm=0.974, train_wall=22, gb_free=6.9, wall=17407
2021-03-11 01:36:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:36:40 | INFO | valid | epoch 069 | valid on 'valid' subset | loss 3.994 | nll_loss 2.441 | ppl 5.43 | wps 48287.4 | wpb 2873.1 | bsz 115.6 | num_updates 76107 | best_loss 3.91
2021-03-11 01:36:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 69 @ 76107 updates
2021-03-11 01:36:40 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:36:42 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:36:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 69 @ 76107 updates, score 3.994) (writing took 2.0525691360235214 seconds)
2021-03-11 01:36:42 | INFO | fairseq_cli.train | end of epoch 69 (average epoch stats below)
2021-03-11 01:36:42 | INFO | train | epoch 069 | loss 3.042 | nll_loss 1.493 | ppl 2.81 | wps 15558.2 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 76107 | lr 0.000114627 | gnorm 0.975 | train_wall 246 | gb_free 7.1 | wall 17415
2021-03-11 01:36:42 | INFO | fairseq.trainer | begin training epoch 70
2021-03-11 01:36:42 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:37:02 | INFO | train_inner | epoch 070:     93 / 1103 loss=3.018, nll_loss=1.463, ppl=2.76, wps=12391.2, ups=3.59, wpb=3453.2, bsz=129.9, num_updates=76200, lr=0.000114557, gnorm=1.023, train_wall=22, gb_free=7.1, wall=17435
2021-03-11 01:37:24 | INFO | train_inner | epoch 070:    193 / 1103 loss=2.996, nll_loss=1.441, ppl=2.71, wps=16205.2, ups=4.49, wpb=3607.8, bsz=157.5, num_updates=76300, lr=0.000114482, gnorm=0.942, train_wall=22, gb_free=7.1, wall=17458
2021-03-11 01:37:47 | INFO | train_inner | epoch 070:    293 / 1103 loss=3.007, nll_loss=1.452, ppl=2.74, wps=16010.4, ups=4.42, wpb=3619.1, bsz=144.1, num_updates=76400, lr=0.000114407, gnorm=0.949, train_wall=22, gb_free=7.5, wall=17480
2021-03-11 01:38:09 | INFO | train_inner | epoch 070:    393 / 1103 loss=3.04, nll_loss=1.488, ppl=2.81, wps=15854.2, ups=4.47, wpb=3545.7, bsz=135.9, num_updates=76500, lr=0.000114332, gnorm=1.005, train_wall=22, gb_free=6.9, wall=17503
2021-03-11 01:38:32 | INFO | train_inner | epoch 070:    493 / 1103 loss=3.032, nll_loss=1.482, ppl=2.79, wps=15960.5, ups=4.43, wpb=3601.3, bsz=150.2, num_updates=76600, lr=0.000114258, gnorm=0.974, train_wall=22, gb_free=6.9, wall=17525
2021-03-11 01:38:54 | INFO | train_inner | epoch 070:    593 / 1103 loss=3.052, nll_loss=1.503, ppl=2.83, wps=15828.4, ups=4.46, wpb=3550.8, bsz=137.5, num_updates=76700, lr=0.000114183, gnorm=0.99, train_wall=22, gb_free=7.1, wall=17548
2021-03-11 01:39:17 | INFO | train_inner | epoch 070:    693 / 1103 loss=3.012, nll_loss=1.458, ppl=2.75, wps=15626.2, ups=4.49, wpb=3478.2, bsz=152.9, num_updates=76800, lr=0.000114109, gnorm=1.003, train_wall=22, gb_free=6.9, wall=17570
2021-03-11 01:39:39 | INFO | train_inner | epoch 070:    793 / 1103 loss=3.07, nll_loss=1.526, ppl=2.88, wps=16350.9, ups=4.46, wpb=3667.2, bsz=147.7, num_updates=76900, lr=0.000114035, gnorm=0.97, train_wall=22, gb_free=7, wall=17592
2021-03-11 01:40:02 | INFO | train_inner | epoch 070:    893 / 1103 loss=3.047, nll_loss=1.499, ppl=2.83, wps=15969, ups=4.46, wpb=3583.3, bsz=143.8, num_updates=77000, lr=0.000113961, gnorm=0.976, train_wall=22, gb_free=7.1, wall=17615
2021-03-11 01:40:24 | INFO | train_inner | epoch 070:    993 / 1103 loss=3.057, nll_loss=1.512, ppl=2.85, wps=16276.9, ups=4.52, wpb=3602.9, bsz=148.4, num_updates=77100, lr=0.000113887, gnorm=0.977, train_wall=22, gb_free=6.9, wall=17637
2021-03-11 01:40:46 | INFO | train_inner | epoch 070:   1093 / 1103 loss=3.057, nll_loss=1.512, ppl=2.85, wps=16329.1, ups=4.51, wpb=3624.2, bsz=147.8, num_updates=77200, lr=0.000113813, gnorm=0.969, train_wall=22, gb_free=7, wall=17659
2021-03-11 01:40:48 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:40:52 | INFO | valid | epoch 070 | valid on 'valid' subset | loss 3.988 | nll_loss 2.437 | ppl 5.41 | wps 48214 | wpb 2873.1 | bsz 115.6 | num_updates 77210 | best_loss 3.91
2021-03-11 01:40:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 70 @ 77210 updates
2021-03-11 01:40:52 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:40:54 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:40:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 70 @ 77210 updates, score 3.988) (writing took 2.052996013313532 seconds)
2021-03-11 01:40:54 | INFO | fairseq_cli.train | end of epoch 70 (average epoch stats below)
2021-03-11 01:40:54 | INFO | train | epoch 070 | loss 3.036 | nll_loss 1.486 | ppl 2.8 | wps 15647.3 | ups 4.37 | wpb 3577.8 | bsz 145.3 | num_updates 77210 | lr 0.000113805 | gnorm 0.98 | train_wall 245 | gb_free 6.8 | wall 17667
2021-03-11 01:40:54 | INFO | fairseq.trainer | begin training epoch 71
2021-03-11 01:40:54 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:41:14 | INFO | train_inner | epoch 071:     90 / 1103 loss=3.048, nll_loss=1.496, ppl=2.82, wps=12648.5, ups=3.54, wpb=3569.8, bsz=125.3, num_updates=77300, lr=0.000113739, gnorm=0.999, train_wall=22, gb_free=7, wall=17687
2021-03-11 01:41:37 | INFO | train_inner | epoch 071:    190 / 1103 loss=3.026, nll_loss=1.474, ppl=2.78, wps=16155.7, ups=4.45, wpb=3631, bsz=145, num_updates=77400, lr=0.000113666, gnorm=0.964, train_wall=22, gb_free=6.7, wall=17710
2021-03-11 01:41:59 | INFO | train_inner | epoch 071:    290 / 1103 loss=2.997, nll_loss=1.44, ppl=2.71, wps=15950.3, ups=4.44, wpb=3593.5, bsz=148, num_updates=77500, lr=0.000113592, gnorm=0.959, train_wall=22, gb_free=7.1, wall=17732
2021-03-11 01:42:22 | INFO | train_inner | epoch 071:    390 / 1103 loss=3.031, nll_loss=1.479, ppl=2.79, wps=16123.6, ups=4.43, wpb=3638.8, bsz=142.6, num_updates=77600, lr=0.000113519, gnorm=0.976, train_wall=22, gb_free=7.2, wall=17755
2021-03-11 01:42:44 | INFO | train_inner | epoch 071:    490 / 1103 loss=3.019, nll_loss=1.467, ppl=2.76, wps=15953.1, ups=4.44, wpb=3592.7, bsz=149.8, num_updates=77700, lr=0.000113446, gnorm=0.97, train_wall=22, gb_free=6.8, wall=17777
2021-03-11 01:43:06 | INFO | train_inner | epoch 071:    590 / 1103 loss=3.029, nll_loss=1.477, ppl=2.78, wps=15908.2, ups=4.49, wpb=3546.2, bsz=136.1, num_updates=77800, lr=0.000113373, gnorm=0.988, train_wall=22, gb_free=6.8, wall=17800
2021-03-11 01:43:29 | INFO | train_inner | epoch 071:    690 / 1103 loss=3.043, nll_loss=1.494, ppl=2.82, wps=16040.1, ups=4.42, wpb=3632.1, bsz=145.8, num_updates=77900, lr=0.0001133, gnorm=0.966, train_wall=23, gb_free=7, wall=17822
2021-03-11 01:43:52 | INFO | train_inner | epoch 071:    790 / 1103 loss=3.039, nll_loss=1.49, ppl=2.81, wps=15986, ups=4.44, wpb=3602.7, bsz=150.9, num_updates=78000, lr=0.000113228, gnorm=0.977, train_wall=22, gb_free=7, wall=17845
2021-03-11 01:44:14 | INFO | train_inner | epoch 071:    890 / 1103 loss=3.014, nll_loss=1.464, ppl=2.76, wps=15748.9, ups=4.5, wpb=3498.1, bsz=171.1, num_updates=78100, lr=0.000113155, gnorm=0.977, train_wall=22, gb_free=7.7, wall=17867
2021-03-11 01:44:36 | INFO | train_inner | epoch 071:    990 / 1103 loss=3.054, nll_loss=1.506, ppl=2.84, wps=15853.3, ups=4.45, wpb=3565.7, bsz=136.6, num_updates=78200, lr=0.000113083, gnorm=0.989, train_wall=22, gb_free=7, wall=17889
2021-03-11 01:44:59 | INFO | train_inner | epoch 071:   1090 / 1103 loss=3.033, nll_loss=1.484, ppl=2.8, wps=15747.1, ups=4.47, wpb=3526.7, bsz=155.7, num_updates=78300, lr=0.000113011, gnorm=0.994, train_wall=22, gb_free=6.7, wall=17912
2021-03-11 01:45:01 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:45:05 | INFO | valid | epoch 071 | valid on 'valid' subset | loss 3.994 | nll_loss 2.441 | ppl 5.43 | wps 48259.3 | wpb 2873.1 | bsz 115.6 | num_updates 78313 | best_loss 3.91
2021-03-11 01:45:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 71 @ 78313 updates
2021-03-11 01:45:05 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:45:07 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:45:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 71 @ 78313 updates, score 3.994) (writing took 2.190216187387705 seconds)
2021-03-11 01:45:07 | INFO | fairseq_cli.train | end of epoch 71 (average epoch stats below)
2021-03-11 01:45:07 | INFO | train | epoch 071 | loss 3.03 | nll_loss 1.479 | ppl 2.79 | wps 15560.4 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 78313 | lr 0.000113001 | gnorm 0.98 | train_wall 246 | gb_free 6.9 | wall 17921
2021-03-11 01:45:08 | INFO | fairseq.trainer | begin training epoch 72
2021-03-11 01:45:08 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:45:27 | INFO | train_inner | epoch 072:     87 / 1103 loss=3.038, nll_loss=1.485, ppl=2.8, wps=12541.6, ups=3.52, wpb=3563.1, bsz=124.5, num_updates=78400, lr=0.000112938, gnorm=1.006, train_wall=22, gb_free=7, wall=17940
2021-03-11 01:45:50 | INFO | train_inner | epoch 072:    187 / 1103 loss=3.005, nll_loss=1.449, ppl=2.73, wps=16054.8, ups=4.44, wpb=3618.9, bsz=149.9, num_updates=78500, lr=0.000112867, gnorm=0.963, train_wall=22, gb_free=7.2, wall=17963
2021-03-11 01:46:12 | INFO | train_inner | epoch 072:    287 / 1103 loss=2.999, nll_loss=1.443, ppl=2.72, wps=15835.6, ups=4.45, wpb=3560, bsz=144.4, num_updates=78600, lr=0.000112795, gnorm=0.975, train_wall=22, gb_free=7.5, wall=17985
2021-03-11 01:46:34 | INFO | train_inner | epoch 072:    387 / 1103 loss=3.027, nll_loss=1.474, ppl=2.78, wps=16023, ups=4.49, wpb=3570.8, bsz=136.1, num_updates=78700, lr=0.000112723, gnorm=0.996, train_wall=22, gb_free=7.6, wall=18008
2021-03-11 01:46:57 | INFO | train_inner | epoch 072:    487 / 1103 loss=3.016, nll_loss=1.461, ppl=2.75, wps=15751.9, ups=4.51, wpb=3494.8, bsz=142, num_updates=78800, lr=0.000112651, gnorm=0.992, train_wall=22, gb_free=7, wall=18030
2021-03-11 01:47:19 | INFO | train_inner | epoch 072:    587 / 1103 loss=2.999, nll_loss=1.447, ppl=2.73, wps=16135.7, ups=4.43, wpb=3642.9, bsz=170, num_updates=78900, lr=0.00011258, gnorm=0.96, train_wall=22, gb_free=7.6, wall=18052
2021-03-11 01:47:42 | INFO | train_inner | epoch 072:    687 / 1103 loss=3.035, nll_loss=1.486, ppl=2.8, wps=16199.8, ups=4.46, wpb=3633.6, bsz=157.7, num_updates=79000, lr=0.000112509, gnorm=0.973, train_wall=22, gb_free=7.1, wall=18075
2021-03-11 01:48:04 | INFO | train_inner | epoch 072:    787 / 1103 loss=3.019, nll_loss=1.468, ppl=2.77, wps=15879.9, ups=4.44, wpb=3579.3, bsz=159.1, num_updates=79100, lr=0.000112438, gnorm=0.968, train_wall=22, gb_free=7.2, wall=18097
2021-03-11 01:48:27 | INFO | train_inner | epoch 072:    887 / 1103 loss=3.056, nll_loss=1.509, ppl=2.85, wps=15933.9, ups=4.46, wpb=3572.1, bsz=148.6, num_updates=79200, lr=0.000112367, gnorm=0.994, train_wall=22, gb_free=7, wall=18120
2021-03-11 01:48:49 | INFO | train_inner | epoch 072:    987 / 1103 loss=3.051, nll_loss=1.5, ppl=2.83, wps=15554.9, ups=4.47, wpb=3478.7, bsz=124.3, num_updates=79300, lr=0.000112296, gnorm=1.021, train_wall=22, gb_free=7.2, wall=18142
2021-03-11 01:49:11 | INFO | train_inner | epoch 072:   1087 / 1103 loss=3.051, nll_loss=1.503, ppl=2.83, wps=15994.6, ups=4.46, wpb=3584.2, bsz=132.2, num_updates=79400, lr=0.000112225, gnorm=0.999, train_wall=22, gb_free=6.8, wall=18165
2021-03-11 01:49:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:49:19 | INFO | valid | epoch 072 | valid on 'valid' subset | loss 4 | nll_loss 2.446 | ppl 5.45 | wps 48175.7 | wpb 2873.1 | bsz 115.6 | num_updates 79416 | best_loss 3.91
2021-03-11 01:49:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 72 @ 79416 updates
2021-03-11 01:49:19 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:49:21 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:49:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 72 @ 79416 updates, score 4.0) (writing took 2.131737008690834 seconds)
2021-03-11 01:49:21 | INFO | fairseq_cli.train | end of epoch 72 (average epoch stats below)
2021-03-11 01:49:21 | INFO | train | epoch 072 | loss 3.026 | nll_loss 1.474 | ppl 2.78 | wps 15568.3 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 79416 | lr 0.000112214 | gnorm 0.983 | train_wall 246 | gb_free 6.9 | wall 18174
2021-03-11 01:49:21 | INFO | fairseq.trainer | begin training epoch 73
2021-03-11 01:49:21 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:49:40 | INFO | train_inner | epoch 073:     84 / 1103 loss=3.004, nll_loss=1.447, ppl=2.73, wps=12666.3, ups=3.5, wpb=3620.6, bsz=139.5, num_updates=79500, lr=0.000112154, gnorm=0.965, train_wall=22, gb_free=7, wall=18193
2021-03-11 01:50:02 | INFO | train_inner | epoch 073:    184 / 1103 loss=3.011, nll_loss=1.456, ppl=2.74, wps=16006.5, ups=4.48, wpb=3573, bsz=144.6, num_updates=79600, lr=0.000112084, gnorm=0.974, train_wall=22, gb_free=7, wall=18215
2021-03-11 01:50:25 | INFO | train_inner | epoch 073:    284 / 1103 loss=3.011, nll_loss=1.456, ppl=2.74, wps=15927.6, ups=4.44, wpb=3586.5, bsz=142.2, num_updates=79700, lr=0.000112014, gnorm=0.98, train_wall=22, gb_free=7, wall=18238
2021-03-11 01:50:47 | INFO | train_inner | epoch 073:    384 / 1103 loss=3.004, nll_loss=1.449, ppl=2.73, wps=15937.4, ups=4.42, wpb=3606.1, bsz=148, num_updates=79800, lr=0.000111943, gnorm=0.97, train_wall=23, gb_free=6.9, wall=18261
2021-03-11 01:51:10 | INFO | train_inner | epoch 073:    484 / 1103 loss=3.014, nll_loss=1.461, ppl=2.75, wps=15918.6, ups=4.44, wpb=3587.3, bsz=149.1, num_updates=79900, lr=0.000111873, gnorm=0.987, train_wall=22, gb_free=6.9, wall=18283
2021-03-11 01:51:32 | INFO | train_inner | epoch 073:    584 / 1103 loss=3.045, nll_loss=1.496, ppl=2.82, wps=16055.1, ups=4.47, wpb=3593.3, bsz=137, num_updates=80000, lr=0.000111803, gnorm=0.996, train_wall=22, gb_free=6.8, wall=18306
2021-03-11 01:51:55 | INFO | train_inner | epoch 073:    684 / 1103 loss=3.028, nll_loss=1.476, ppl=2.78, wps=15936.8, ups=4.46, wpb=3576.5, bsz=146.1, num_updates=80100, lr=0.000111734, gnorm=0.993, train_wall=22, gb_free=6.9, wall=18328
2021-03-11 01:52:17 | INFO | train_inner | epoch 073:    784 / 1103 loss=3.019, nll_loss=1.468, ppl=2.77, wps=15942.1, ups=4.45, wpb=3578.8, bsz=155.4, num_updates=80200, lr=0.000111664, gnorm=0.991, train_wall=22, gb_free=7.4, wall=18350
2021-03-11 01:52:39 | INFO | train_inner | epoch 073:    884 / 1103 loss=3.026, nll_loss=1.476, ppl=2.78, wps=15944.3, ups=4.5, wpb=3542.9, bsz=155.4, num_updates=80300, lr=0.000111594, gnorm=1.017, train_wall=22, gb_free=7, wall=18373
2021-03-11 01:53:02 | INFO | train_inner | epoch 073:    984 / 1103 loss=3.021, nll_loss=1.469, ppl=2.77, wps=15792.9, ups=4.46, wpb=3540.2, bsz=149.7, num_updates=80400, lr=0.000111525, gnorm=0.985, train_wall=22, gb_free=7, wall=18395
2021-03-11 01:53:24 | INFO | train_inner | epoch 073:   1084 / 1103 loss=3.031, nll_loss=1.479, ppl=2.79, wps=16012.8, ups=4.49, wpb=3563.8, bsz=136.6, num_updates=80500, lr=0.000111456, gnorm=0.994, train_wall=22, gb_free=6.9, wall=18417
2021-03-11 01:53:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:53:32 | INFO | valid | epoch 073 | valid on 'valid' subset | loss 3.994 | nll_loss 2.445 | ppl 5.44 | wps 48250.8 | wpb 2873.1 | bsz 115.6 | num_updates 80519 | best_loss 3.91
2021-03-11 01:53:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 73 @ 80519 updates
2021-03-11 01:53:32 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:53:34 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:53:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 73 @ 80519 updates, score 3.994) (writing took 2.051988083869219 seconds)
2021-03-11 01:53:34 | INFO | fairseq_cli.train | end of epoch 73 (average epoch stats below)
2021-03-11 01:53:34 | INFO | train | epoch 073 | loss 3.02 | nll_loss 1.468 | ppl 2.77 | wps 15586.1 | ups 4.36 | wpb 3577.8 | bsz 145.3 | num_updates 80519 | lr 0.000111442 | gnorm 0.988 | train_wall 246 | gb_free 7.3 | wall 18427
2021-03-11 01:53:34 | INFO | fairseq.trainer | begin training epoch 74
2021-03-11 01:53:34 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:53:52 | INFO | train_inner | epoch 074:     81 / 1103 loss=2.991, nll_loss=1.434, ppl=2.7, wps=12506.8, ups=3.57, wpb=3498.8, bsz=148, num_updates=80600, lr=0.000111386, gnorm=1.008, train_wall=22, gb_free=7, wall=18445
2021-03-11 01:54:15 | INFO | train_inner | epoch 074:    181 / 1103 loss=2.99, nll_loss=1.433, ppl=2.7, wps=16035.8, ups=4.41, wpb=3639, bsz=147, num_updates=80700, lr=0.000111317, gnorm=0.966, train_wall=23, gb_free=6.9, wall=18468
2021-03-11 01:54:37 | INFO | train_inner | epoch 074:    281 / 1103 loss=2.989, nll_loss=1.432, ppl=2.7, wps=15960.8, ups=4.43, wpb=3606.9, bsz=150, num_updates=80800, lr=0.000111249, gnorm=0.968, train_wall=22, gb_free=7.2, wall=18491
2021-03-11 01:55:00 | INFO | train_inner | epoch 074:    381 / 1103 loss=3.032, nll_loss=1.48, ppl=2.79, wps=16201.8, ups=4.46, wpb=3631.5, bsz=139.9, num_updates=80900, lr=0.00011118, gnorm=0.989, train_wall=22, gb_free=7.3, wall=18513
2021-03-11 01:55:22 | INFO | train_inner | epoch 074:    481 / 1103 loss=3.02, nll_loss=1.466, ppl=2.76, wps=15896.1, ups=4.46, wpb=3560.3, bsz=138.4, num_updates=81000, lr=0.000111111, gnorm=0.997, train_wall=22, gb_free=7.1, wall=18535
2021-03-11 01:55:45 | INFO | train_inner | epoch 074:    581 / 1103 loss=3.011, nll_loss=1.458, ppl=2.75, wps=15910.6, ups=4.45, wpb=3576.3, bsz=153.9, num_updates=81100, lr=0.000111043, gnorm=0.979, train_wall=22, gb_free=7.1, wall=18558
2021-03-11 01:56:07 | INFO | train_inner | epoch 074:    681 / 1103 loss=2.992, nll_loss=1.435, ppl=2.7, wps=15705.9, ups=4.43, wpb=3542.6, bsz=154.3, num_updates=81200, lr=0.000110974, gnorm=0.988, train_wall=22, gb_free=7.1, wall=18580
2021-03-11 01:56:30 | INFO | train_inner | epoch 074:    781 / 1103 loss=3.031, nll_loss=1.477, ppl=2.78, wps=15818, ups=4.46, wpb=3542.9, bsz=133.6, num_updates=81300, lr=0.000110906, gnorm=1.012, train_wall=22, gb_free=7, wall=18603
2021-03-11 01:56:52 | INFO | train_inner | epoch 074:    881 / 1103 loss=3.018, nll_loss=1.464, ppl=2.76, wps=15694.6, ups=4.43, wpb=3543.4, bsz=140.5, num_updates=81400, lr=0.000110838, gnorm=0.995, train_wall=22, gb_free=6.9, wall=18625
2021-03-11 01:57:15 | INFO | train_inner | epoch 074:    981 / 1103 loss=3.055, nll_loss=1.508, ppl=2.84, wps=16190.5, ups=4.45, wpb=3639.1, bsz=145.1, num_updates=81500, lr=0.00011077, gnorm=0.976, train_wall=22, gb_free=6.9, wall=18648
2021-03-11 01:57:37 | INFO | train_inner | epoch 074:   1081 / 1103 loss=3.044, nll_loss=1.495, ppl=2.82, wps=16037.2, ups=4.47, wpb=3588.2, bsz=144.6, num_updates=81600, lr=0.000110702, gnorm=1.001, train_wall=22, gb_free=7.1, wall=18670
2021-03-11 01:57:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 01:57:46 | INFO | valid | epoch 074 | valid on 'valid' subset | loss 4.007 | nll_loss 2.456 | ppl 5.49 | wps 48213.9 | wpb 2873.1 | bsz 115.6 | num_updates 81622 | best_loss 3.91
2021-03-11 01:57:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 74 @ 81622 updates
2021-03-11 01:57:46 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:57:48 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 01:57:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 74 @ 81622 updates, score 4.007) (writing took 2.058373052626848 seconds)
2021-03-11 01:57:48 | INFO | fairseq_cli.train | end of epoch 74 (average epoch stats below)
2021-03-11 01:57:48 | INFO | train | epoch 074 | loss 3.015 | nll_loss 1.462 | ppl 2.75 | wps 15562.4 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 81622 | lr 0.000110687 | gnorm 0.989 | train_wall 246 | gb_free 7 | wall 18681
2021-03-11 01:57:48 | INFO | fairseq.trainer | begin training epoch 75
2021-03-11 01:57:48 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 01:58:05 | INFO | train_inner | epoch 075:     78 / 1103 loss=2.991, nll_loss=1.432, ppl=2.7, wps=12561, ups=3.52, wpb=3567.6, bsz=138.5, num_updates=81700, lr=0.000110634, gnorm=0.984, train_wall=22, gb_free=7, wall=18699
2021-03-11 01:58:28 | INFO | train_inner | epoch 075:    178 / 1103 loss=2.987, nll_loss=1.428, ppl=2.69, wps=15968.8, ups=4.45, wpb=3589.4, bsz=137, num_updates=81800, lr=0.000110566, gnorm=0.967, train_wall=22, gb_free=6.8, wall=18721
2021-03-11 01:58:50 | INFO | train_inner | epoch 075:    278 / 1103 loss=2.997, nll_loss=1.442, ppl=2.72, wps=16151.3, ups=4.48, wpb=3601.4, bsz=162.3, num_updates=81900, lr=0.000110499, gnorm=0.985, train_wall=22, gb_free=7.1, wall=18743
2021-03-11 01:59:13 | INFO | train_inner | epoch 075:    378 / 1103 loss=3.003, nll_loss=1.448, ppl=2.73, wps=16092.1, ups=4.44, wpb=3623.3, bsz=143.6, num_updates=82000, lr=0.000110432, gnorm=0.979, train_wall=22, gb_free=6.8, wall=18766
2021-03-11 01:59:35 | INFO | train_inner | epoch 075:    478 / 1103 loss=3.014, nll_loss=1.459, ppl=2.75, wps=15912.1, ups=4.5, wpb=3534.3, bsz=139.2, num_updates=82100, lr=0.000110364, gnorm=1.031, train_wall=22, gb_free=6.8, wall=18788
2021-03-11 01:59:58 | INFO | train_inner | epoch 075:    578 / 1103 loss=2.973, nll_loss=1.413, ppl=2.66, wps=15689.8, ups=4.4, wpb=3567.3, bsz=159.3, num_updates=82200, lr=0.000110297, gnorm=0.972, train_wall=23, gb_free=6.9, wall=18811
2021-03-11 02:00:20 | INFO | train_inner | epoch 075:    678 / 1103 loss=3.011, nll_loss=1.46, ppl=2.75, wps=16074, ups=4.45, wpb=3609.4, bsz=162.5, num_updates=82300, lr=0.00011023, gnorm=0.988, train_wall=22, gb_free=7.6, wall=18833
2021-03-11 02:00:43 | INFO | train_inner | epoch 075:    778 / 1103 loss=3.033, nll_loss=1.482, ppl=2.79, wps=16052.5, ups=4.39, wpb=3656.1, bsz=143.4, num_updates=82400, lr=0.000110163, gnorm=0.995, train_wall=23, gb_free=7, wall=18856
2021-03-11 02:01:05 | INFO | train_inner | epoch 075:    878 / 1103 loss=3.04, nll_loss=1.49, ppl=2.81, wps=15952, ups=4.48, wpb=3563.1, bsz=134.7, num_updates=82500, lr=0.000110096, gnorm=1.001, train_wall=22, gb_free=7.2, wall=18878
2021-03-11 02:01:28 | INFO | train_inner | epoch 075:    978 / 1103 loss=3.032, nll_loss=1.48, ppl=2.79, wps=15770.9, ups=4.44, wpb=3550, bsz=134.2, num_updates=82600, lr=0.00011003, gnorm=1.013, train_wall=22, gb_free=7.4, wall=18901
2021-03-11 02:01:50 | INFO | train_inner | epoch 075:   1078 / 1103 loss=3.044, nll_loss=1.493, ppl=2.82, wps=15917.3, ups=4.52, wpb=3518.6, bsz=136.2, num_updates=82700, lr=0.000109963, gnorm=1.007, train_wall=22, gb_free=7, wall=18923
2021-03-11 02:01:55 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:01:59 | INFO | valid | epoch 075 | valid on 'valid' subset | loss 4.005 | nll_loss 2.452 | ppl 5.47 | wps 48214.9 | wpb 2873.1 | bsz 115.6 | num_updates 82725 | best_loss 3.91
2021-03-11 02:01:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 75 @ 82725 updates
2021-03-11 02:01:59 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:02:01 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:02:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 75 @ 82725 updates, score 4.005) (writing took 2.059489395469427 seconds)
2021-03-11 02:02:01 | INFO | fairseq_cli.train | end of epoch 75 (average epoch stats below)
2021-03-11 02:02:01 | INFO | train | epoch 075 | loss 3.01 | nll_loss 1.456 | ppl 2.74 | wps 15562.7 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 82725 | lr 0.000109947 | gnorm 0.993 | train_wall 246 | gb_free 6.8 | wall 18934
2021-03-11 02:02:01 | INFO | fairseq.trainer | begin training epoch 76
2021-03-11 02:02:01 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 02:02:18 | INFO | train_inner | epoch 076:     75 / 1103 loss=2.985, nll_loss=1.427, ppl=2.69, wps=12726.4, ups=3.5, wpb=3631.4, bsz=152.3, num_updates=82800, lr=0.000109897, gnorm=0.969, train_wall=22, gb_free=7, wall=18952
2021-03-11 02:02:41 | INFO | train_inner | epoch 076:    175 / 1103 loss=2.978, nll_loss=1.418, ppl=2.67, wps=16018.3, ups=4.46, wpb=3590.1, bsz=144.3, num_updates=82900, lr=0.00010983, gnorm=0.988, train_wall=22, gb_free=6.8, wall=18974
2021-03-11 02:03:04 | INFO | train_inner | epoch 076:    275 / 1103 loss=2.954, nll_loss=1.391, ppl=2.62, wps=15834.7, ups=4.41, wpb=3592.7, bsz=157.5, num_updates=83000, lr=0.000109764, gnorm=0.96, train_wall=23, gb_free=6.8, wall=18997
2021-03-11 02:03:26 | INFO | train_inner | epoch 076:    375 / 1103 loss=3, nll_loss=1.444, ppl=2.72, wps=15907.7, ups=4.46, wpb=3563.8, bsz=140.9, num_updates=83100, lr=0.000109698, gnorm=1, train_wall=22, gb_free=6.8, wall=19019
2021-03-11 02:03:48 | INFO | train_inner | epoch 076:    475 / 1103 loss=2.985, nll_loss=1.428, ppl=2.69, wps=15982.7, ups=4.44, wpb=3598.1, bsz=157.4, num_updates=83200, lr=0.000109632, gnorm=0.971, train_wall=22, gb_free=6.7, wall=19042
2021-03-11 02:04:11 | INFO | train_inner | epoch 076:    575 / 1103 loss=2.991, nll_loss=1.434, ppl=2.7, wps=15891.1, ups=4.45, wpb=3573, bsz=153.4, num_updates=83300, lr=0.000109566, gnorm=0.984, train_wall=22, gb_free=6.8, wall=19064
2021-03-11 02:04:33 | INFO | train_inner | epoch 076:    675 / 1103 loss=3.042, nll_loss=1.49, ppl=2.81, wps=15888.4, ups=4.46, wpb=3560.3, bsz=123.8, num_updates=83400, lr=0.000109501, gnorm=1.025, train_wall=22, gb_free=7, wall=19086
2021-03-11 02:04:56 | INFO | train_inner | epoch 076:    775 / 1103 loss=3.011, nll_loss=1.455, ppl=2.74, wps=15764.1, ups=4.45, wpb=3543.4, bsz=133.6, num_updates=83500, lr=0.000109435, gnorm=1.007, train_wall=22, gb_free=7.3, wall=19109
2021-03-11 02:05:18 | INFO | train_inner | epoch 076:    875 / 1103 loss=3.023, nll_loss=1.47, ppl=2.77, wps=15791.9, ups=4.48, wpb=3528.6, bsz=141.7, num_updates=83600, lr=0.00010937, gnorm=1.009, train_wall=22, gb_free=7.1, wall=19131
2021-03-11 02:05:40 | INFO | train_inner | epoch 076:    975 / 1103 loss=3.037, nll_loss=1.487, ppl=2.8, wps=15870.1, ups=4.49, wpb=3536.4, bsz=149, num_updates=83700, lr=0.000109304, gnorm=1.013, train_wall=22, gb_free=7.5, wall=19154
2021-03-11 02:06:03 | INFO | train_inner | epoch 076:   1075 / 1103 loss=3.029, nll_loss=1.478, ppl=2.79, wps=15956.7, ups=4.44, wpb=3593.8, bsz=150.2, num_updates=83800, lr=0.000109239, gnorm=0.99, train_wall=22, gb_free=7, wall=19176
2021-03-11 02:06:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:06:13 | INFO | valid | epoch 076 | valid on 'valid' subset | loss 4 | nll_loss 2.452 | ppl 5.47 | wps 48186.8 | wpb 2873.1 | bsz 115.6 | num_updates 83828 | best_loss 3.91
2021-03-11 02:06:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 76 @ 83828 updates
2021-03-11 02:06:13 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:06:15 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:06:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 76 @ 83828 updates, score 4.0) (writing took 2.163887072354555 seconds)
2021-03-11 02:06:15 | INFO | fairseq_cli.train | end of epoch 76 (average epoch stats below)
2021-03-11 02:06:15 | INFO | train | epoch 076 | loss 3.005 | nll_loss 1.45 | ppl 2.73 | wps 15551.3 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 83828 | lr 0.000109221 | gnorm 0.992 | train_wall 246 | gb_free 7.2 | wall 19188
2021-03-11 02:06:15 | INFO | fairseq.trainer | begin training epoch 77
2021-03-11 02:06:15 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 02:06:31 | INFO | train_inner | epoch 077:     72 / 1103 loss=3.008, nll_loss=1.452, ppl=2.74, wps=12659.1, ups=3.55, wpb=3568.3, bsz=134.5, num_updates=83900, lr=0.000109174, gnorm=0.995, train_wall=22, gb_free=7, wall=19204
2021-03-11 02:06:54 | INFO | train_inner | epoch 077:    172 / 1103 loss=2.968, nll_loss=1.406, ppl=2.65, wps=16040, ups=4.41, wpb=3639.4, bsz=150.8, num_updates=84000, lr=0.000109109, gnorm=0.961, train_wall=23, gb_free=7.1, wall=19227
2021-03-11 02:07:16 | INFO | train_inner | epoch 077:    272 / 1103 loss=2.954, nll_loss=1.392, ppl=2.62, wps=15839.6, ups=4.45, wpb=3557.8, bsz=153.8, num_updates=84100, lr=0.000109044, gnorm=0.97, train_wall=22, gb_free=7.2, wall=19249
2021-03-11 02:07:39 | INFO | train_inner | epoch 077:    372 / 1103 loss=2.981, nll_loss=1.421, ppl=2.68, wps=15851.7, ups=4.43, wpb=3577.4, bsz=145.7, num_updates=84200, lr=0.000108979, gnorm=0.999, train_wall=22, gb_free=7.2, wall=19272
2021-03-11 02:08:01 | INFO | train_inner | epoch 077:    472 / 1103 loss=3.002, nll_loss=1.446, ppl=2.72, wps=15953.7, ups=4.46, wpb=3580, bsz=140.1, num_updates=84300, lr=0.000108915, gnorm=0.997, train_wall=22, gb_free=7.6, wall=19294
2021-03-11 02:08:24 | INFO | train_inner | epoch 077:    572 / 1103 loss=3.037, nll_loss=1.485, ppl=2.8, wps=16026.8, ups=4.48, wpb=3577.8, bsz=133.8, num_updates=84400, lr=0.00010885, gnorm=1.025, train_wall=22, gb_free=6.9, wall=19317
2021-03-11 02:08:46 | INFO | train_inner | epoch 077:    672 / 1103 loss=3.013, nll_loss=1.457, ppl=2.75, wps=15865.7, ups=4.49, wpb=3532.3, bsz=143.8, num_updates=84500, lr=0.000108786, gnorm=1.004, train_wall=22, gb_free=7.7, wall=19339
2021-03-11 02:09:08 | INFO | train_inner | epoch 077:    772 / 1103 loss=3.014, nll_loss=1.46, ppl=2.75, wps=15955.3, ups=4.46, wpb=3576.6, bsz=140.3, num_updates=84600, lr=0.000108721, gnorm=0.995, train_wall=22, gb_free=7.2, wall=19361
2021-03-11 02:09:31 | INFO | train_inner | epoch 077:    872 / 1103 loss=2.995, nll_loss=1.441, ppl=2.71, wps=15901.5, ups=4.44, wpb=3582.2, bsz=160.9, num_updates=84700, lr=0.000108657, gnorm=0.99, train_wall=22, gb_free=7, wall=19384
2021-03-11 02:09:53 | INFO | train_inner | epoch 077:    972 / 1103 loss=3.019, nll_loss=1.466, ppl=2.76, wps=15896.2, ups=4.43, wpb=3586.9, bsz=141.8, num_updates=84800, lr=0.000108593, gnorm=1.015, train_wall=22, gb_free=7.3, wall=19407
2021-03-11 02:10:16 | INFO | train_inner | epoch 077:   1072 / 1103 loss=3.023, nll_loss=1.47, ppl=2.77, wps=15910.7, ups=4.43, wpb=3591.1, bsz=136.1, num_updates=84900, lr=0.000108529, gnorm=0.995, train_wall=22, gb_free=7, wall=19429
2021-03-11 02:10:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:10:27 | INFO | valid | epoch 077 | valid on 'valid' subset | loss 4.018 | nll_loss 2.463 | ppl 5.51 | wps 48262.1 | wpb 2873.1 | bsz 115.6 | num_updates 84931 | best_loss 3.91
2021-03-11 02:10:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 77 @ 84931 updates
2021-03-11 02:10:27 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:10:29 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:10:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 77 @ 84931 updates, score 4.018) (writing took 2.1528105549514294 seconds)
2021-03-11 02:10:29 | INFO | fairseq_cli.train | end of epoch 77 (average epoch stats below)
2021-03-11 02:10:29 | INFO | train | epoch 077 | loss 2.998 | nll_loss 1.442 | ppl 2.72 | wps 15549.2 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 84931 | lr 0.000108509 | gnorm 0.995 | train_wall 246 | gb_free 7.1 | wall 19442
2021-03-11 02:10:29 | INFO | fairseq.trainer | begin training epoch 78
2021-03-11 02:10:29 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 02:10:45 | INFO | train_inner | epoch 078:     69 / 1103 loss=2.97, nll_loss=1.41, ppl=2.66, wps=12672.2, ups=3.5, wpb=3620.5, bsz=153.7, num_updates=85000, lr=0.000108465, gnorm=0.969, train_wall=22, gb_free=7.3, wall=19458
2021-03-11 02:11:07 | INFO | train_inner | epoch 078:    169 / 1103 loss=2.969, nll_loss=1.405, ppl=2.65, wps=15607.1, ups=4.47, wpb=3491.5, bsz=141.3, num_updates=85100, lr=0.000108401, gnorm=1.017, train_wall=22, gb_free=7, wall=19480
2021-03-11 02:11:30 | INFO | train_inner | epoch 078:    269 / 1103 loss=2.964, nll_loss=1.402, ppl=2.64, wps=15997.8, ups=4.41, wpb=3624.3, bsz=153.5, num_updates=85200, lr=0.000108338, gnorm=0.961, train_wall=23, gb_free=6.9, wall=19503
2021-03-11 02:11:52 | INFO | train_inner | epoch 078:    369 / 1103 loss=2.988, nll_loss=1.43, ppl=2.7, wps=16030, ups=4.48, wpb=3579.6, bsz=144.6, num_updates=85300, lr=0.000108274, gnorm=0.994, train_wall=22, gb_free=7.2, wall=19525
2021-03-11 02:12:14 | INFO | train_inner | epoch 078:    469 / 1103 loss=2.974, nll_loss=1.413, ppl=2.66, wps=15823.3, ups=4.45, wpb=3555.2, bsz=142.9, num_updates=85400, lr=0.000108211, gnorm=1.013, train_wall=22, gb_free=7.2, wall=19548
2021-03-11 02:12:37 | INFO | train_inner | epoch 078:    569 / 1103 loss=2.995, nll_loss=1.438, ppl=2.71, wps=16002.3, ups=4.43, wpb=3612.9, bsz=152.2, num_updates=85500, lr=0.000108148, gnorm=0.988, train_wall=22, gb_free=6.8, wall=19570
2021-03-11 02:12:59 | INFO | train_inner | epoch 078:    669 / 1103 loss=2.989, nll_loss=1.433, ppl=2.7, wps=15979.7, ups=4.49, wpb=3555.2, bsz=157.8, num_updates=85600, lr=0.000108084, gnorm=0.985, train_wall=22, gb_free=7.2, wall=19592
2021-03-11 02:13:22 | INFO | train_inner | epoch 078:    769 / 1103 loss=3.024, nll_loss=1.471, ppl=2.77, wps=15972.7, ups=4.48, wpb=3565.3, bsz=141.4, num_updates=85700, lr=0.000108021, gnorm=1.018, train_wall=22, gb_free=7.2, wall=19615
2021-03-11 02:13:44 | INFO | train_inner | epoch 078:    869 / 1103 loss=3.042, nll_loss=1.491, ppl=2.81, wps=15996.8, ups=4.48, wpb=3567.3, bsz=133.5, num_updates=85800, lr=0.000107958, gnorm=1.02, train_wall=22, gb_free=7, wall=19637
2021-03-11 02:14:06 | INFO | train_inner | epoch 078:    969 / 1103 loss=3.018, nll_loss=1.465, ppl=2.76, wps=15952, ups=4.48, wpb=3561.4, bsz=147.8, num_updates=85900, lr=0.000107896, gnorm=1.003, train_wall=22, gb_free=6.9, wall=19659
2021-03-11 02:14:29 | INFO | train_inner | epoch 078:   1069 / 1103 loss=2.992, nll_loss=1.435, ppl=2.7, wps=15872.3, ups=4.4, wpb=3610.8, bsz=147.9, num_updates=86000, lr=0.000107833, gnorm=0.99, train_wall=23, gb_free=6.8, wall=19682
2021-03-11 02:14:36 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:14:40 | INFO | valid | epoch 078 | valid on 'valid' subset | loss 4.009 | nll_loss 2.455 | ppl 5.48 | wps 48200.7 | wpb 2873.1 | bsz 115.6 | num_updates 86034 | best_loss 3.91
2021-03-11 02:14:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 78 @ 86034 updates
2021-03-11 02:14:40 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:14:43 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:14:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 78 @ 86034 updates, score 4.009) (writing took 2.4115727432072163 seconds)
2021-03-11 02:14:43 | INFO | fairseq_cli.train | end of epoch 78 (average epoch stats below)
2021-03-11 02:14:43 | INFO | train | epoch 078 | loss 2.995 | nll_loss 1.438 | ppl 2.71 | wps 15548.3 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 86034 | lr 0.000107811 | gnorm 0.997 | train_wall 246 | gb_free 7 | wall 19696
2021-03-11 02:14:43 | INFO | fairseq.trainer | begin training epoch 79
2021-03-11 02:14:43 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 02:14:57 | INFO | train_inner | epoch 079:     66 / 1103 loss=2.994, nll_loss=1.435, ppl=2.7, wps=12462.3, ups=3.51, wpb=3552.8, bsz=133.5, num_updates=86100, lr=0.00010777, gnorm=1.017, train_wall=22, gb_free=7.6, wall=19711
2021-03-11 02:15:20 | INFO | train_inner | epoch 079:    166 / 1103 loss=2.946, nll_loss=1.382, ppl=2.61, wps=15893.7, ups=4.42, wpb=3595.9, bsz=150.4, num_updates=86200, lr=0.000107708, gnorm=0.965, train_wall=23, gb_free=7.2, wall=19733
2021-03-11 02:15:42 | INFO | train_inner | epoch 079:    266 / 1103 loss=2.978, nll_loss=1.414, ppl=2.66, wps=15699.4, ups=4.47, wpb=3510.6, bsz=128.7, num_updates=86300, lr=0.000107645, gnorm=1.014, train_wall=22, gb_free=7.2, wall=19756
2021-03-11 02:16:05 | INFO | train_inner | epoch 079:    366 / 1103 loss=2.965, nll_loss=1.402, ppl=2.64, wps=15816.8, ups=4.48, wpb=3528.3, bsz=143, num_updates=86400, lr=0.000107583, gnorm=1.011, train_wall=22, gb_free=7, wall=19778
2021-03-11 02:16:27 | INFO | train_inner | epoch 079:    466 / 1103 loss=2.988, nll_loss=1.431, ppl=2.7, wps=16050.2, ups=4.44, wpb=3611.8, bsz=152.6, num_updates=86500, lr=0.000107521, gnorm=0.99, train_wall=22, gb_free=7.5, wall=19800
2021-03-11 02:16:50 | INFO | train_inner | epoch 079:    566 / 1103 loss=2.964, nll_loss=1.402, ppl=2.64, wps=15733, ups=4.48, wpb=3515.7, bsz=148.6, num_updates=86600, lr=0.000107459, gnorm=1.007, train_wall=22, gb_free=6.9, wall=19823
2021-03-11 02:17:12 | INFO | train_inner | epoch 079:    666 / 1103 loss=3.009, nll_loss=1.453, ppl=2.74, wps=16059.2, ups=4.46, wpb=3599.6, bsz=137.2, num_updates=86700, lr=0.000107397, gnorm=0.996, train_wall=22, gb_free=7.3, wall=19845
2021-03-11 02:17:34 | INFO | train_inner | epoch 079:    766 / 1103 loss=3.014, nll_loss=1.459, ppl=2.75, wps=16234.7, ups=4.44, wpb=3654, bsz=134.8, num_updates=86800, lr=0.000107335, gnorm=1.001, train_wall=22, gb_free=7, wall=19868
2021-03-11 02:17:57 | INFO | train_inner | epoch 079:    866 / 1103 loss=3.036, nll_loss=1.484, ppl=2.8, wps=15973, ups=4.47, wpb=3569.9, bsz=134.5, num_updates=86900, lr=0.000107273, gnorm=1.019, train_wall=22, gb_free=7.1, wall=19890
2021-03-11 02:18:19 | INFO | train_inner | epoch 079:    966 / 1103 loss=2.994, nll_loss=1.441, ppl=2.71, wps=15925.2, ups=4.43, wpb=3592.8, bsz=170.4, num_updates=87000, lr=0.000107211, gnorm=0.99, train_wall=22, gb_free=7.2, wall=19913
2021-03-11 02:18:42 | INFO | train_inner | epoch 079:   1066 / 1103 loss=2.996, nll_loss=1.441, ppl=2.71, wps=15996.7, ups=4.4, wpb=3634, bsz=153.8, num_updates=87100, lr=0.00010715, gnorm=0.99, train_wall=23, gb_free=6.9, wall=19935
2021-03-11 02:18:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:18:54 | INFO | valid | epoch 079 | valid on 'valid' subset | loss 4.011 | nll_loss 2.461 | ppl 5.51 | wps 48180.8 | wpb 2873.1 | bsz 115.6 | num_updates 87137 | best_loss 3.91
2021-03-11 02:18:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 79 @ 87137 updates
2021-03-11 02:18:54 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:18:56 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:18:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 79 @ 87137 updates, score 4.011) (writing took 2.2257236391305923 seconds)
2021-03-11 02:18:56 | INFO | fairseq_cli.train | end of epoch 79 (average epoch stats below)
2021-03-11 02:18:56 | INFO | train | epoch 079 | loss 2.99 | nll_loss 1.432 | ppl 2.7 | wps 15550.9 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 87137 | lr 0.000107127 | gnorm 0.999 | train_wall 246 | gb_free 6.6 | wall 19950
2021-03-11 02:18:56 | INFO | fairseq.trainer | begin training epoch 80
2021-03-11 02:18:56 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 02:19:11 | INFO | train_inner | epoch 080:     63 / 1103 loss=2.972, nll_loss=1.414, ppl=2.66, wps=12503.5, ups=3.49, wpb=3586.3, bsz=162.5, num_updates=87200, lr=0.000107088, gnorm=0.979, train_wall=22, gb_free=7.1, wall=19964
2021-03-11 02:19:33 | INFO | train_inner | epoch 080:    163 / 1103 loss=2.973, nll_loss=1.412, ppl=2.66, wps=16026.8, ups=4.43, wpb=3615.7, bsz=143.5, num_updates=87300, lr=0.000107027, gnorm=1.009, train_wall=22, gb_free=6.9, wall=19987
2021-03-11 02:19:56 | INFO | train_inner | epoch 080:    263 / 1103 loss=2.958, nll_loss=1.396, ppl=2.63, wps=15799.7, ups=4.47, wpb=3536.1, bsz=148.2, num_updates=87400, lr=0.000106966, gnorm=1.003, train_wall=22, gb_free=7, wall=20009
2021-03-11 02:20:18 | INFO | train_inner | epoch 080:    363 / 1103 loss=2.934, nll_loss=1.369, ppl=2.58, wps=15871.2, ups=4.41, wpb=3598.9, bsz=167.5, num_updates=87500, lr=0.000106904, gnorm=0.968, train_wall=23, gb_free=7.3, wall=20032
2021-03-11 02:20:41 | INFO | train_inner | epoch 080:    463 / 1103 loss=2.976, nll_loss=1.415, ppl=2.67, wps=15927.2, ups=4.46, wpb=3573.4, bsz=145.4, num_updates=87600, lr=0.000106843, gnorm=0.999, train_wall=22, gb_free=6.9, wall=20054
2021-03-11 02:21:03 | INFO | train_inner | epoch 080:    563 / 1103 loss=2.979, nll_loss=1.418, ppl=2.67, wps=15736.6, ups=4.51, wpb=3491.6, bsz=134.9, num_updates=87700, lr=0.000106783, gnorm=1.028, train_wall=22, gb_free=7.5, wall=20076
2021-03-11 02:21:25 | INFO | train_inner | epoch 080:    663 / 1103 loss=3.013, nll_loss=1.457, ppl=2.74, wps=16056.9, ups=4.46, wpb=3598.4, bsz=129, num_updates=87800, lr=0.000106722, gnorm=1.013, train_wall=22, gb_free=7.1, wall=20099
2021-03-11 02:21:48 | INFO | train_inner | epoch 080:    763 / 1103 loss=3.008, nll_loss=1.451, ppl=2.73, wps=15969.4, ups=4.44, wpb=3595.2, bsz=129.3, num_updates=87900, lr=0.000106661, gnorm=1.01, train_wall=22, gb_free=6.8, wall=20121
2021-03-11 02:22:10 | INFO | train_inner | epoch 080:    863 / 1103 loss=3.017, nll_loss=1.463, ppl=2.76, wps=16040.9, ups=4.49, wpb=3573.7, bsz=143.5, num_updates=88000, lr=0.0001066, gnorm=1.01, train_wall=22, gb_free=6.8, wall=20143
2021-03-11 02:22:33 | INFO | train_inner | epoch 080:    963 / 1103 loss=3.008, nll_loss=1.456, ppl=2.74, wps=16273.5, ups=4.48, wpb=3634.3, bsz=157.6, num_updates=88100, lr=0.00010654, gnorm=0.989, train_wall=22, gb_free=7.1, wall=20166
2021-03-11 02:22:55 | INFO | train_inner | epoch 080:   1063 / 1103 loss=3.002, nll_loss=1.446, ppl=2.72, wps=16056.1, ups=4.51, wpb=3560, bsz=146.2, num_updates=88200, lr=0.000106479, gnorm=1.008, train_wall=22, gb_free=7, wall=20188
2021-03-11 02:23:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:23:07 | INFO | valid | epoch 080 | valid on 'valid' subset | loss 4.018 | nll_loss 2.462 | ppl 5.51 | wps 48246.4 | wpb 2873.1 | bsz 115.6 | num_updates 88240 | best_loss 3.91
2021-03-11 02:23:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 80 @ 88240 updates
2021-03-11 02:23:07 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:23:10 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:23:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 80 @ 88240 updates, score 4.018) (writing took 2.14547024294734 seconds)
2021-03-11 02:23:10 | INFO | fairseq_cli.train | end of epoch 80 (average epoch stats below)
2021-03-11 02:23:10 | INFO | train | epoch 080 | loss 2.985 | nll_loss 1.426 | ppl 2.69 | wps 15591 | ups 4.36 | wpb 3577.8 | bsz 145.3 | num_updates 88240 | lr 0.000106455 | gnorm 1.002 | train_wall 246 | gb_free 7.4 | wall 20203
2021-03-11 02:23:10 | INFO | fairseq.trainer | begin training epoch 81
2021-03-11 02:23:10 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 02:23:23 | INFO | train_inner | epoch 081:     60 / 1103 loss=2.975, nll_loss=1.415, ppl=2.67, wps=12694.7, ups=3.51, wpb=3614.3, bsz=143.2, num_updates=88300, lr=0.000106419, gnorm=0.981, train_wall=22, gb_free=6.7, wall=20216
2021-03-11 02:23:46 | INFO | train_inner | epoch 081:    160 / 1103 loss=2.955, nll_loss=1.39, ppl=2.62, wps=15883.1, ups=4.45, wpb=3568.6, bsz=145.7, num_updates=88400, lr=0.000106359, gnorm=1.006, train_wall=22, gb_free=6.8, wall=20239
2021-03-11 02:24:08 | INFO | train_inner | epoch 081:    260 / 1103 loss=2.942, nll_loss=1.377, ppl=2.6, wps=15778.2, ups=4.44, wpb=3554, bsz=150.3, num_updates=88500, lr=0.000106299, gnorm=1.003, train_wall=22, gb_free=7.1, wall=20261
2021-03-11 02:24:31 | INFO | train_inner | epoch 081:    360 / 1103 loss=2.963, nll_loss=1.4, ppl=2.64, wps=15806.8, ups=4.48, wpb=3526.5, bsz=142.2, num_updates=88600, lr=0.000106239, gnorm=1.011, train_wall=22, gb_free=7.1, wall=20284
2021-03-11 02:24:53 | INFO | train_inner | epoch 081:    460 / 1103 loss=2.97, nll_loss=1.407, ppl=2.65, wps=15681.4, ups=4.51, wpb=3477.4, bsz=140, num_updates=88700, lr=0.000106179, gnorm=1.016, train_wall=22, gb_free=7.1, wall=20306
2021-03-11 02:25:15 | INFO | train_inner | epoch 081:    560 / 1103 loss=3.007, nll_loss=1.451, ppl=2.73, wps=16146.9, ups=4.44, wpb=3637.9, bsz=139.4, num_updates=88800, lr=0.000106119, gnorm=1.007, train_wall=22, gb_free=7, wall=20328
2021-03-11 02:25:38 | INFO | train_inner | epoch 081:    660 / 1103 loss=2.954, nll_loss=1.393, ppl=2.63, wps=16013, ups=4.41, wpb=3627.1, bsz=162.2, num_updates=88900, lr=0.000106059, gnorm=0.982, train_wall=23, gb_free=6.9, wall=20351
2021-03-11 02:26:00 | INFO | train_inner | epoch 081:    760 / 1103 loss=2.975, nll_loss=1.415, ppl=2.67, wps=15716.3, ups=4.46, wpb=3526.4, bsz=150.1, num_updates=89000, lr=0.000106, gnorm=1.004, train_wall=22, gb_free=7, wall=20373
2021-03-11 02:26:23 | INFO | train_inner | epoch 081:    860 / 1103 loss=3.005, nll_loss=1.45, ppl=2.73, wps=15982.5, ups=4.45, wpb=3592.6, bsz=149.8, num_updates=89100, lr=0.00010594, gnorm=1.008, train_wall=22, gb_free=7, wall=20396
2021-03-11 02:26:45 | INFO | train_inner | epoch 081:    960 / 1103 loss=3.017, nll_loss=1.462, ppl=2.76, wps=15955.9, ups=4.47, wpb=3571.9, bsz=139, num_updates=89200, lr=0.000105881, gnorm=1.023, train_wall=22, gb_free=7.2, wall=20418
2021-03-11 02:27:08 | INFO | train_inner | epoch 081:   1060 / 1103 loss=3.019, nll_loss=1.465, ppl=2.76, wps=16012.2, ups=4.42, wpb=3625.4, bsz=131.4, num_updates=89300, lr=0.000105822, gnorm=1.015, train_wall=23, gb_free=7.2, wall=20441
2021-03-11 02:27:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:27:21 | INFO | valid | epoch 081 | valid on 'valid' subset | loss 3.999 | nll_loss 2.452 | ppl 5.47 | wps 48256.9 | wpb 2873.1 | bsz 115.6 | num_updates 89343 | best_loss 3.91
2021-03-11 02:27:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 81 @ 89343 updates
2021-03-11 02:27:21 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:27:23 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:27:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 81 @ 89343 updates, score 3.999) (writing took 2.143554352223873 seconds)
2021-03-11 02:27:23 | INFO | fairseq_cli.train | end of epoch 81 (average epoch stats below)
2021-03-11 02:27:23 | INFO | train | epoch 081 | loss 2.982 | nll_loss 1.422 | ppl 2.68 | wps 15551.6 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 89343 | lr 0.000105796 | gnorm 1.007 | train_wall 246 | gb_free 7.2 | wall 20456
2021-03-11 02:27:23 | INFO | fairseq.trainer | begin training epoch 82
2021-03-11 02:27:23 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 02:27:36 | INFO | train_inner | epoch 082:     57 / 1103 loss=2.987, nll_loss=1.429, ppl=2.69, wps=12704, ups=3.51, wpb=3616.2, bsz=146.2, num_updates=89400, lr=0.000105762, gnorm=1.023, train_wall=22, gb_free=6.6, wall=20469
2021-03-11 02:27:59 | INFO | train_inner | epoch 082:    157 / 1103 loss=2.939, nll_loss=1.373, ppl=2.59, wps=15900.3, ups=4.46, wpb=3564.3, bsz=151.3, num_updates=89500, lr=0.000105703, gnorm=1.007, train_wall=22, gb_free=6.9, wall=20492
2021-03-11 02:28:21 | INFO | train_inner | epoch 082:    257 / 1103 loss=2.975, nll_loss=1.413, ppl=2.66, wps=16027.7, ups=4.44, wpb=3613.1, bsz=133.6, num_updates=89600, lr=0.000105644, gnorm=1, train_wall=22, gb_free=7.2, wall=20514
2021-03-11 02:28:44 | INFO | train_inner | epoch 082:    357 / 1103 loss=2.963, nll_loss=1.398, ppl=2.64, wps=15837.4, ups=4.45, wpb=3557.9, bsz=133.6, num_updates=89700, lr=0.000105585, gnorm=1.006, train_wall=22, gb_free=7.3, wall=20537
2021-03-11 02:29:06 | INFO | train_inner | epoch 082:    457 / 1103 loss=2.978, nll_loss=1.418, ppl=2.67, wps=16132.2, ups=4.48, wpb=3604.5, bsz=142.6, num_updates=89800, lr=0.000105527, gnorm=0.995, train_wall=22, gb_free=7, wall=20559
2021-03-11 02:29:29 | INFO | train_inner | epoch 082:    557 / 1103 loss=2.981, nll_loss=1.423, ppl=2.68, wps=16061.1, ups=4.42, wpb=3630, bsz=151.5, num_updates=89900, lr=0.000105468, gnorm=0.992, train_wall=22, gb_free=7.2, wall=20582
2021-03-11 02:29:51 | INFO | train_inner | epoch 082:    657 / 1103 loss=2.967, nll_loss=1.407, ppl=2.65, wps=15879.6, ups=4.47, wpb=3550.3, bsz=161.5, num_updates=90000, lr=0.000105409, gnorm=1.004, train_wall=22, gb_free=7.1, wall=20604
2021-03-11 02:30:14 | INFO | train_inner | epoch 082:    757 / 1103 loss=2.995, nll_loss=1.438, ppl=2.71, wps=15994, ups=4.43, wpb=3610.7, bsz=139.3, num_updates=90100, lr=0.000105351, gnorm=1.001, train_wall=22, gb_free=6.4, wall=20627
2021-03-11 02:30:36 | INFO | train_inner | epoch 082:    857 / 1103 loss=2.987, nll_loss=1.43, ppl=2.69, wps=16119.4, ups=4.45, wpb=3622.4, bsz=158.2, num_updates=90200, lr=0.000105292, gnorm=0.995, train_wall=22, gb_free=7.1, wall=20649
2021-03-11 02:30:58 | INFO | train_inner | epoch 082:    957 / 1103 loss=2.967, nll_loss=1.404, ppl=2.65, wps=15446.5, ups=4.46, wpb=3460.4, bsz=139.6, num_updates=90300, lr=0.000105234, gnorm=1.033, train_wall=22, gb_free=6.8, wall=20672
2021-03-11 02:31:21 | INFO | train_inner | epoch 082:   1057 / 1103 loss=2.983, nll_loss=1.424, ppl=2.68, wps=15819.3, ups=4.48, wpb=3533.8, bsz=150.2, num_updates=90400, lr=0.000105176, gnorm=1.019, train_wall=22, gb_free=6.7, wall=20694
2021-03-11 02:31:31 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:31:35 | INFO | valid | epoch 082 | valid on 'valid' subset | loss 4.018 | nll_loss 2.468 | ppl 5.53 | wps 48142.5 | wpb 2873.1 | bsz 115.6 | num_updates 90446 | best_loss 3.91
2021-03-11 02:31:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 82 @ 90446 updates
2021-03-11 02:31:35 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:31:37 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:31:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 82 @ 90446 updates, score 4.018) (writing took 2.20364161580801 seconds)
2021-03-11 02:31:37 | INFO | fairseq_cli.train | end of epoch 82 (average epoch stats below)
2021-03-11 02:31:37 | INFO | train | epoch 082 | loss 2.975 | nll_loss 1.414 | ppl 2.67 | wps 15551.9 | ups 4.35 | wpb 3577.8 | bsz 145.3 | num_updates 90446 | lr 0.000105149 | gnorm 1.005 | train_wall 246 | gb_free 6.9 | wall 20710
2021-03-11 02:31:37 | INFO | fairseq.trainer | begin training epoch 83
2021-03-11 02:31:37 | INFO | fairseq_cli.train | Start iterating over samples
2021-03-11 02:31:49 | INFO | train_inner | epoch 083:     54 / 1103 loss=2.983, nll_loss=1.425, ppl=2.69, wps=12723.6, ups=3.51, wpb=3630, bsz=147.1, num_updates=90500, lr=0.000105118, gnorm=0.991, train_wall=22, gb_free=7.5, wall=20722
2021-03-11 02:32:12 | INFO | train_inner | epoch 083:    154 / 1103 loss=2.944, nll_loss=1.378, ppl=2.6, wps=16106.4, ups=4.49, wpb=3586.7, bsz=144.9, num_updates=90600, lr=0.00010506, gnorm=0.989, train_wall=22, gb_free=7, wall=20745
2021-03-11 02:32:34 | INFO | train_inner | epoch 083:    254 / 1103 loss=2.963, nll_loss=1.398, ppl=2.63, wps=15921.1, ups=4.48, wpb=3550.6, bsz=128.4, num_updates=90700, lr=0.000105002, gnorm=1.025, train_wall=22, gb_free=6.9, wall=20767
2021-03-11 02:32:56 | INFO | train_inner | epoch 083:    354 / 1103 loss=2.985, nll_loss=1.424, ppl=2.68, wps=16149, ups=4.49, wpb=3599.7, bsz=134.6, num_updates=90800, lr=0.000104944, gnorm=1.012, train_wall=22, gb_free=7.2, wall=20789
2021-03-11 02:33:18 | INFO | train_inner | epoch 083:    454 / 1103 loss=2.953, nll_loss=1.389, ppl=2.62, wps=15701.8, ups=4.49, wpb=3498.9, bsz=146.6, num_updates=90900, lr=0.000104886, gnorm=1.018, train_wall=22, gb_free=7, wall=20812
2021-03-11 02:33:41 | INFO | train_inner | epoch 083:    554 / 1103 loss=2.982, nll_loss=1.422, ppl=2.68, wps=16129.1, ups=4.49, wpb=3593.9, bsz=139.4, num_updates=91000, lr=0.000104828, gnorm=1.02, train_wall=22, gb_free=7, wall=20834
2021-03-11 02:34:03 | INFO | train_inner | epoch 083:    654 / 1103 loss=2.965, nll_loss=1.405, ppl=2.65, wps=16120.3, ups=4.43, wpb=3640.5, bsz=158.9, num_updates=91100, lr=0.000104771, gnorm=0.991, train_wall=22, gb_free=7.2, wall=20856
2021-03-11 02:34:26 | INFO | train_inner | epoch 083:    754 / 1103 loss=2.983, nll_loss=1.425, ppl=2.69, wps=15963.6, ups=4.49, wpb=3559, bsz=151.4, num_updates=91200, lr=0.000104713, gnorm=1.012, train_wall=22, gb_free=7.1, wall=20879
2021-03-11 02:34:48 | INFO | train_inner | epoch 083:    854 / 1103 loss=2.981, nll_loss=1.421, ppl=2.68, wps=15926.2, ups=4.46, wpb=3570, bsz=143, num_updates=91300, lr=0.000104656, gnorm=1.013, train_wall=22, gb_free=7.2, wall=20901
2021-03-11 02:35:10 | INFO | train_inner | epoch 083:    954 / 1103 loss=2.968, nll_loss=1.406, ppl=2.65, wps=15750.2, ups=4.48, wpb=3517.9, bsz=147.4, num_updates=91400, lr=0.000104599, gnorm=1.027, train_wall=22, gb_free=6.8, wall=20924
2021-03-11 02:35:33 | INFO | train_inner | epoch 083:   1054 / 1103 loss=2.998, nll_loss=1.442, ppl=2.72, wps=16175, ups=4.45, wpb=3637.4, bsz=146.9, num_updates=91500, lr=0.000104542, gnorm=1.006, train_wall=22, gb_free=6.9, wall=20946
2021-03-11 02:35:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2021-03-11 02:35:48 | INFO | valid | epoch 083 | valid on 'valid' subset | loss 4.017 | nll_loss 2.467 | ppl 5.53 | wps 48164.1 | wpb 2873.1 | bsz 115.6 | num_updates 91549 | best_loss 3.91
2021-03-11 02:35:48 | INFO | fairseq_cli.train | early stop since valid performance hasn't improved for last 50 runs
2021-03-11 02:35:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 83 @ 91549 updates
2021-03-11 02:35:48 | INFO | fairseq.trainer | Saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:35:50 | INFO | fairseq.trainer | Finished saving checkpoint to /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt
2021-03-11 02:35:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint /home/amax/Data/flstm/iwslt/multifeatgen_baseline_new/checkpoint_last.pt (epoch 83 @ 91549 updates, score 4.017) (writing took 2.066120717674494 seconds)
2021-03-11 02:35:50 | INFO | fairseq_cli.train | end of epoch 83 (average epoch stats below)
2021-03-11 02:35:50 | INFO | train | epoch 083 | loss 2.971 | nll_loss 1.41 | ppl 2.66 | wps 15618.4 | ups 4.37 | wpb 3577.8 | bsz 145.3 | num_updates 91549 | lr 0.000104514 | gnorm 1.009 | train_wall 245 | gb_free 7.2 | wall 20963
2021-03-11 02:35:50 | INFO | fairseq_cli.train | done training in 20962.8 seconds
